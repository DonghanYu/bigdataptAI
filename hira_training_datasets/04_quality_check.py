#!/usr/bin/env python3
"""
HIRA ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ì¤‘ë³µ ì œê±°
- ê¸¸ì´ ê²€ì¦
- í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
- í•„í„°ë§

ëª©í‘œ: ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ ì„ ë³„
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Set
from collections import Counter, defaultdict
import argparse
from datetime import datetime


class QualityChecker:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°"""

    def __init__(self, input_path: str):
        """ì´ˆê¸°í™”"""
        with open(input_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.filtered_data = []
        self.rejected = []

        self.stats = {
            "initial_count": len(self.data),
            "filtered_count": 0,
            "rejected_count": 0,
            "rejection_reasons": Counter(),
            "quality_scores": []
        }

    def check_all(self, min_score: float = 0.6):
        """ì „ì²´ í’ˆì§ˆ ê²€ì¦"""
        print("="*80)
        print("ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
        print("="*80 + "\n")

        print(f"ì…ë ¥ ë°ì´í„°: {len(self.data)}ê°œ")
        print(f"ìµœì†Œ í’ˆì§ˆ ì ìˆ˜: {min_score}\n")

        # 1. ì¤‘ë³µ ì œê±°
        print("[1/5] ì¤‘ë³µ ì œê±° ì¤‘...")
        self._remove_duplicates()

        # 2. ê¸¸ì´ ê²€ì¦
        print("[2/5] ê¸¸ì´ ê²€ì¦ ì¤‘...")
        self._check_lengths()

        # 3. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        print("[3/5] í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì¤‘...")
        self._calculate_quality_scores()

        # 4. í•„í„°ë§
        print("[4/5] í•„í„°ë§ ì¤‘...")
        self._filter_by_score(min_score)

        # 5. ìµœì¢… ì •ë¦¬
        print("[5/5] ìµœì¢… ì •ë¦¬ ì¤‘...")
        self._finalize()

        print(f"\nâœ… ê²€ì¦ ì™„ë£Œ")
        print(f"   í†µê³¼: {self.stats['filtered_count']}ê°œ")
        print(f"   ì œì™¸: {self.stats['rejected_count']}ê°œ")

        return self.filtered_data

    def _remove_duplicates(self):
        """ì¤‘ë³µ ì œê±°"""
        seen_questions = set()
        unique_data = []

        for qa in self.data:
            q = qa["instruction"]

            if q not in seen_questions:
                seen_questions.add(q)
                unique_data.append(qa)
            else:
                self.rejected.append({
                    "data": qa,
                    "reason": "duplicate_question"
                })

        duplicates = len(self.data) - len(unique_data)
        print(f"   ì œê±°ëœ ì¤‘ë³µ: {duplicates}ê°œ")
        print(f"   ë‚¨ì€ ë°ì´í„°: {len(unique_data)}ê°œ")

        self.data = unique_data
        self.stats["rejection_reasons"]["duplicate"] = duplicates

    def _check_lengths(self):
        """ê¸¸ì´ ê²€ì¦"""
        valid_data = []

        for qa in self.data:
            q_len = len(qa["instruction"])
            a_len = len(qa["output"])

            # ê¸¸ì´ ê¸°ì¤€: ì§ˆë¬¸ 5-100ì, ë‹µë³€ 20-500ì
            if 5 <= q_len <= 100 and 20 <= a_len <= 500:
                valid_data.append(qa)
            else:
                self.rejected.append({
                    "data": qa,
                    "reason": f"length_invalid (Q:{q_len}, A:{a_len})"
                })

        rejected = len(self.data) - len(valid_data)
        print(f"   ê¸¸ì´ ë¶€ì í•©: {rejected}ê°œ")
        print(f"   ë‚¨ì€ ë°ì´í„°: {len(valid_data)}ê°œ")

        self.data = valid_data
        self.stats["rejection_reasons"]["length"] = rejected

    def _calculate_quality_scores(self):
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        for qa in self.data:
            score = self._calculate_score(qa)
            qa["metadata"]["quality_score"] = score
            self.stats["quality_scores"].append(score)

        avg_score = sum(self.stats["quality_scores"]) / len(self.stats["quality_scores"])
        print(f"   í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_score:.3f}")

    def _calculate_score(self, qa: Dict) -> float:
        """ê°œë³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 1.0
        question = qa["instruction"]
        answer = qa["output"]

        # 1. ì§ˆë¬¸ í’ˆì§ˆ (0.4)
        q_score = 0.0

        # ì§ˆë¬¸ ê¸¸ì´ ì ì •ì„±
        q_len = len(question)
        if 10 <= q_len <= 30:
            q_score += 0.15
        elif 5 <= q_len < 10 or 30 < q_len <= 50:
            q_score += 0.10
        else:
            q_score += 0.05

        # ì§ˆë¬¸ ë¶€í˜¸ ì¡´ì¬
        if '?' in question or 'ìš”' in question[-2:]:
            q_score += 0.10
        else:
            q_score -= 0.05

        # ì§ˆë¬¸ ëª…í™•ì„± (ì˜ë¬¸ì‚¬ ì¡´ì¬)
        interrogatives = ['ì–´ë–»ê²Œ', 'ë­”ê°€ìš”', 'ë¬´ì—‡', 'ì–´ë””ì„œ', 'ì–¸ì œ', 'ì™œ', 'ëˆ„ê°€']
        if any(word in question for word in interrogatives):
            q_score += 0.10

        # ì¤‘ë³µ ë‹¨ì–´ ì²´í¬
        words = question.split()
        if len(words) != len(set(words)):
            q_score -= 0.05

        # 2. ë‹µë³€ í’ˆì§ˆ (0.4)
        a_score = 0.0

        # ë‹µë³€ ê¸¸ì´ ì ì •ì„±
        a_len = len(answer)
        if 50 <= a_len <= 200:
            a_score += 0.15
        elif 20 <= a_len < 50 or 200 < a_len <= 350:
            a_score += 0.10
        else:
            a_score += 0.05

        # ë‹µë³€ êµ¬ì¡°ì„±
        if 'ìŠµë‹ˆë‹¤' in answer or 'ë©ë‹ˆë‹¤' in answer:
            a_score += 0.10

        # ë©”ë‰´ ê²½ë¡œ í¬í•¨
        if '>' in answer or 'ë©”ë‰´' in answer:
            a_score += 0.10

        # ì˜ˆì‹œ í¬í•¨
        if 'ì˜ˆ:' in answer or 'ì˜ˆë¥¼ ë“¤ì–´' in answer:
            a_score += 0.05

        # 3. ì¼ê´€ì„± (0.2)
        c_score = 0.0

        # Q-A í‚¤ì›Œë“œ ì¼ì¹˜
        q_keywords = set(re.findall(r'[ê°€-í£]{2,}', question))
        a_keywords = set(re.findall(r'[ê°€-í£]{2,}', answer))

        overlap = len(q_keywords & a_keywords)
        if overlap >= 2:
            c_score += 0.15
        elif overlap == 1:
            c_score += 0.10
        else:
            c_score += 0.05

        # Qì— ìˆëŠ” ì£¼ìš” í‚¤ì›Œë“œê°€ Aì—ë„ ìˆëŠ”ì§€
        important_words = ['ë°ì´í„°', 'ì‹ ì²­', 'í†µê³„', 'ì½”ë“œ', 'API', 'ë¶„ì„']
        for word in important_words:
            if word in question and word in answer:
                c_score += 0.05
                break

        # ìµœì¢… ì ìˆ˜
        final_score = max(0.0, min(1.0, q_score + a_score + c_score))

        return final_score

    def _filter_by_score(self, min_score: float):
        """ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§"""
        filtered = []

        for qa in self.data:
            score = qa["metadata"]["quality_score"]

            if score >= min_score:
                filtered.append(qa)
            else:
                self.rejected.append({
                    "data": qa,
                    "reason": f"low_quality_score ({score:.3f})"
                })

        rejected = len(self.data) - len(filtered)
        print(f"   ë‚®ì€ í’ˆì§ˆ ì ìˆ˜: {rejected}ê°œ")
        print(f"   ë‚¨ì€ ë°ì´í„°: {len(filtered)}ê°œ")

        self.filtered_data = filtered
        self.stats["filtered_count"] = len(filtered)
        self.stats["rejected_count"] = self.stats["initial_count"] - self.stats["filtered_count"]
        self.stats["rejection_reasons"]["low_score"] = rejected

    def _finalize(self):
        """ìµœì¢… ì •ë¦¬"""
        # ID ì¬ë¶€ì—¬
        for idx, qa in enumerate(self.filtered_data):
            menu = qa["metadata"]["menu"]
            qa["id"] = f"hira_{menu}_{idx:05d}"

        print(f"   ìµœì¢… ë°ì´í„°: {len(self.filtered_data)}ê°œ")

    def save_data(self, output_path: str):
        """í•„í„°ë§ëœ ë°ì´í„° ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.filtered_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… í•„í„°ë§ ë°ì´í„° ì €ì¥: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {output_path.stat().st_size:,} bytes")

    def save_report(self, output_path: str):
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "summary": {
                "initial_count": self.stats["initial_count"],
                "filtered_count": self.stats["filtered_count"],
                "rejected_count": self.stats["rejected_count"],
                "pass_rate": self.stats["filtered_count"] / self.stats["initial_count"],
            },
            "quality_scores": {
                "average": sum(self.stats["quality_scores"]) / len(self.stats["quality_scores"]) if self.stats["quality_scores"] else 0,
                "min": min(self.stats["quality_scores"]) if self.stats["quality_scores"] else 0,
                "max": max(self.stats["quality_scores"]) if self.stats["quality_scores"] else 0,
                "distribution": {
                    "excellent (0.9-1.0)": sum(1 for s in self.stats["quality_scores"] if s >= 0.9),
                    "good (0.8-0.9)": sum(1 for s in self.stats["quality_scores"] if 0.8 <= s < 0.9),
                    "fair (0.7-0.8)": sum(1 for s in self.stats["quality_scores"] if 0.7 <= s < 0.8),
                    "acceptable (0.6-0.7)": sum(1 for s in self.stats["quality_scores"] if 0.6 <= s < 0.7),
                    "poor (<0.6)": sum(1 for s in self.stats["quality_scores"] if s < 0.6),
                }
            },
            "rejection_reasons": dict(self.stats["rejection_reasons"]),
            "timestamp": datetime.now().isoformat()
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ… í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    def print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š í’ˆì§ˆ ê²€ì¦ í†µê³„")
        print("="*80)

        print(f"\nì „ì²´:")
        print(f"  ì…ë ¥: {self.stats['initial_count']}ê°œ")
        print(f"  í†µê³¼: {self.stats['filtered_count']}ê°œ")
        print(f"  ì œì™¸: {self.stats['rejected_count']}ê°œ")
        print(f"  í†µê³¼ìœ¨: {self.stats['filtered_count']/self.stats['initial_count']*100:.1f}%")

        print(f"\nì œì™¸ ì‚¬ìœ :")
        for reason, count in self.stats["rejection_reasons"].most_common():
            pct = (count / self.stats['rejected_count']) * 100 if self.stats['rejected_count'] else 0
            print(f"  {reason:20s}: {count:4d}ê°œ ({pct:5.1f}%)")

        print(f"\ní’ˆì§ˆ ì ìˆ˜ ë¶„í¬:")
        if self.stats["quality_scores"]:
            avg = sum(self.stats["quality_scores"]) / len(self.stats["quality_scores"])
            print(f"  í‰ê· : {avg:.3f}")
            print(f"  ìµœì†Œ: {min(self.stats['quality_scores']):.3f}")
            print(f"  ìµœëŒ€: {max(self.stats['quality_scores']):.3f}")

            bins = {
                "ìš°ìˆ˜ (0.9-1.0)": sum(1 for s in self.stats["quality_scores"] if s >= 0.9),
                "ì–‘í˜¸ (0.8-0.9)": sum(1 for s in self.stats["quality_scores"] if 0.8 <= s < 0.9),
                "ë³´í†µ (0.7-0.8)": sum(1 for s in self.stats["quality_scores"] if 0.7 <= s < 0.8),
                "í—ˆìš© (0.6-0.7)": sum(1 for s in self.stats["quality_scores"] if 0.6 <= s < 0.7),
            }

            for grade, count in bins.items():
                pct = (count / len(self.stats["quality_scores"])) * 100
                print(f"  {grade:15s}: {count:4d}ê°œ ({pct:5.1f}%)")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description="HIRA ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    parser.add_argument("--input", type=str,
                       default="output/temp/template_generated.json",
                       help="ì…ë ¥ íŒŒì¼")
    parser.add_argument("--output", type=str,
                       default="output/temp/quality_filtered.json",
                       help="ì¶œë ¥ íŒŒì¼ (í•„í„°ë§ëœ ë°ì´í„°)")
    parser.add_argument("--report", type=str,
                       default="output/v1.0/metadata/quality_report.json",
                       help="í’ˆì§ˆ ë¦¬í¬íŠ¸ íŒŒì¼")
    parser.add_argument("--min-score", type=float, default=0.6,
                       help="ìµœì†Œ í’ˆì§ˆ ì ìˆ˜ (default: 0.6)")
    args = parser.parse_args()

    print("\n" + "="*80)
    print("HIRA ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸° v1.0")
    print("="*80 + "\n")

    # ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent
    input_path = base_dir / args.input
    output_path = base_dir / args.output
    report_path = base_dir / args.report

    # ê²€ì¦ê¸° ì´ˆê¸°í™”
    checker = QualityChecker(input_path)

    # ê²€ì¦ ì‹¤í–‰
    filtered_data = checker.check_all(min_score=args.min_score)

    # ì €ì¥
    checker.save_data(output_path)
    checker.save_report(report_path)

    # í†µê³„
    checker.print_statistics()

    print("\n" + "="*80)
    print("ğŸ‰ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ!")
    print("="*80)
    print(f"\në‹¤ìŒ ë‹¨ê³„: ë°ì´í„°ì…‹ ë¶„í• ")
    print(f"python3 05_split_dataset.py --input {output_path}")


if __name__ == "__main__":
    main()
