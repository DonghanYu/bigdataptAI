#!/usr/bin/env python3
"""
HIRA SOLAR-10.7B ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤
- í•™ìŠµëœ LoRA ëª¨ë¸ ë¡œë“œ
- Gradio ì›¹ UI ì œê³µ
- HIRA ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€
"""

import argparse
import sys
from pathlib import Path
import json

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    import gradio as gr
    LIBS_AVAILABLE = True
except ImportError:
    LIBS_AVAILABLE = False
    print("âš ï¸  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install torch transformers peft gradio")


class HIRAInference:
    """HIRA ì¶”ë¡  í´ë˜ìŠ¤"""

    def __init__(self, base_model_path: str, lora_adapter_path: str = None):
        """
        Args:
            base_model_path: SOLAR ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
            lora_adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ì„ íƒ)
        """
        self.base_model_path = Path(base_model_path)
        self.lora_adapter_path = Path(lora_adapter_path) if lora_adapter_path else None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.tokenizer = None
        self.model = None

        print("="*80)
        print("HIRA SOLAR-10.7B ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤")
        print("="*80 + "\n")

        self._print_environment()
        self._load_model()

    def _print_environment(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        print("ğŸ“Š í™˜ê²½:")
        print(f"  Device: {self.device}")
        print(f"  PyTorch: {torch.__version__}")

        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"  VRAM: {vram:.1f} GB")

    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"\nëª¨ë¸ ë¡œë“œ ì¤‘...")
        print(f"  ê¸°ë³¸ ëª¨ë¸: {self.base_model_path}")

        # í† í¬ë‚˜ì´ì €
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path,
            trust_remote_code=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print(f"  âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

        # ê¸°ë³¸ ëª¨ë¸
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )

        print(f"  âœ“ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # LoRA ì–´ëŒ‘í„°
        if self.lora_adapter_path and self.lora_adapter_path.exists():
            print(f"  LoRA ì–´ëŒ‘í„°: {self.lora_adapter_path}")
            self.model = PeftModel.from_pretrained(
                self.model,
                self.lora_adapter_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            print(f"  âœ“ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"  âš ï¸  LoRA ì–´ëŒ‘í„° ì—†ìŒ (ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©)")

        self.model.eval()

    def generate(
        self,
        instruction: str,
        max_length: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50
    ) -> str:
        """
        ë‹µë³€ ìƒì„±

        Args:
            instruction: ì§ˆë¬¸/ëª…ë ¹
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            temperature: ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)
            top_p: Nucleus sampling
            top_k: Top-K sampling

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"### Instruction:\n{instruction.strip()}\n\n### Response:\n"

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=max_length,
            truncation=True
        ).to(self.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Response ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "### Response:" in generated_text:
            response = generated_text.split("### Response:")[-1].strip()
        else:
            response = generated_text.strip()

        return response

    def batch_generate(self, questions: list) -> list:
        """
        ë°°ì¹˜ ìƒì„±

        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë‹µë³€ ë¦¬ìŠ¤íŠ¸
        """
        answers = []
        for question in questions:
            answer = self.generate(question)
            answers.append(answer)

        return answers


def create_gradio_interface(inference: HIRAInference):
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""

    def predict(instruction, temperature, top_p, top_k, max_length):
        """ì˜ˆì¸¡ í•¨ìˆ˜"""
        try:
            response = inference.generate(
                instruction=instruction,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_length=max_length
            )
            return response
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ì˜ˆì‹œ ì§ˆë¬¸
    examples = [
        ["ìƒë³‘ì½”ë“œëŠ” ì–´ë–»ê²Œ ì¡°íšŒí•˜ë‚˜ìš”?"],
        ["í™˜ìí‘œë³¸ ë°ì´í„° ì‹ ì²­ ë°©ë²•ì€?"],
        ["HIRA ë°ì´í„° ê·œëª¨ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?"],
        ["ë§ì¶¤í˜• ë°ì´í„°ì™€ í™˜ìí‘œë³¸ì˜ ì°¨ì´ëŠ”?"],
        ["API í‚¤ëŠ” ì–´ë–»ê²Œ ë°œê¸‰ë°›ë‚˜ìš”?"],
        ["SAS Studio ì‹ ì²­ ë°©ë²•"],
        ["ì§„ë£Œë¹„ í†µê³„ëŠ” ì–´ë””ì„œ í™•ì¸í•˜ë‚˜ìš”?"],
        ["ë¹…ë°ì´í„°ë¶„ì„ì„¼í„°ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?"],
    ]

    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title="HIRA SOLAR-10.7B", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¥ HIRA ë³´ê±´ì˜ë£Œë¹…ë°ì´í„° ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
        gr.Markdown("SOLAR-10.7B ê¸°ë°˜ LoRA íŒŒì¸íŠœë‹ ëª¨ë¸")

        with gr.Row():
            with gr.Column(scale=2):
                instruction_input = gr.Textbox(
                    label="ì§ˆë¬¸",
                    placeholder="HIRA ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    lines=3
                )

                with gr.Accordion("ê³ ê¸‰ ì„¤ì •", open=False):
                    temperature_slider = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature (ì°½ì˜ì„±)",
                        info="ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
                    )

                    top_p_slider = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.9,
                        step=0.05,
                        label="Top-p (ë‹¤ì–‘ì„±)",
                        info="Nucleus sampling"
                    )

                    top_k_slider = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=50,
                        step=1,
                        label="Top-k",
                        info="ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤"
                    )

                    max_length_slider = gr.Slider(
                        minimum=128,
                        maximum=1024,
                        value=512,
                        step=64,
                        label="Max Length",
                        info="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
                    )

                submit_btn = gr.Button("ë‹µë³€ ìƒì„±", variant="primary")
                clear_btn = gr.ClearButton()

            with gr.Column(scale=2):
                output = gr.Textbox(
                    label="ë‹µë³€",
                    lines=10,
                    show_copy_button=True
                )

        gr.Markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        gr.Examples(
            examples=examples,
            inputs=[instruction_input],
            label="í´ë¦­í•˜ì—¬ ì§ˆë¬¸ ì…ë ¥"
        )

        gr.Markdown("""
        ### ğŸ“Š ëª¨ë¸ ì •ë³´
        - **ê¸°ë³¸ ëª¨ë¸**: SOLAR-10.7B (Upstage)
        - **íŒŒì¸íŠœë‹**: LoRA (Low-Rank Adaptation)
        - **ë°ì´í„°ì…‹**: HIRA 1,423ê°œ Q&A
        - **í•™ìŠµ ë©”ë‰´**: ì„œë¹„ìŠ¤ ì†Œê°œ, ë³´ê±´ì˜ë£Œë¹…ë°ì´í„°, ì˜ë£Œí†µê³„ì •ë³´, ê³µê³µë°ì´í„°, ê³ ê°ì§€ì›
        """)

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        submit_btn.click(
            fn=predict,
            inputs=[
                instruction_input,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                max_length_slider
            ],
            outputs=output
        )

        clear_btn.add([instruction_input, output])

    return demo


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="HIRA SOLAR-10.7B ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤")

    parser.add_argument("--base-model-path", type=str,
                       default="/home/work/LLM_Meditron/bigdataAI/solar_10.7b_package/model",
                       help="SOLAR ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--lora-adapter-path", type=str,
                       default=None,
                       help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ (í•™ìŠµëœ ëª¨ë¸)")
    parser.add_argument("--share", action="store_true",
                       help="ê³µê°œ ë§í¬ ìƒì„±")
    parser.add_argument("--server-name", type=str,
                       default="0.0.0.0",
                       help="ì„œë²„ ì£¼ì†Œ")
    parser.add_argument("--server-port", type=int,
                       default=7860,
                       help="ì„œë²„ í¬íŠ¸")

    args = parser.parse_args()

    # í™˜ê²½ ì²´í¬
    if not LIBS_AVAILABLE:
        print("\nâŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   pip install torch transformers peft gradio")
        sys.exit(1)

    # ëª¨ë¸ ë¡œë“œ
    inference = HIRAInference(
        base_model_path=args.base_model_path,
        lora_adapter_path=args.lora_adapter_path
    )

    # Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    print(f"\nğŸš€ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘...")
    print(f"   URL: http://{args.server_name}:{args.server_port}")

    if args.share:
        print(f"   ê³µê°œ ë§í¬ ìƒì„± ì¤‘...")

    demo = create_gradio_interface(inference)
    demo.launch(
        server_name=args.server_name,
        server_port=args.server_port,
        share=args.share
    )


if __name__ == "__main__":
    main()
