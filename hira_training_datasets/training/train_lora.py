#!/usr/bin/env python3
"""
HIRA SOLAR-10.7B LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- HIRA ë°ì´í„°ì…‹ìœ¼ë¡œ SOLAR-10.7B ëª¨ë¸ í•™ìŠµ
- LoRA (Low-Rank Adaptation) ì‚¬ìš©
- Train/Val/Test ë¶„í•  ì§€ì›
- Gradio ì¸í„°í˜ì´ìŠ¤ ì œê³µ
"""

import sys
import os
import argparse
from pathlib import Path
from datetime import datetime
import json

# bitsandbytes ê²½ê³  ì°¨ë‹¨
os.environ['BITSANDBYTES_NOWELCOME'] = '1'

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model, TaskType, PeftModel
    from tqdm import tqdm
    import numpy as np
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸  PyTorch/Transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   GPU í™˜ê²½ì—ì„œ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("   pip install torch transformers peft accelerate bitsandbytes")


class HIRADataset(Dataset):
    """HIRA ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""

    def __init__(self, file_path: str, tokenizer, max_length: int = 512):
        """
        Args:
            file_path: JSONL íŒŒì¼ ê²½ë¡œ
            tokenizer: Hugging Face tokenizer
            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.data = []

        # JSONL íŒŒì¼ ë¡œë“œ
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    if 'instruction' in item and 'output' in item:
                        self.data.append(item)
                except json.JSONDecodeError:
                    continue

        self.tokenizer = tokenizer
        self.max_length = max_length

        print(f"  ğŸ“‚ ë¡œë“œ: {len(self.data):,}ê°œ ({Path(file_path).name})")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # í”„ë¡¬í”„íŠ¸ í˜•ì‹
        instruction = item['instruction'].strip()
        response = item['output'].strip()

        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"

        # í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            prompt,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }


class HIRATrainer:
    """HIRA LoRA í•™ìŠµê¸°"""

    def __init__(self, config: dict):
        """
        Args:
            config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ê²½ë¡œ ì„¤ì •
        self.model_path = Path(config['model_path'])
        self.data_path = Path(config['data_path'])
        self.output_path = Path(config['output_path'])
        self.output_path.mkdir(parents=True, exist_ok=True)

        # ëª¨ë¸ & í† í¬ë‚˜ì´ì €
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.val_dataset = None

        print("="*80)
        print("HIRA SOLAR-10.7B LoRA í•™ìŠµ")
        print("="*80 + "\n")

        self._print_environment()
        self._print_config()

    def _print_environment(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        print("ğŸ“Š í™˜ê²½:")
        print(f"  Device: {self.device}")
        print(f"  PyTorch: {torch.__version__}")

        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"  VRAM: {vram:.1f} GB")
        else:
            print("  âš ï¸  GPU ì—†ìŒ (CPU ëª¨ë“œ, í•™ìŠµ ë§¤ìš° ëŠë¦¼)")

    def _print_config(self):
        """ì„¤ì • ì •ë³´ ì¶œë ¥"""
        print(f"\nâš™ï¸  í•™ìŠµ ì„¤ì •:")
        for key, value in self.config.items():
            if key not in ['model_path', 'data_path', 'output_path']:
                print(f"  {key}: {value}")

    def load_model(self):
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"\n[1/5] ëª¨ë¸ ë¡œë“œ ì¤‘...")
        print(f"  ê²½ë¡œ: {self.model_path}")

        # í† í¬ë‚˜ì´ì €
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print(f"  âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

        # ëª¨ë¸
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )

        print(f"  âœ“ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.config['lora_r'],
            lora_alpha=self.config['lora_alpha'],
            lora_dropout=self.config['lora_dropout'],
            target_modules=["q_proj", "v_proj"]  # SOLAR ëª¨ë¸ìš©
        )

        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()

        print(f"  âœ“ LoRA ì ìš© ì™„ë£Œ")

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print(f"\n[2/5] ë°ì´í„° ë¡œë“œ ì¤‘...")

        train_path = self.data_path / "train.jsonl"
        val_path = self.data_path / "val.json"

        if not train_path.exists():
            raise FileNotFoundError(f"Train íŒŒì¼ ì—†ìŒ: {train_path}")

        # Train ë°ì´í„°
        self.train_dataset = HIRADataset(
            train_path,
            self.tokenizer,
            max_length=self.config['max_length']
        )

        # Validation ë°ì´í„°
        if val_path.exists():
            # JSONì„ JSONLë¡œ ë³€í™˜
            val_jsonl = self.data_path / "val.jsonl"
            if not val_jsonl.exists():
                with open(val_path, 'r', encoding='utf-8') as f:
                    val_data = json.load(f)
                with open(val_jsonl, 'w', encoding='utf-8') as f:
                    for item in val_data:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')

            self.val_dataset = HIRADataset(
                val_jsonl,
                self.tokenizer,
                max_length=self.config['max_length']
            )
        else:
            print("  âš ï¸  Validation ë°ì´í„° ì—†ìŒ")
            self.val_dataset = None

    def train(self):
        """í•™ìŠµ ì‹¤í–‰"""
        print(f"\n[3/5] í•™ìŠµ ì‹œì‘...")

        # Training Arguments
        training_args = TrainingArguments(
            output_dir=str(self.output_path),
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
            learning_rate=self.config['learning_rate'],
            warmup_steps=self.config['warmup_steps'],
            logging_steps=self.config['logging_steps'],
            eval_strategy="steps" if self.val_dataset else "no",
            eval_steps=self.config.get('eval_steps', 100),
            save_steps=self.config.get('save_steps', 500),
            save_total_limit=3,
            fp16=torch.cuda.is_available(),
            dataloader_pin_memory=False,
            report_to="none",
            load_best_model_at_end=True if self.val_dataset else False,
            metric_for_best_model="eval_loss" if self.val_dataset else None,
        )

        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            tokenizer=self.tokenizer,
        )

        # í•™ìŠµ ì‹œì‘
        start_time = datetime.now()
        print(f"  ì‹œì‘ ì‹œê°: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        trainer.train()

        end_time = datetime.now()
        duration = end_time - start_time
        print(f"\n  âœ“ í•™ìŠµ ì™„ë£Œ!")
        print(f"  ì¢…ë£Œ ì‹œê°: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  ì†Œìš” ì‹œê°„: {duration}")

    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        print(f"\n[4/5] ëª¨ë¸ ì €ì¥ ì¤‘...")

        # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
        lora_path = self.output_path / "lora_adapter"
        self.model.save_pretrained(lora_path)
        self.tokenizer.save_pretrained(lora_path)

        print(f"  âœ“ LoRA ì–´ëŒ‘í„° ì €ì¥: {lora_path}")

        # ì„¤ì • ì €ì¥
        config_path = self.output_path / "training_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)

        print(f"  âœ“ í•™ìŠµ ì„¤ì • ì €ì¥: {config_path}")

    def evaluate(self):
        """í‰ê°€ ì‹¤í–‰"""
        print(f"\n[5/5] í‰ê°€ ì¤‘...")

        if self.val_dataset is None:
            print("  âš ï¸  Validation ë°ì´í„° ì—†ìŒ")
            return

        self.model.eval()

        val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False
        )

        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="  í‰ê°€"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                total_loss += outputs.loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        perplexity = np.exp(avg_loss)

        print(f"\n  í‰ê°€ ê²°ê³¼:")
        print(f"    Loss: {avg_loss:.4f}")
        print(f"    Perplexity: {perplexity:.2f}")

        # ê²°ê³¼ ì €ì¥
        results = {
            "val_loss": avg_loss,
            "perplexity": perplexity,
            "evaluated_at": datetime.now().isoformat()
        }

        with open(self.output_path / "eval_results.json", 'w') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="HIRA SOLAR-10.7B LoRA í•™ìŠµ")

    # ê²½ë¡œ
    parser.add_argument("--model-path", type=str,
                       default="/home/work/LLM_Meditron/bigdataAI/solar_10.7b_package/model",
                       help="SOLAR ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--data-path", type=str,
                       default="../output/v1.0/full",
                       help="ë°ì´í„° ê²½ë¡œ (train.jsonl í¬í•¨)")
    parser.add_argument("--output-path", type=str,
                       default="./trained_models/hira_solar_lora",
                       help="ì¶œë ¥ ê²½ë¡œ")

    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    parser.add_argument("--batch-size", type=int, default=2,
                       help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--gradient-accumulation-steps", type=int, default=4,
                       help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…")
    parser.add_argument("--learning-rate", type=float, default=5e-5,
                       help="í•™ìŠµë¥ ")
    parser.add_argument("--num-epochs", type=int, default=3,
                       help="ì—í­ ìˆ˜")
    parser.add_argument("--max-length", type=int, default=512,
                       help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")

    # LoRA íŒŒë¼ë¯¸í„°
    parser.add_argument("--lora-r", type=int, default=16,
                       help="LoRA rank")
    parser.add_argument("--lora-alpha", type=int, default=32,
                       help="LoRA alpha")
    parser.add_argument("--lora-dropout", type=float, default=0.05,
                       help="LoRA dropout")

    args = parser.parse_args()

    # í™˜ê²½ ì²´í¬
    if not TORCH_AVAILABLE:
        print("\nâŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   GPU í™˜ê²½ì—ì„œ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜ í›„ ì¬ì‹¤í–‰í•˜ì„¸ìš”:")
        print("   pip install torch transformers peft accelerate bitsandbytes")
        sys.exit(1)

    # ì„¤ì •
    config = {
        "model_path": args.model_path,
        "data_path": args.data_path,
        "output_path": args.output_path,
        "batch_size": args.batch_size,
        "gradient_accumulation_steps": args.gradient_accumulation_steps,
        "learning_rate": args.learning_rate,
        "num_epochs": args.num_epochs,
        "max_length": args.max_length,
        "lora_r": args.lora_r,
        "lora_alpha": args.lora_alpha,
        "lora_dropout": args.lora_dropout,
        "warmup_steps": 100,
        "logging_steps": 10,
        "eval_steps": 100,
        "save_steps": 500,
    }

    # í•™ìŠµ ì‹¤í–‰
    trainer = HIRATrainer(config)

    try:
        trainer.load_model()
        trainer.load_data()
        trainer.train()
        trainer.save_model()
        trainer.evaluate()

        print("\n" + "="*80)
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("="*80)
        print(f"\nëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {trainer.output_path}")
        print(f"\në‹¤ìŒ ë‹¨ê³„:")
        print(f"  python3 inference_interface.py --model-path {trainer.output_path}/lora_adapter")

    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
