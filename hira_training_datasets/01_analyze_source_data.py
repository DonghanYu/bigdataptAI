#!/usr/bin/env python3
"""
HIRA ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„
- í˜„ì¬ ë°ì´í„° í†µê³„
- ë©”ë‰´/ì£¼ì œë³„ ë¶„í¬
- ì§ˆë¬¸/ë‹µë³€ ê¸¸ì´ ë¶„ì„
- ë¶€ì¡±í•œ ì˜ì—­ ì‹ë³„
"""

import json
import sys
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List
import re


class HIRADataAnalyzer:
    """HIRA ë°ì´í„° ë¶„ì„ê¸°"""

    def __init__(self, source_path: str):
        """ì´ˆê¸°í™”"""
        self.source_path = Path(source_path)

        with open(self.source_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.stats = {
            "overview": {},
            "menu_distribution": {},
            "topic_distribution": {},
            "question_analysis": {},
            "answer_analysis": {},
            "pattern_analysis": {},
            "gaps": []
        }

    def analyze_all(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("="*80)
        print("HIRA ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„")
        print("="*80 + "\n")

        self.analyze_overview()
        self.analyze_menu_distribution()
        self.analyze_questions()
        self.analyze_answers()
        self.analyze_patterns()
        self.identify_gaps()

        print("\n" + "="*80)
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("="*80)

        return self.stats

    def analyze_overview(self):
        """ì „ì²´ ê°œìš” ë¶„ì„"""
        print("[1/6] ì „ì²´ ê°œìš” ë¶„ì„ ì¤‘...")

        total_menus = len(self.data.get("menu_structure", {}))
        total_topics = sum(
            menu["topics_count"]
            for menu in self.data.get("menu_structure", {}).values()
        )
        total_qa = sum(
            qa_group["qa_count"]
            for qa_group in self.data.get("core_qa", {}).values()
        )

        self.stats["overview"] = {
            "total_menus": total_menus,
            "total_topics": total_topics,
            "total_qa_pairs": total_qa,
            "avg_qa_per_menu": total_qa / total_menus if total_menus else 0,
            "avg_qa_per_topic": total_qa / total_topics if total_topics else 0
        }

        print(f"   ë©”ë‰´: {total_menus}ê°œ")
        print(f"   ì£¼ì œ: {total_topics}ê°œ")
        print(f"   Q&A: {total_qa}ìŒ")
        print(f"   ë©”ë‰´ë‹¹ í‰ê· : {self.stats['overview']['avg_qa_per_menu']:.1f}ê°œ")
        print(f"   ì£¼ì œë‹¹ í‰ê· : {self.stats['overview']['avg_qa_per_topic']:.1f}ê°œ")

    def analyze_menu_distribution(self):
        """ë©”ë‰´ë³„ ë¶„í¬ ë¶„ì„"""
        print("\n[2/6] ë©”ë‰´ë³„ ë¶„í¬ ë¶„ì„ ì¤‘...")

        menu_dist = {}

        for menu_id, qa_group in self.data.get("core_qa", {}).items():
            menu_name = self.data["menu_structure"][menu_id]["name"]
            qa_count = qa_group["qa_count"]
            topics_count = self.data["menu_structure"][menu_id]["topics_count"]

            menu_dist[menu_id] = {
                "name": menu_name,
                "qa_count": qa_count,
                "topics_count": topics_count,
                "qa_per_topic": qa_count / topics_count if topics_count else 0
            }

        self.stats["menu_distribution"] = menu_dist

        # ì •ë ¬ (Q&A ìˆ˜ ê¸°ì¤€)
        sorted_menus = sorted(
            menu_dist.items(),
            key=lambda x: x[1]["qa_count"],
            reverse=True
        )

        print(f"\n   {'ë©”ë‰´':<20} {'Q&A':<10} {'ì£¼ì œ':<10} {'ì£¼ì œë‹¹ Q&A':<15}")
        print("   " + "-"*55)

        for menu_id, info in sorted_menus:
            print(f"   {info['name']:<20} {info['qa_count']:<10} "
                  f"{info['topics_count']:<10} {info['qa_per_topic']:<15.1f}")

    def analyze_questions(self):
        """ì§ˆë¬¸ ë¶„ì„"""
        print("\n[3/6] ì§ˆë¬¸ ë¶„ì„ ì¤‘...")

        all_questions = []
        question_starts = []
        question_types = Counter()

        for qa_group in self.data.get("core_qa", {}).values():
            for qa in qa_group["qa_pairs"]:
                q = qa["question"]
                all_questions.append(q)

                # ì‹œì‘ ë‹¨ì–´
                first_word = q.split()[0] if q.split() else ""
                question_starts.append(first_word)

                # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
                if any(word in q for word in ["ì–´ë–»ê²Œ", "ë°©ë²•", "ì ˆì°¨"]):
                    question_types["how_to"] += 1
                elif any(word in q for word in ["ë­”ê°€ìš”", "ë¬´ì—‡", "ì´ë€"]):
                    question_types["what_is"] += 1
                elif any(word in q for word in ["ì–´ë””ì„œ", "ì–´ë””ì—", "ì–´ëŠ"]):
                    question_types["where"] += 1
                elif any(word in q for word in ["ì°¨ì´", "ë‹¤ë¥¸", "êµ¬ë¶„"]):
                    question_types["comparison"] += 1
                else:
                    question_types["other"] += 1

        lengths = [len(q) for q in all_questions]

        self.stats["question_analysis"] = {
            "total": len(all_questions),
            "length_min": min(lengths),
            "length_max": max(lengths),
            "length_avg": sum(lengths) / len(lengths),
            "top_starts": Counter(question_starts).most_common(10),
            "types": dict(question_types)
        }

        print(f"   ì´ ì§ˆë¬¸: {len(all_questions)}ê°œ")
        print(f"   ê¸¸ì´: ìµœì†Œ {min(lengths)}ì, ìµœëŒ€ {max(lengths)}ì, í‰ê·  {self.stats['question_analysis']['length_avg']:.1f}ì")
        print(f"\n   ì§ˆë¬¸ ìœ í˜•:")
        for qtype, count in question_types.most_common():
            pct = (count / len(all_questions)) * 100
            print(f"      {qtype}: {count}ê°œ ({pct:.1f}%)")

        print(f"\n   ìì£¼ ì‚¬ìš©ë˜ëŠ” ì‹œì‘ ë‹¨ì–´ (Top 5):")
        for word, count in self.stats["question_analysis"]["top_starts"][:5]:
            print(f"      {word}: {count}íšŒ")

    def analyze_answers(self):
        """ë‹µë³€ ë¶„ì„"""
        print("\n[4/6] ë‹µë³€ ë¶„ì„ ì¤‘...")

        all_answers = []

        for qa_group in self.data.get("core_qa", {}).values():
            for qa in qa_group["qa_pairs"]:
                all_answers.append(qa["answer"])

        lengths = [len(a) for a in all_answers]

        # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„
        has_examples = sum(1 for a in all_answers if "ì˜ˆ:" in a or "ì˜ˆë¥¼ ë“¤ì–´" in a)
        has_steps = sum(1 for a in all_answers if any(str(i) in a for i in range(1, 6)))
        has_links = sum(1 for a in all_answers if ">" in a)  # ë©”ë‰´ ê²½ë¡œ

        self.stats["answer_analysis"] = {
            "total": len(all_answers),
            "length_min": min(lengths),
            "length_max": max(lengths),
            "length_avg": sum(lengths) / len(lengths),
            "has_examples": has_examples,
            "has_steps": has_steps,
            "has_menu_paths": has_links
        }

        print(f"   ì´ ë‹µë³€: {len(all_answers)}ê°œ")
        print(f"   ê¸¸ì´: ìµœì†Œ {min(lengths)}ì, ìµœëŒ€ {max(lengths)}ì, í‰ê·  {self.stats['answer_analysis']['length_avg']:.1f}ì")
        print(f"   ì˜ˆì‹œ í¬í•¨: {has_examples}ê°œ ({has_examples/len(all_answers)*100:.1f}%)")
        print(f"   ë‹¨ê³„ ì„¤ëª…: {has_steps}ê°œ ({has_steps/len(all_answers)*100:.1f}%)")
        print(f"   ë©”ë‰´ ê²½ë¡œ: {has_links}ê°œ ({has_links/len(all_answers)*100:.1f}%)")

    def analyze_patterns(self):
        """íŒ¨í„´ ë¶„ì„"""
        print("\n[5/6] íŒ¨í„´ ë¶„ì„ ì¤‘...")

        # í‚¤ì›Œë“œ ë¹ˆë„
        all_text = []
        for qa_group in self.data.get("core_qa", {}).values():
            for qa in qa_group["qa_pairs"]:
                all_text.append(qa["question"] + " " + qa["answer"])

        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        common_words = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ì™€", "ê³¼",
                       "ë¡œ", "ìœ¼ë¡œ", "ì—ì„œ", "í•˜ë‚˜ìš”", "ë­”ê°€ìš”", "ì–´ë–»ê²Œ", "ì…ë‹ˆë‹¤",
                       "ìˆìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "í• ", "ìˆ˜", "ìˆëŠ”", "ëŒ€í•œ"]

        for text in all_text:
            words = re.findall(r'[ê°€-í£]+', text)
            keywords.extend([w for w in words if len(w) >= 2 and w not in common_words])

        keyword_freq = Counter(keywords).most_common(30)

        self.stats["pattern_analysis"] = {
            "top_keywords": keyword_freq[:20]
        }

        print(f"   ì£¼ìš” í‚¤ì›Œë“œ (Top 10):")
        for word, count in keyword_freq[:10]:
            print(f"      {word}: {count}íšŒ")

    def identify_gaps(self):
        """ë¶€ì¡±í•œ ì˜ì—­ ì‹ë³„"""
        print("\n[6/6] ë¶€ì¡±í•œ ì˜ì—­ ì‹ë³„ ì¤‘...")

        gaps = []

        # ë©”ë‰´ë³„ Q&A ìˆ˜ í™•ì¸
        for menu_id, info in self.stats["menu_distribution"].items():
            if info["qa_count"] < 50:
                gaps.append({
                    "type": "low_menu_coverage",
                    "menu": info["name"],
                    "current": info["qa_count"],
                    "recommended": 100,
                    "priority": "high"
                })
            elif info["qa_count"] < 80:
                gaps.append({
                    "type": "medium_menu_coverage",
                    "menu": info["name"],
                    "current": info["qa_count"],
                    "recommended": 150,
                    "priority": "medium"
                })

        # ì§ˆë¬¸ ìœ í˜• ë¶ˆê· í˜• í™•ì¸
        qtypes = self.stats["question_analysis"]["types"]
        total_q = sum(qtypes.values())

        for qtype, count in qtypes.items():
            ratio = count / total_q
            if ratio < 0.1:  # 10% ë¯¸ë§Œ
                gaps.append({
                    "type": "low_question_type",
                    "question_type": qtype,
                    "current": count,
                    "current_ratio": f"{ratio*100:.1f}%",
                    "recommended": int(total_q * 0.15),
                    "priority": "low"
                })

        self.stats["gaps"] = gaps

        if gaps:
            print(f"   ë°œê²¬ëœ ë¶€ì¡± ì˜ì—­: {len(gaps)}ê°œ")
            print(f"\n   {'ìœ í˜•':<20} {'ì˜ì—­':<20} {'í˜„ì¬':<10} {'ê¶Œì¥':<10} {'ìš°ì„ ìˆœìœ„':<10}")
            print("   " + "-"*70)
            for gap in gaps[:10]:
                area = gap.get("menu") or gap.get("question_type", "N/A")
                print(f"   {gap['type']:<20} {area:<20} {gap['current']:<10} "
                      f"{gap['recommended']:<10} {gap['priority']:<10}")
        else:
            print("   ë¶€ì¡±í•œ ì˜ì—­ ì—†ìŒ (ê· í˜• ì¡íŒ ë°ì´í„°)")

    def save_report(self, output_path: str):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ë¶„ì„ ìš”ì•½")
        print("="*80)

        print(f"\nâœ… í˜„ì¬ ë°ì´í„°:")
        print(f"   - ì´ Q&A: {self.stats['overview']['total_qa_pairs']}ìŒ")
        print(f"   - ë©”ë‰´: {self.stats['overview']['total_menus']}ê°œ")
        print(f"   - ì£¼ì œ: {self.stats['overview']['total_topics']}ê°œ")

        print(f"\nğŸ“ˆ ì¦ê°• ëª©í‘œ (Option B):")
        print(f"   - ëª©í‘œ Q&A: 5,000ìŒ")
        print(f"   - í•„ìš” ì¦ê°•: {5000 - self.stats['overview']['total_qa_pairs']}ìŒ")
        print(f"   - ì¦ê°• ë°°ìœ¨: {5000 / self.stats['overview']['total_qa_pairs']:.1f}x")

        print(f"\nâš ï¸ ì£¼ì˜ ì˜ì—­:")
        high_priority = [g for g in self.stats["gaps"] if g["priority"] == "high"]
        if high_priority:
            for gap in high_priority[:3]:
                area = gap.get("menu") or gap.get("question_type", "")
                print(f"   - {area}: {gap['current']}ê°œ â†’ {gap['recommended']}ê°œ ê¶Œì¥")
        else:
            print(f"   - ì—†ìŒ (ê· í˜• ì¡íŒ ë°ì´í„°)")

        print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. ê·œì¹™ ê¸°ë°˜ ì¦ê°•: 323 â†’ 2,000ê°œ (6ë°°)")
        print(f"   2. í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±: 2,000 â†’ 5,000ê°œ (2.5ë°°)")
        print(f"   3. í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("HIRA ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„ê¸° v1.0")
    print("="*80 + "\n")

    # ì†ŒìŠ¤ ë°ì´í„° ê²½ë¡œ
    source_path = Path(__file__).parent / "source_data" / "hira_source.json"

    if not source_path.exists():
        print(f"âŒ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}")
        print(f"\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì„¸ìš”:")
        print(f"cp ../hira_crawler/output/hira_data_from_yaml_*.json source_data/hira_source.json")
        sys.exit(1)

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = HIRADataAnalyzer(source_path)

    # ë¶„ì„ ì‹¤í–‰
    stats = analyzer.analyze_all()

    # ë¦¬í¬íŠ¸ ì €ì¥
    output_dir = Path(__file__).parent / "output" / "v1.0" / "metadata"
    analyzer.save_report(output_dir / "source_analysis.json")

    # ìš”ì•½ ì¶œë ¥
    analyzer.print_summary()

    print("\n" + "="*80)
    print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()
