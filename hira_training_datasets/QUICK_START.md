# HIRA 학습 데이터셋 개발 - 빠른 시작 가이드

## 🎯 핵심 요약

### 현재 → 목표
```
323개 Q&A (수동 큐레이션)
    ↓
5,000개 Q&A (규칙+템플릿)
    ↓
10,000개 Q&A (GPT 추가, 선택)
```

### 3단계 증강 전략

```
┌─────────────────────────────────────────────────────────┐
│  Phase 1: 규칙 기반 변형 (Rule-Based)                   │
│  323 → 2,000개 (6배)                                    │
│  - 어미 변형, 조사 변경, 동의어 치환                     │
│  - 비용: $0, 시간: 2일                                  │
└─────────────────────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────────────────────┐
│  Phase 2: 템플릿 기반 생성 (Template-Based)             │
│  2,000 → 5,000개 (2.5배)                                │
│  - 주제별 질문 패턴, 답변 템플릿 매칭                   │
│  - 비용: $0, 시간: 2일                                  │
└─────────────────────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────────────────────┐
│  Phase 3: GPT 기반 증강 (GPT-Based, 선택적)             │
│  5,000 → 10,000개 (2배)                                 │
│  - LLM을 활용한 고품질 변형 생성                        │
│  - 비용: $7-17, 시간: 1일                               │
└─────────────────────────────────────────────────────────┘
```

## 📊 메뉴별 타겟 분포

```
서비스 소개          ██████████████████ 1,500개 (15%)
보건의료빅데이터    ████████████████████████████████ 3,000개 (30%)
의료통계정보        ████████████████████ 2,000개 (20%)
공공데이터          ██████████████████ 1,500개 (15%)
고객지원            ████████████████████ 2,000개 (20%)
                                          ────────
                                          10,000개
```

## 🔧 주요 변형 기법

### 1. 규칙 기반 (Rule-Based)
```python
원본: "상병코드는 어떻게 조회하나요?"

변형 1 (어미): "상병코드는 어떻게 조회해요?"
변형 2 (형식): "상병코드 조회 방법은?"
변형 3 (동의어): "상병코드는 어떻게 검색하나요?"
변형 4 (조사): "상병코드를 어떻게 조회하나요?"
변형 5 (축약): "상병코드 조회 방법"
변형 6 (확장): "HIRA 상병코드는 어디서 어떻게 조회하나요?"
```

### 2. 템플릿 기반 (Template-Based)
```yaml
주제: "데이터 신청"
키워드: ["신청", "제출", "승인"]

템플릿:
  - "{주제} 어떻게 하나요?" → "데이터 신청 어떻게 하나요?"
  - "{주제} 절차를 알려주세요" → "데이터 신청 절차를 알려주세요"
  - "{키워드}은 어디서 하나요?" → "신청은 어디서 하나요?"
```

### 3. GPT 기반 (GPT-Based)
```
프롬프트: "다음 질문의 의도를 유지하면서 5가지 다른 표현으로 바꿔주세요"
원본: "환자표본 데이터는 어떻게 신청하나요?"

GPT 생성:
1. "환자표본 자료 신청 방법 알려주세요"
2. "환자 샘플 데이터 받으려면 어떻게 해야 하나요?"
3. "환자표본 데이터셋 어디서 요청하나요?"
4. "환자표본DB 이용 신청 절차는?"
5. "환자 표본 정보 신청하는 곳이 어디인가요?"
```

## 📁 출력 구조

```
hira_training_datasets/
├── v1.0/
│   ├── full/
│   │   ├── train.jsonl          # LoRA 학습용 (8,000개)
│   │   ├── val.json             # 검증용 (1,000개)
│   │   └── test.json            # 테스트용 (1,000개)
│   │
│   ├── by_menu/
│   │   ├── service_intro.json   # 메뉴별 분할
│   │   ├── healthcare_bigdata.json
│   │   └── ...
│   │
│   ├── metadata/
│   │   ├── statistics.json      # 통계 정보
│   │   ├── quality_report.json  # 품질 보고서
│   │   └── generation_log.json  # 생성 이력
│   │
│   └── docs/
│       ├── README.md
│       └── USAGE_GUIDE.md
```

## ⚡ 빠른 실행

### Step 1: 환경 준비
```bash
cd /home/user/bigdataptAI/hira_training_datasets
pip install pyyaml tqdm pandas matplotlib
```

### Step 2: 데이터 분석
```bash
python3 01_analyze_source_data.py
# 출력: 현재 데이터 통계, 주제 분포, 부족한 영역 식별
```

### Step 3: 규칙 기반 증강
```bash
python3 02_rule_based_augment.py --multiplier 6
# 323개 → 약 2,000개
```

### Step 4: 템플릿 기반 생성
```bash
python3 03_template_based_generate.py --target 5000
# 2,000개 → 5,000개
```

### Step 5: 품질 검증
```bash
python3 04_quality_check.py --input output/augmented_data.json
# 중복 제거, 품질 필터링, 통계 생성
```

### Step 6: 최종 분할
```bash
python3 05_split_dataset.py --train 0.8 --val 0.1 --test 0.1
# Train/Val/Test 분할
```

## 📊 예상 결과

### 데이터 품질 지표
| 지표 | 목표 | 예상 |
|------|------|------|
| 총 데이터 수 | 5,000+ | 5,000-10,000 |
| 중복률 | < 5% | 2-3% |
| 평균 품질 점수 | > 0.85 | 0.87-0.92 |
| 다양성 점수 | > 0.80 | 0.82-0.88 |
| 주제 커버리지 | 100% | 100% |

### 주제별 밀도 (목표)
```
고밀도 (500+ QA):
  - 데이터 신청/요청
  - 환자표본 데이터
  - 통계 정보 조회

중밀도 (200-500 QA):
  - 코드 조회
  - API 사용
  - 교육 자료

저밀도 (100-200 QA):
  - 시스템 오류
  - 로그인 문제
  - 기타 지원
```

## 🎯 3가지 옵션 비교

| | Option A<br>프로토타입 | Option B<br>완전판 | Option C<br>프리미엄 |
|---|---|---|---|
| **규모** | 2,000개 | 5,000개 | 10,000개 |
| **방법** | 규칙만 | 규칙+템플릿 | 규칙+템플릿+GPT |
| **시간** | 2-3일 | 7-10일 | 10-14일 |
| **비용** | $0 | $0 | $7-17 |
| **품질** | 중 | 상 | 최상 |
| **추천** | 빠른 검증 | ✅ **권장** | 최고 품질 |

## 💡 추천 로드맵

### Week 1: 기반 구축 (Option B)
- Day 1-2: 데이터 분석 + 규칙 기반 증강 구현
- Day 3-4: 템플릿 기반 생성 구현
- Day 5: 품질 검증 시스템 구축

### Week 2: 생성 및 검증
- Day 6-7: 전체 데이터 생성 (5,000개)
- Day 8-9: 품질 검증 및 필터링
- Day 10: 최종 분할 및 문서화

### (선택) Week 3: GPT 증강 (Option C)
- Day 11-12: GPT API 연동 및 증강
- Day 13: 추가 품질 검증
- Day 14: 최종 배포 준비

## 🚀 바로 시작하기

```bash
# 1. 프로젝트 폴더 이동
cd /home/user/bigdataptAI/hira_training_datasets

# 2. 소스 데이터 복사
cp ../hira_crawler/output/hira_data_from_yaml_*.json ./source_data.json

# 3. 첫 번째 스크립트 실행
python3 01_analyze_source_data.py

# 계획안 확인
cat DATASET_DEVELOPMENT_PLAN.md
```

---

**준비되셨나요?** 어떤 옵션으로 시작하시겠습니까?

- **A**: 빠른 프로토타입 (2-3일, 2,000개)
- **B**: 완전한 데이터셋 (7-10일, 5,000개) ⭐ 추천
- **C**: 프리미엄 데이터셋 (10-14일, 10,000개)

입력: ____
