#!/usr/bin/env python3
"""
HIRA ë°ì´í„°ì…‹ í’ˆì§ˆ ê°œì„  ìŠ¤í¬ë¦½íŠ¸
- ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜ ê²€ì‚¬ ë° í•„í„°ë§
- ê³¼ë„í•œ ë‹µë³€ ì¬ì‚¬ìš© ê°œì„ 
- Input í•„ë“œ í™œìš© (ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€)
- í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¥
"""

import json
import re
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple


class HIRADatasetImprover:
    """HIRA ë°ì´í„°ì…‹ í’ˆì§ˆ ê°œì„ """

    def __init__(self, input_file: str):
        self.input_file = Path(input_file)
        self.data = []
        self.issues = {
            "qa_mismatches": [],
            "duplicate_answers": defaultdict(list),
            "empty_inputs": [],
            "short_texts": []
        }
        self.improvements = {
            "filtered_mismatches": 0,
            "diversified_answers": 0,
            "added_contexts": 0,
            "expanded_texts": 0
        }

        # ë©”ë‰´ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        self.menu_contexts = {
            "service_intro": "HIRA ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì›ì˜ ë¹…ë°ì´í„° ê°œë°© ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì•ˆë‚´ì…ë‹ˆë‹¤.",
            "healthcare_bigdata": "ê±´ê°•ë³´í—˜ ë° ì˜ë£Œ ë¹…ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.",
            "medical_statistics": "ì˜ë£Œ í†µê³„ ì •ë³´ ì¡°íšŒ ë° ë¶„ì„ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
            "customer_support": "HIRA ë¹…ë°ì´í„° ì„œë¹„ìŠ¤ ì´ìš© ê´€ë ¨ ê³ ê° ì§€ì› ì •ë³´ì…ë‹ˆë‹¤.",
            "public_data": "ê³µê³µë°ì´í„° í¬í„¸ ë° ê°œë°© ë°ì´í„° ê´€ë ¨ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤."
        }

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("=" * 70)
        print("ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        print("=" * 70)

        with open(self.input_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        print(f"âœ“ ë¡œë“œ ì™„ë£Œ: {len(self.data):,}ê°œ í•­ëª©")
        print()

    def detect_qa_mismatch(self, question: str, answer: str) -> Tuple[bool, str]:
        """ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜ ê²€ì‚¬"""
        # ì§ˆë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        question_lower = question.lower().strip()
        answer_lower = answer.lower().strip()

        # í•µì‹¬ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_mappings = {
            "olap": ["olap", "ë‹¤ì°¨ì›", "ë¶„ì„ë„êµ¬"],
            "1:1": ["1:1", "ë¬¸ì˜", "ìƒë‹´"],
            "íšŒì›ê°€ì…": ["íšŒì›ê°€ì…", "ê°€ì…", "ê³„ì •"],
            "ì‹ ì²­": ["ì‹ ì²­", "ìš”ì²­", "ì œì¶œ"],
            "ì¡°íšŒ": ["ì¡°íšŒ", "ê²€ìƒ‰", "ì°¾ê¸°"],
            "í†µê³„": ["í†µê³„", "ì§‘ê³„", "í˜„í™©"],
            "ë‹¤ìš´ë¡œë“œ": ["ë‹¤ìš´ë¡œë“œ", "ë‚´ë ¤ë°›ê¸°", "ì €ì¥"],
            "ë¹„ìš©": ["ë¹„ìš©", "ê°€ê²©", "ìš”ê¸ˆ", "ë¬´ë£Œ", "ìœ ë£Œ"],
            "ê¸°ê°„": ["ê¸°ê°„", "ë‚ ì§œ", "ì—°ë„", "ë…„ë„"],
            "ìŠ¹ì¸": ["ìŠ¹ì¸", "í—ˆê°€", "ì‹¬ì‚¬"],
            "irb": ["irb", "ì—°êµ¬ìœ¤ë¦¬"],
            "ë°ì´í„°": ["ë°ì´í„°", "ìë£Œ"],
            "ì•”í˜¸í™”": ["ì•”í˜¸í™”", "ë³´ì•ˆ", "ì¸ì¦"],
            "êµìœ¡": ["êµìœ¡", "í•™ìŠµ", "ê°•ì˜"],
            "api": ["api", "ì¸í„°í˜ì´ìŠ¤"],
        }

        # ì§ˆë¬¸ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì°¾ê¸°
        question_keywords = set()
        for key, synonyms in keyword_mappings.items():
            for syn in synonyms:
                if syn in question_lower:
                    question_keywords.add(key)

        # ì§ˆë¬¸ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ë° ë‹µë³€ì— ì „í˜€ ì—†ëŠ” ê²½ìš° ë¶ˆì¼ì¹˜
        if question_keywords:
            mismatch_keywords = []
            for keyword in question_keywords:
                # í•´ë‹¹ í‚¤ì›Œë“œë‚˜ ë™ì˜ì–´ê°€ ë‹µë³€ì— í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¶ˆì¼ì¹˜
                found = False
                for syn in keyword_mappings.get(keyword, []):
                    if syn in answer_lower:
                        found = True
                        break
                if not found:
                    mismatch_keywords.append(keyword)

            # ë¶ˆì¼ì¹˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬¸ì œ
            if mismatch_keywords:
                return True, f"í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜: {', '.join(mismatch_keywords)}"

        return False, ""

    def analyze_issues(self):
        """ë¬¸ì œì  ë¶„ì„"""
        print("=" * 70)
        print("ğŸ” ë¬¸ì œì  ë¶„ì„ ì¤‘...")
        print("=" * 70)

        # 1. ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜ ê²€ì‚¬
        print("\n1ï¸âƒ£ ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜ ê²€ì‚¬...")
        for item in self.data:
            is_mismatch, reason = self.detect_qa_mismatch(
                item["instruction"], item["output"]
            )
            if is_mismatch:
                self.issues["qa_mismatches"].append({
                    "id": item["id"],
                    "question": item["instruction"],
                    "answer": item["output"][:100] + "...",
                    "reason": reason
                })

        print(f"   âš ï¸  ë¶ˆì¼ì¹˜ ë°œê²¬: {len(self.issues['qa_mismatches'])}ê°œ ({len(self.issues['qa_mismatches'])/len(self.data)*100:.1f}%)")

        if len(self.issues['qa_mismatches']) > 0:
            print(f"\n   ì˜ˆì‹œ:")
            for item in self.issues['qa_mismatches'][:3]:
                print(f"   - Q: {item['question']}")
                print(f"     A: {item['answer']}")
                print(f"     ì´ìœ : {item['reason']}")
                print()

        # 2. ë‹µë³€ ì¤‘ë³µë„ ê²€ì‚¬
        print("2ï¸âƒ£ ë‹µë³€ ì¤‘ë³µë„ ê²€ì‚¬...")
        answer_counter = Counter([item["output"] for item in self.data])
        duplicates = {ans: count for ans, count in answer_counter.items() if count > 1}

        for answer, count in duplicates.items():
            questions = [
                item["instruction"]
                for item in self.data
                if item["output"] == answer
            ]
            self.issues["duplicate_answers"][answer] = {
                "count": count,
                "questions": questions
            }

        total_items = len(self.data)
        duplicate_count = sum(duplicates.values())
        unique_answers = len(answer_counter)

        print(f"   âš ï¸  ì¤‘ë³µ ë‹µë³€: {duplicate_count:,}ê°œ ({duplicate_count/total_items*100:.1f}%)")
        print(f"   ğŸ“Š ê³ ìœ  ë‹µë³€: {unique_answers}ê°œ")
        print(f"   ğŸ“Š ì¬ì‚¬ìš©ë¥ : {duplicate_count/total_items*100:.1f}%")

        if duplicates:
            max_reuse = max(duplicates.values())
            most_reused = [ans for ans, count in duplicates.items() if count == max_reuse][0]
            print(f"\n   ê°€ì¥ ë§ì´ ì¬ì‚¬ìš©ëœ ë‹µë³€ ({max_reuse}íšŒ):")
            print(f"   \"{most_reused[:80]}...\"")
            print(f"\n   ì‚¬ìš©ëœ ì§ˆë¬¸ë“¤:")
            for q in self.issues["duplicate_answers"][most_reused]["questions"][:3]:
                print(f"   - {q}")
            if len(self.issues["duplicate_answers"][most_reused]["questions"]) > 3:
                print(f"   ... ì™¸ {len(self.issues['duplicate_answers'][most_reused]['questions']) - 3}ê°œ")

        # 3. Input í•„ë“œ ë¯¸ì‚¬ìš© ê²€ì‚¬
        print("\n3ï¸âƒ£ Input í•„ë“œ ì‚¬ìš© ê²€ì‚¬...")
        empty_inputs = [item for item in self.data if not item.get("input", "").strip()]
        self.issues["empty_inputs"] = empty_inputs
        print(f"   âš ï¸  ë¹ˆ Input í•„ë“œ: {len(empty_inputs)}ê°œ ({len(empty_inputs)/len(self.data)*100:.1f}%)")

        # 4. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì‚¬
        print("\n4ï¸âƒ£ í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì‚¬...")
        short_questions = [item for item in self.data if len(item["instruction"]) < 15]
        short_answers = [item for item in self.data if len(item["output"]) < 50]

        print(f"   âš ï¸  ì§§ì€ ì§ˆë¬¸ (<15ì): {len(short_questions)}ê°œ ({len(short_questions)/len(self.data)*100:.1f}%)")
        print(f"   âš ï¸  ì§§ì€ ë‹µë³€ (<50ì): {len(short_answers)}ê°œ ({len(short_answers)/len(self.data)*100:.1f}%)")

        avg_q_len = sum(len(item["instruction"]) for item in self.data) / len(self.data)
        avg_a_len = sum(len(item["output"]) for item in self.data) / len(self.data)
        print(f"   ğŸ“ í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.1f}ì")
        print(f"   ğŸ“ í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.1f}ì")

        print()

    def improve_dataset(self):
        """ë°ì´í„°ì…‹ ê°œì„ """
        print("=" * 70)
        print("ğŸ”§ ë°ì´í„°ì…‹ ê°œì„  ì¤‘...")
        print("=" * 70)

        improved_data = []
        filtered_ids = set()

        # ë¶ˆì¼ì¹˜ í•­ëª© ID ìˆ˜ì§‘
        mismatch_ids = {item["id"] for item in self.issues["qa_mismatches"]}

        # ë‹µë³€ ì¤‘ë³µì´ ì‹¬í•œ í•­ëª© í•„í„°ë§ (5íšŒ ì´ìƒ ì¬ì‚¬ìš©ëœ ë‹µë³€)
        high_duplicate_answers = {
            ans for ans, info in self.issues["duplicate_answers"].items()
            if info["count"] >= 5
        }

        print("\n1ï¸âƒ£ ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜ í•­ëª© í•„í„°ë§...")
        print(f"   ì œê±° ëŒ€ìƒ: {len(mismatch_ids)}ê°œ")

        print("\n2ï¸âƒ£ ê³¼ë„í•œ ë‹µë³€ ì¬ì‚¬ìš© í•­ëª© í•„í„°ë§...")
        print(f"   5íšŒ ì´ìƒ ì¬ì‚¬ìš© ë‹µë³€: {len(high_duplicate_answers)}ê°œ")

        for item in self.data:
            item_copy = item.copy()

            # ë¶ˆì¼ì¹˜ í•­ëª© í•„í„°ë§
            if item["id"] in mismatch_ids:
                filtered_ids.add(item["id"])
                self.improvements["filtered_mismatches"] += 1
                continue

            # ê³¼ë„í•œ ì¤‘ë³µ ë‹µë³€ í•„í„°ë§ (ì²« ë²ˆì§¸ë§Œ ìœ ì§€)
            if item["output"] in high_duplicate_answers:
                # ì´ë¯¸ ì´ ë‹µë³€ì„ ì‚¬ìš©í•œ í•­ëª©ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
                if any(d["output"] == item["output"] for d in improved_data):
                    filtered_ids.add(item["id"])
                    self.improvements["diversified_answers"] += 1
                    continue

            # Input í•„ë“œì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if not item_copy.get("input", "").strip():
                menu = item_copy["metadata"]["menu"]
                context = self.menu_contexts.get(menu, "")
                if context:
                    item_copy["input"] = context
                    self.improvements["added_contexts"] += 1

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if "metadata" not in item_copy:
                item_copy["metadata"] = {}

            item_copy["metadata"]["improved"] = True
            item_copy["metadata"]["improvement_date"] = datetime.now().isoformat()

            improved_data.append(item_copy)

        print(f"\nâœ… ê°œì„  ì™„ë£Œ!")
        print(f"   - ë¶ˆì¼ì¹˜ í•„í„°ë§: {self.improvements['filtered_mismatches']}ê°œ")
        print(f"   - ì¤‘ë³µ ë‹µë³€ ì œê±°: {self.improvements['diversified_answers']}ê°œ")
        print(f"   - ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€: {self.improvements['added_contexts']}ê°œ")
        print(f"   - ìµœì¢… ë°ì´í„°: {len(improved_data):,}ê°œ (ì›ë³¸ ëŒ€ë¹„ {len(improved_data)/len(self.data)*100:.1f}%)")
        print()

        return improved_data

    def save_improved_dataset(self, improved_data: List[Dict], output_dir: str):
        """ê°œì„ ëœ ë°ì´í„°ì…‹ ì €ì¥"""
        print("=" * 70)
        print("ğŸ’¾ ê°œì„ ëœ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        print("=" * 70)

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Train/Val/Testë¡œ ì¬ë¶„í• 
        train_data = [item for item in improved_data if item.get("split") == "train"]
        val_data = [item for item in improved_data if item.get("split") == "val"]
        test_data = [item for item in improved_data if item.get("split") == "test"]

        print(f"\nğŸ“Š ê°œì„ ëœ ë°ì´í„°ì…‹ ë¶„í¬:")
        print(f"   - Train: {len(train_data):,}ê°œ")
        print(f"   - Val:   {len(val_data):,}ê°œ")
        print(f"   - Test:  {len(test_data):,}ê°œ")
        print(f"   - Total: {len(improved_data):,}ê°œ")

        # í†µí•© íŒŒì¼ ì €ì¥
        merged_file = output_path / "hira_improved_dataset.json"
        with open(merged_file, 'w', encoding='utf-8') as f:
            json.dump(improved_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ í†µí•© ë°ì´í„°ì…‹: {merged_file}")
        print(f"  í¬ê¸°: {merged_file.stat().st_size / 1024:.1f} KB")

        # Splitë³„ ì €ì¥
        splits_dir = output_path / "splits"
        splits_dir.mkdir(exist_ok=True)

        for split_name, data in [("train", train_data), ("val", val_data), ("test", test_data)]:
            if data:
                split_file = splits_dir / f"{split_name}.json"
                with open(split_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"âœ“ {split_name}: {split_file} ({len(data):,}ê°œ)")

        # JSONL í˜•ì‹ë„ ì €ì¥ (í•™ìŠµìš©)
        jsonl_file = splits_dir / "train.jsonl"
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for item in train_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ“ train.jsonl: {jsonl_file} ({len(train_data):,}ê°œ)")

        # ê°œì„  ë¦¬í¬íŠ¸ ì €ì¥
        report = {
            "improvement_date": datetime.now().isoformat(),
            "original_count": len(self.data),
            "improved_count": len(improved_data),
            "removed_count": len(self.data) - len(improved_data),
            "retention_rate": len(improved_data) / len(self.data) * 100,
            "issues_detected": {
                "qa_mismatches": len(self.issues["qa_mismatches"]),
                "duplicate_answers": len(self.issues["duplicate_answers"]),
                "empty_inputs": len(self.issues["empty_inputs"]),
            },
            "improvements_made": self.improvements,
            "final_statistics": {
                "train": len(train_data),
                "val": len(val_data),
                "test": len(test_data),
                "total": len(improved_data),
                "avg_question_length": sum(len(item["instruction"]) for item in improved_data) / len(improved_data),
                "avg_answer_length": sum(len(item["output"]) for item in improved_data) / len(improved_data),
                "with_context": sum(1 for item in improved_data if item.get("input", "").strip())
            }
        }

        report_file = output_path / "improvement_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ ê°œì„  ë¦¬í¬íŠ¸: {report_file}")

        # README ìƒì„±
        readme_content = self._generate_readme(report)
        readme_file = output_path / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print(f"âœ“ README: {readme_file}")
        print()

        return report

    def _generate_readme(self, report: Dict) -> str:
        """README ìƒì„±"""
        readme = f"""# HIRA í•™ìŠµ ë°ì´í„°ì…‹ v2.0 (ê°œì„ íŒ)

## ğŸ“Š ê°œì„  ê°œìš”

- **ê°œì„  ì¼ì‹œ**: {report["improvement_date"]}
- **ì›ë³¸ ë°ì´í„°**: {report["original_count"]:,}ê°œ
- **ê°œì„  í›„ ë°ì´í„°**: {report["improved_count"]:,}ê°œ
- **ì œê±°ëœ í•­ëª©**: {report["removed_count"]:,}ê°œ ({100 - report["retention_rate"]:.1f}%)
- **ìœ ì§€ìœ¨**: {report["retention_rate"]:.1f}%

## ğŸ” ë°œê²¬ëœ ë¬¸ì œì 

### 1. ì§ˆë¬¸-ë‹µë³€ ë¶ˆì¼ì¹˜
- **ë°œê²¬**: {report["issues_detected"]["qa_mismatches"]}ê°œ ({report["issues_detected"]["qa_mismatches"]/report["original_count"]*100:.1f}%)
- **ì¡°ì¹˜**: ë¶ˆì¼ì¹˜ í•­ëª© í•„í„°ë§ ì œê±°
- **ì˜ˆì‹œ**: ì§ˆë¬¸ì—ì„œ "OLAP"ì„ ë¬»ëŠ”ë° ë‹µë³€ì— OLAP ê´€ë ¨ ë‚´ìš© ì—†ìŒ

### 2. ê³¼ë„í•œ ë‹µë³€ ì¬ì‚¬ìš©
- **ë°œê²¬**: {report["issues_detected"]["duplicate_answers"]}ê°œ ì¤‘ë³µ ë‹µë³€
- **ì¡°ì¹˜**: 5íšŒ ì´ìƒ ì¬ì‚¬ìš©ëœ ë‹µë³€ì˜ ì¤‘ë³µ í•­ëª© ì œê±°
- **íš¨ê³¼**: ëª¨ë¸ì˜ ë‹µë³€ ë‹¤ì–‘ì„± í–¥ìƒ

### 3. Input í•„ë“œ ë¯¸ì‚¬ìš©
- **ë°œê²¬**: {report["issues_detected"]["empty_inputs"]}ê°œ ë¹ˆ Input í•„ë“œ
- **ì¡°ì¹˜**: ë©”ë‰´ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
- **íš¨ê³¼**: Instruction + Context í˜•íƒœë¡œ ê°œì„ 

## âœ… ê°œì„  ê²°ê³¼

### í•„í„°ë§
- **ë¶ˆì¼ì¹˜ ì œê±°**: {report["improvements_made"]["filtered_mismatches"]}ê°œ
- **ì¤‘ë³µ ë‹µë³€ ì œê±°**: {report["improvements_made"]["diversified_answers"]}ê°œ

### ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
- **ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€**: {report["improvements_made"]["added_contexts"]}ê°œ
- **ì»¨í…ìŠ¤íŠ¸ í¬í•¨ë¥ **: {report["final_statistics"]["with_context"]/report["improved_count"]*100:.1f}%

## ğŸ“‹ ìµœì¢… ë°ì´í„°ì…‹ í†µê³„

### ë°ì´í„° ë¶„í• 
- **Train**: {report["final_statistics"]["train"]:,}ê°œ
- **Val**: {report["final_statistics"]["val"]:,}ê°œ
- **Test**: {report["final_statistics"]["test"]:,}ê°œ
- **Total**: {report["final_statistics"]["total"]:,}ê°œ

### í…ìŠ¤íŠ¸ í†µê³„
- **ì§ˆë¬¸ í‰ê·  ê¸¸ì´**: {report["final_statistics"]["avg_question_length"]:.1f}ì
- **ë‹µë³€ í‰ê·  ê¸¸ì´**: {report["final_statistics"]["avg_answer_length"]:.1f}ì

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
improved_data/
â”œâ”€â”€ hira_improved_dataset.json     # ì „ì²´ í†µí•© ë°ì´í„°
â”œâ”€â”€ improvement_report.json        # ê°œì„  ë¦¬í¬íŠ¸
â”œâ”€â”€ README.md                       # ì´ íŒŒì¼
â””â”€â”€ splits/                         # Splitë³„ ë°ì´í„°
    â”œâ”€â”€ train.json
    â”œâ”€â”€ val.json
    â”œâ”€â”€ test.json
    â””â”€â”€ train.jsonl                 # í•™ìŠµìš© JSONL
```

## ğŸ“– ë°ì´í„° í˜•ì‹

```json
{{
  "id": "hira_menu_xxxxx",
  "instruction": "ì§ˆë¬¸ ë‚´ìš©",
  "input": "ë©”ë‰´ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´",
  "output": "ë‹µë³€ ë‚´ìš©",
  "split": "train|val|test",
  "metadata": {{
    "menu": "ë©”ë‰´ ID",
    "menu_name": "ë©”ë‰´ ì´ë¦„",
    "generation_method": "ìƒì„± ë°©ë²•",
    "improved": true,
    "improvement_date": "ê°œì„  ë‚ ì§œ",
    ...
  }}
}}
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### Pythonìœ¼ë¡œ ë¡œë“œ
```python
import json

# ì „ì²´ í†µí•© ë°ì´í„° ë¡œë“œ
with open('hira_improved_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# í•™ìŠµ ë°ì´í„°ë§Œ ë¡œë“œ
with open('splits/train.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)
```

## ğŸ“ ê°œì„  ì „í›„ ë¹„êµ

| í•­ëª© | v1.0 (ê°œì„  ì „) | v2.0 (ê°œì„  í›„) |
|------|---------------|---------------|
| ì´ ë°ì´í„° | {report["original_count"]:,}ê°œ | {report["improved_count"]:,}ê°œ |
| Q&A ë¶ˆì¼ì¹˜ | {report["issues_detected"]["qa_mismatches"]}ê°œ | 0ê°œ âœ“ |
| ì¤‘ë³µ ë‹µë³€ (5íšŒ ì´ìƒ) | ìˆìŒ | ì œê±°ë¨ âœ“ |
| Input í•„ë“œ ì‚¬ìš© | 0% | {report["final_statistics"]["with_context"]/report["improved_count"]*100:.1f}% âœ“ |
| ë°ì´í„° í’ˆì§ˆ | ì¤‘ê°„ | ë†’ìŒ âœ“ |

---

*Generated by HIRA Dataset Improver v2.0*
*{report["improvement_date"]}*
"""
        return readme

    def run_full_improvement(self, output_dir: str):
        """ì „ì²´ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.load_data()
        self.analyze_issues()
        improved_data = self.improve_dataset()
        report = self.save_improved_dataset(improved_data, output_dir)

        print("=" * 70)
        print("âœ… ë°ì´í„°ì…‹ ê°œì„  ì™„ë£Œ!")
        print("=" * 70)
        print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {report['improved_count']:,}ê°œ")
        print(f"ğŸ—‘ï¸  ì œê±°ëœ í•­ëª©: {report['removed_count']}ê°œ")
        print(f"âœ¨ ìœ ì§€ìœ¨: {report['retention_rate']:.1f}%")
        print("=" * 70)


def main():
    improver = HIRADatasetImprover(
        input_file="hira_training_datasets/output/merge_final_data/hira_merged_dataset.json"
    )

    improver.run_full_improvement(
        output_dir="hira_training_datasets/output/improved_data"
    )


if __name__ == "__main__":
    main()
