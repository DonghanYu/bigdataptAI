#!/usr/bin/env python3
"""
HIRA í•™ìŠµ ë°ì´í„°ì…‹ ì ê²€ ë° í†µí•© ìŠ¤í¬ë¦½íŠ¸
- Train/Val/Test ë°ì´í„° í’ˆì§ˆ ì ê²€
- ì¤‘ë³µ ì œê±° ë° ê²€ì¦
- í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ í†µí•©
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Any


class HIRADatasetInspector:
    """HIRA ë°ì´í„°ì…‹ ì ê²€ ë° í†µí•©"""

    def __init__(self, base_path: str = "output/v1.0/full"):
        self.base_path = Path(base_path)
        self.train_data = []
        self.val_data = []
        self.test_data = []
        self.all_data = []

        self.report = {
            "inspection_time": datetime.now().isoformat(),
            "files_processed": [],
            "total_count": 0,
            "duplicates_found": 0,
            "quality_issues": [],
            "statistics": {},
            "merged_output": ""
        }

    def load_datasets(self):
        """ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ"""
        print("=" * 60)
        print("ğŸ“‚ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ ì¤‘...")
        print("=" * 60)

        files = {
            "train": self.base_path / "train.json",
            "val": self.base_path / "val.json",
            "test": self.base_path / "test.json"
        }

        for split_name, file_path in files.items():
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if split_name == "train":
                    self.train_data = data
                elif split_name == "val":
                    self.val_data = data
                elif split_name == "test":
                    self.test_data = data

                self.report["files_processed"].append({
                    "file": str(file_path),
                    "split": split_name,
                    "count": len(data),
                    "size_kb": file_path.stat().st_size / 1024
                })

                print(f"âœ“ {split_name:5s}: {len(data):5d}ê°œ ({file_path.stat().st_size/1024:.1f} KB)")
            else:
                print(f"âœ— {split_name:5s}: íŒŒì¼ ì—†ìŒ - {file_path}")

        self.all_data = self.train_data + self.val_data + self.test_data
        self.report["total_count"] = len(self.all_data)

        print(f"\nğŸ“Š ì „ì²´ ë°ì´í„°: {len(self.all_data):,}ê°œ")
        print()

    def check_duplicates(self):
        """ì¤‘ë³µ ê²€ì‚¬"""
        print("=" * 60)
        print("ğŸ” ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
        print("=" * 60)

        # ID ì¤‘ë³µ ê²€ì‚¬
        id_counter = Counter([item["id"] for item in self.all_data])
        duplicates = {id_: count for id_, count in id_counter.items() if count > 1}

        if duplicates:
            print(f"âš ï¸  ì¤‘ë³µëœ ID ë°œê²¬: {len(duplicates)}ê°œ")
            for id_, count in list(duplicates.items())[:5]:
                print(f"   - {id_}: {count}íšŒ")
            if len(duplicates) > 5:
                print(f"   ... ì™¸ {len(duplicates) - 5}ê°œ")
            self.report["duplicates_found"] = len(duplicates)
        else:
            print("âœ“ ì¤‘ë³µ ì—†ìŒ")
            self.report["duplicates_found"] = 0

        # ì§ˆë¬¸-ë‹µë³€ ì¡°í•© ì¤‘ë³µ ê²€ì‚¬
        qa_pairs = Counter([
            (item["instruction"], item["output"])
            for item in self.all_data
        ])
        qa_duplicates = {pair: count for pair, count in qa_pairs.items() if count > 1}

        if qa_duplicates:
            print(f"âš ï¸  ë™ì¼í•œ Q&A ì¡°í•© ë°œê²¬: {len(qa_duplicates)}ê°œ")
            self.report["quality_issues"].append({
                "type": "duplicate_qa_pairs",
                "count": len(qa_duplicates)
            })
        else:
            print("âœ“ Q&A ì¡°í•© ì¤‘ë³µ ì—†ìŒ")

        print()

    def validate_structure(self):
        """ë°ì´í„° êµ¬ì¡° ê²€ì¦"""
        print("=" * 60)
        print("ğŸ”§ ë°ì´í„° êµ¬ì¡° ê²€ì¦ ì¤‘...")
        print("=" * 60)

        required_fields = ["id", "instruction", "input", "output", "metadata"]
        metadata_fields = ["menu", "menu_name", "generation_method", "created_at"]

        missing_fields = defaultdict(int)
        missing_metadata = defaultdict(int)
        empty_instructions = 0
        empty_outputs = 0

        for idx, item in enumerate(self.all_data):
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            for field in required_fields:
                if field not in item:
                    missing_fields[field] += 1

            # ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¸
            if "metadata" in item:
                for field in metadata_fields:
                    if field not in item["metadata"]:
                        missing_metadata[field] += 1

            # ë¹ˆ ê°’ í™•ì¸
            if not item.get("instruction", "").strip():
                empty_instructions += 1
            if not item.get("output", "").strip():
                empty_outputs += 1

        if missing_fields:
            print("âš ï¸  í•„ìˆ˜ í•„ë“œ ëˆ„ë½:")
            for field, count in missing_fields.items():
                print(f"   - {field}: {count}ê°œ")
            self.report["quality_issues"].append({
                "type": "missing_required_fields",
                "details": dict(missing_fields)
            })
        else:
            print("âœ“ ëª¨ë“  í•„ìˆ˜ í•„ë“œ ì¡´ì¬")

        if missing_metadata:
            print("âš ï¸  ë©”íƒ€ë°ì´í„° í•„ë“œ ëˆ„ë½:")
            for field, count in missing_metadata.items():
                print(f"   - {field}: {count}ê°œ")
            self.report["quality_issues"].append({
                "type": "missing_metadata_fields",
                "details": dict(missing_metadata)
            })
        else:
            print("âœ“ ëª¨ë“  ë©”íƒ€ë°ì´í„° í•„ë“œ ì¡´ì¬")

        if empty_instructions > 0:
            print(f"âš ï¸  ë¹ˆ ì§ˆë¬¸(instruction): {empty_instructions}ê°œ")
            self.report["quality_issues"].append({
                "type": "empty_instructions",
                "count": empty_instructions
            })
        else:
            print("âœ“ ë¹ˆ ì§ˆë¬¸ ì—†ìŒ")

        if empty_outputs > 0:
            print(f"âš ï¸  ë¹ˆ ë‹µë³€(output): {empty_outputs}ê°œ")
            self.report["quality_issues"].append({
                "type": "empty_outputs",
                "count": empty_outputs
            })
        else:
            print("âœ“ ë¹ˆ ë‹µë³€ ì—†ìŒ")

        print()

    def calculate_statistics(self):
        """í†µê³„ ê³„ì‚°"""
        print("=" * 60)
        print("ğŸ“Š í†µê³„ ë¶„ì„ ì¤‘...")
        print("=" * 60)

        # ë©”ë‰´ë³„ ë¶„í¬
        menu_dist = Counter([item["metadata"]["menu"] for item in self.all_data])

        # ìƒì„± ë°©ë²•ë³„ ë¶„í¬
        gen_method_dist = Counter([
            item["metadata"].get("generation_method", "unknown")
            for item in self.all_data
        ])

        # í’ˆì§ˆ ì ìˆ˜ í†µê³„
        quality_scores = [
            item["metadata"].get("quality_score", 0)
            for item in self.all_data
        ]

        # ê¸¸ì´ í†µê³„
        question_lengths = [len(item["instruction"]) for item in self.all_data]
        answer_lengths = [len(item["output"]) for item in self.all_data]

        stats = {
            "menu_distribution": dict(menu_dist),
            "generation_method_distribution": dict(gen_method_dist),
            "quality_scores": {
                "min": min(quality_scores) if quality_scores else 0,
                "max": max(quality_scores) if quality_scores else 0,
                "avg": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
                "count_high": sum(1 for s in quality_scores if s >= 0.7),
                "count_medium": sum(1 for s in quality_scores if 0.6 <= s < 0.7),
                "count_low": sum(1 for s in quality_scores if s < 0.6)
            },
            "question_length": {
                "min": min(question_lengths),
                "max": max(question_lengths),
                "avg": sum(question_lengths) / len(question_lengths)
            },
            "answer_length": {
                "min": min(answer_lengths),
                "max": max(answer_lengths),
                "avg": sum(answer_lengths) / len(answer_lengths)
            },
            "split_distribution": {
                "train": len(self.train_data),
                "val": len(self.val_data),
                "test": len(self.test_data),
                "total": len(self.all_data)
            }
        }

        self.report["statistics"] = stats

        # ì¶œë ¥
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¶„í• :")
        print(f"   - Train: {stats['split_distribution']['train']:5d}ê°œ ({stats['split_distribution']['train']/stats['split_distribution']['total']*100:.1f}%)")
        print(f"   - Val:   {stats['split_distribution']['val']:5d}ê°œ ({stats['split_distribution']['val']/stats['split_distribution']['total']*100:.1f}%)")
        print(f"   - Test:  {stats['split_distribution']['test']:5d}ê°œ ({stats['split_distribution']['test']/stats['split_distribution']['total']*100:.1f}%)")
        print(f"   - Total: {stats['split_distribution']['total']:5d}ê°œ")

        print(f"\nğŸ“‹ ë©”ë‰´ë³„ ë¶„í¬:")
        for menu, count in sorted(menu_dist.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {menu:20s}: {count:5d}ê°œ ({count/len(self.all_data)*100:5.1f}%)")

        print(f"\nğŸ”§ ìƒì„± ë°©ë²•ë³„ ë¶„í¬:")
        for method, count in sorted(gen_method_dist.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {method:25s}: {count:5d}ê°œ ({count/len(self.all_data)*100:5.1f}%)")

        print(f"\nâ­ í’ˆì§ˆ ì ìˆ˜:")
        print(f"   - ìµœì†Œ: {stats['quality_scores']['min']:.2f}")
        print(f"   - ìµœëŒ€: {stats['quality_scores']['max']:.2f}")
        print(f"   - í‰ê· : {stats['quality_scores']['avg']:.2f}")
        print(f"   - ê³ í’ˆì§ˆ (â‰¥0.7): {stats['quality_scores']['count_high']:5d}ê°œ ({stats['quality_scores']['count_high']/len(self.all_data)*100:5.1f}%)")
        print(f"   - ì¤‘í’ˆì§ˆ (0.6~0.7): {stats['quality_scores']['count_medium']:5d}ê°œ ({stats['quality_scores']['count_medium']/len(self.all_data)*100:5.1f}%)")
        print(f"   - ì €í’ˆì§ˆ (<0.6): {stats['quality_scores']['count_low']:5d}ê°œ ({stats['quality_scores']['count_low']/len(self.all_data)*100:5.1f}%)")

        print(f"\nğŸ“ ì§ˆë¬¸ ê¸¸ì´:")
        print(f"   - ìµœì†Œ: {stats['question_length']['min']}ì")
        print(f"   - ìµœëŒ€: {stats['question_length']['max']}ì")
        print(f"   - í‰ê· : {stats['question_length']['avg']:.1f}ì")

        print(f"\nğŸ“ ë‹µë³€ ê¸¸ì´:")
        print(f"   - ìµœì†Œ: {stats['answer_length']['min']}ì")
        print(f"   - ìµœëŒ€: {stats['answer_length']['max']}ì")
        print(f"   - í‰ê· : {stats['answer_length']['avg']:.1f}ì")

        print()

    def merge_and_save(self, output_dir: str = "output/merge_final_data"):
        """ë°ì´í„° í†µí•© ë° ì €ì¥"""
        print("=" * 60)
        print("ğŸ’¾ ë°ì´í„° í†µí•© ë° ì €ì¥ ì¤‘...")
        print("=" * 60)

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # í†µí•© ë°ì´í„° ì €ì¥ (ê° split ì •ë³´ í¬í•¨)
        merged_data = []

        for item in self.train_data:
            item_copy = item.copy()
            item_copy["split"] = "train"
            merged_data.append(item_copy)

        for item in self.val_data:
            item_copy = item.copy()
            item_copy["split"] = "val"
            merged_data.append(item_copy)

        for item in self.test_data:
            item_copy = item.copy()
            item_copy["split"] = "test"
            merged_data.append(item_copy)

        # ë©”ì¸ í†µí•© íŒŒì¼
        merged_file = output_path / "hira_merged_dataset.json"
        with open(merged_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ í†µí•© ë°ì´í„°ì…‹ ì €ì¥: {merged_file}")
        print(f"  - ì´ {len(merged_data):,}ê°œ í•­ëª©")
        print(f"  - íŒŒì¼ í¬ê¸°: {merged_file.stat().st_size / 1024:.1f} KB")

        # Splitë³„ë¡œë„ ì €ì¥ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
        splits_dir = output_path / "splits"
        splits_dir.mkdir(exist_ok=True)

        for split_name, data in [("train", self.train_data), ("val", self.val_data), ("test", self.test_data)]:
            split_file = splits_dir / f"{split_name}.json"
            with open(split_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ {split_name} ë°ì´í„°: {split_file} ({len(data):,}ê°œ)")

        # ê²€ì‚¬ ë¦¬í¬íŠ¸ ì €ì¥
        report_file = output_path / "inspection_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2)

        print(f"âœ“ ê²€ì‚¬ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

        # README ìƒì„±
        readme_file = output_path / "README.md"
        readme_content = self._generate_readme()
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print(f"âœ“ README ì €ì¥: {readme_file}")

        self.report["merged_output"] = str(merged_file)

        print()

    def _generate_readme(self) -> str:
        """README ìƒì„±"""
        stats = self.report["statistics"]

        readme = f"""# HIRA í•™ìŠµ ë°ì´í„°ì…‹ (ìµœì¢… í†µí•©ë³¸)

## ğŸ“Š ë°ì´í„°ì…‹ ê°œìš”

- **ìƒì„±ì¼ì‹œ**: {self.report["inspection_time"]}
- **ì´ ë°ì´í„° ìˆ˜**: {self.report["total_count"]:,}ê°œ
- **Train/Val/Test ë¶„í• **: {stats['split_distribution']['train']}/{stats['split_distribution']['val']}/{stats['split_distribution']['test']}

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
merge_final_data/
â”œâ”€â”€ hira_merged_dataset.json      # ì „ì²´ í†µí•© ë°ì´í„° (split ì •ë³´ í¬í•¨)
â”œâ”€â”€ inspection_report.json        # ë°ì´í„° ê²€ì‚¬ ë¦¬í¬íŠ¸
â”œâ”€â”€ README.md                      # ì´ íŒŒì¼
â””â”€â”€ splits/                        # Splitë³„ ë°ì´í„°
    â”œâ”€â”€ train.json
    â”œâ”€â”€ val.json
    â””â”€â”€ test.json
```

## ğŸ“‹ ë°ì´í„° í†µê³„

### ë°ì´í„°ì…‹ ë¶„í• 
- **Train**: {stats['split_distribution']['train']:,}ê°œ ({stats['split_distribution']['train']/stats['split_distribution']['total']*100:.1f}%)
- **Val**: {stats['split_distribution']['val']:,}ê°œ ({stats['split_distribution']['val']/stats['split_distribution']['total']*100:.1f}%)
- **Test**: {stats['split_distribution']['test']:,}ê°œ ({stats['split_distribution']['test']/stats['split_distribution']['total']*100:.1f}%)

### ë©”ë‰´ë³„ ë¶„í¬
"""
        for menu, count in sorted(stats['menu_distribution'].items(), key=lambda x: x[1], reverse=True):
            readme += f"- **{menu}**: {count:,}ê°œ ({count/self.report['total_count']*100:.1f}%)\n"

        readme += f"""
### ìƒì„± ë°©ë²•ë³„ ë¶„í¬
"""
        for method, count in sorted(stats['generation_method_distribution'].items(), key=lambda x: x[1], reverse=True):
            readme += f"- **{method}**: {count:,}ê°œ ({count/self.report['total_count']*100:.1f}%)\n"

        readme += f"""
### í’ˆì§ˆ ì ìˆ˜
- **í‰ê· **: {stats['quality_scores']['avg']:.2f}
- **ìµœì†Œ**: {stats['quality_scores']['min']:.2f}
- **ìµœëŒ€**: {stats['quality_scores']['max']:.2f}
- **ê³ í’ˆì§ˆ (â‰¥0.7)**: {stats['quality_scores']['count_high']:,}ê°œ ({stats['quality_scores']['count_high']/self.report['total_count']*100:.1f}%)
- **ì¤‘í’ˆì§ˆ (0.6~0.7)**: {stats['quality_scores']['count_medium']:,}ê°œ ({stats['quality_scores']['count_medium']/self.report['total_count']*100:.1f}%)
- **ì €í’ˆì§ˆ (<0.6)**: {stats['quality_scores']['count_low']:,}ê°œ ({stats['quality_scores']['count_low']/self.report['total_count']*100:.1f}%)

### í…ìŠ¤íŠ¸ ê¸¸ì´
- **ì§ˆë¬¸ í‰ê· **: {stats['question_length']['avg']:.1f}ì (ë²”ìœ„: {stats['question_length']['min']}~{stats['question_length']['max']}ì)
- **ë‹µë³€ í‰ê· **: {stats['answer_length']['avg']:.1f}ì (ë²”ìœ„: {stats['answer_length']['min']}~{stats['answer_length']['max']}ì)

## âœ… í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼

- **ì¤‘ë³µ ID**: {self.report['duplicates_found']}ê°œ
- **í’ˆì§ˆ ì´ìŠˆ**: {len(self.report['quality_issues'])}ê±´

## ğŸ“– ë°ì´í„° í˜•ì‹

```json
{{
  "id": "hira_menu_xxxxx",
  "instruction": "ì§ˆë¬¸ ë‚´ìš©",
  "input": "",
  "output": "ë‹µë³€ ë‚´ìš©",
  "split": "train|val|test",
  "metadata": {{
    "menu": "ë©”ë‰´ ID",
    "menu_name": "ë©”ë‰´ ì´ë¦„",
    "generation_method": "ìƒì„± ë°©ë²•",
    "created_at": "ìƒì„± ì‹œê°„",
    "question_length": ì§ˆë¬¸_ê¸¸ì´,
    "answer_length": ë‹µë³€_ê¸¸ì´,
    "quality_score": í’ˆì§ˆ_ì ìˆ˜
  }}
}}
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### Pythonìœ¼ë¡œ ë¡œë“œ
```python
import json

# ì „ì²´ í†µí•© ë°ì´í„° ë¡œë“œ
with open('hira_merged_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Splitë³„ë¡œ í•„í„°ë§
train_data = [item for item in data if item['split'] == 'train']
val_data = [item for item in data if item['split'] == 'val']
test_data = [item for item in data if item['split'] == 'test']
```

### íŠ¹ì • Split ë¡œë“œ
```python
# ê°œë³„ split íŒŒì¼ ë¡œë“œ
with open('splits/train.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)
```

## ğŸ“ ìƒì„± ê³¼ì •

1. **ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„**: 323ê°œ Q&A ìŒ
2. **ê·œì¹™ ê¸°ë°˜ ì¦ê°•**: 323 â†’ 1,064ê°œ (3.3ë°°)
3. **í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±**: 1,064 â†’ 3,032ê°œ
4. **í’ˆì§ˆ í•„í„°ë§**: 3,032 â†’ 1,423ê°œ (í†µê³¼ìœ¨ 46.9%)
5. **ë°ì´í„°ì…‹ ë¶„í• **: Train/Val/Test = 80/10/10
6. **ìµœì¢… ê²€ì¦ ë° í†µí•©**: {self.report['total_count']:,}ê°œ

---

*Generated by HIRA Dataset Inspector v1.0*
*{self.report["inspection_time"]}*
"""
        return readme

    def run_full_inspection(self, output_dir: str = "output/merge_final_data"):
        """ì „ì²´ ê²€ì‚¬ ì‹¤í–‰"""
        self.load_datasets()
        self.check_duplicates()
        self.validate_structure()
        self.calculate_statistics()
        self.merge_and_save(output_dir)

        print("=" * 60)
        print("âœ… ê²€ì‚¬ ë° í†µí•© ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ğŸ“Š ì´ ë°ì´í„°: {self.report['total_count']:,}ê°œ")
        print(f"âš ï¸  ì¤‘ë³µ: {self.report['duplicates_found']}ê°œ")
        print(f"âš ï¸  í’ˆì§ˆ ì´ìŠˆ: {len(self.report['quality_issues'])}ê±´")
        print("=" * 60)


def main():
    inspector = HIRADatasetInspector(
        base_path="hira_training_datasets/output/v1.0/full"
    )

    inspector.run_full_inspection(
        output_dir="hira_training_datasets/output/merge_final_data"
    )


if __name__ == "__main__":
    main()
