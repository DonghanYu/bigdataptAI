#!/usr/bin/env python3
"""
SOLAR-10.7B LoRA í•™ìŠµ - ìµœì í™” ë²„ì „
bitsandbytes ì™„ì „ ìš°íšŒ + ê²€ì¦ëœ ì„¤ì •
"""

import sys
import os
import json
from pathlib import Path

import importlib.util
import transformers
from transformers.utils import import_utils


def disable_bitsandbytes():
    """Force Transformers to behave as if bitsandbytes is unavailable."""

    def _return_false(*args, **kwargs):  # noqa: D401 - small helper
        return False

    os.environ.setdefault("BITSANDBYTES_NOWELCOME", "1")

    # Ensure module discovery fails before Transformers performs availability checks.
    original_find_spec = importlib.util.find_spec

    def _patched_find_spec(name, package=None):
        if name == "bitsandbytes" or name.startswith("bitsandbytes."):
            return None
        return original_find_spec(name, package)

    importlib.util.find_spec = _patched_find_spec

    import_utils.is_bitsandbytes_available = _return_false
    # Older versions also consult the private helper below.
    if hasattr(import_utils, "_is_bitsandbytes_available"):
        import_utils._is_bitsandbytes_available = _return_false

    # Remove any eager-imported modules so future lookups fail fast.
    for module_name in list(sys.modules):
        if module_name == "bitsandbytes" or module_name.startswith("bitsandbytes."):
            sys.modules.pop(module_name, None)

    # Prevent optional dependency checks from trying to import it again.
    import_utils.BACKENDS_MAPPING["bitsandbytes"] = (None, None, None)


disable_bitsandbytes()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from datetime import datetime

# í™˜ê²½ ì„¤ì •
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

print("=" * 60)
print("SOLAR-10.7B LoRA í•™ìŠµ - ìµœì í™” ë²„ì „")
print("=" * 60)

# ê²½ë¡œ ì„¤ì •
BASE_PATH = Path("./workspace")
MODEL_PATH = BASE_PATH / "../solar_10.7b_package/model"  # SOLAR ëª¨ë¸ ê²½ë¡œ
DATA_PATH = BASE_PATH / "data/hira"
OUTPUT_PATH = BASE_PATH / "models/solar_hira"
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

# GPU í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\n[í™˜ê²½]")
print(f"Device: {device}")
print(f"PyTorch: {torch.__version__}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"CUDA: {torch.version.cuda}")


class HIRADataset(Dataset):
    """HIRA ê±´ê°•ë³´í—˜ ë°ì´í„°ì…‹"""

    def __init__(self, data_file, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []

        # JSONL íŒŒì¼ ë¡œë“œ
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    self.data.append(json.loads(line))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]

        # SOLAR í”„ë¡¬í”„íŠ¸ í˜•ì‹
        text = f"""### Instruction:
{sample.get('instruction', '')}

### Input:
{sample.get('input', '')}

### Response:
{sample.get('output', '')}"""

        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].squeeze()
        attention_mask = encoding["attention_mask"].squeeze()
        labels = input_ids.clone()
        labels[attention_mask == 0] = -100

        # Response ë¶€ë¶„ë§Œ í•™ìŠµ (ì„ íƒì‚¬í•­)
        response_token_ids = self.tokenizer.encode("### Response:", add_special_tokens=False)
        response_start_idx = None
        search_limit = len(input_ids) - len(response_token_ids) + 1
        if search_limit > 0:
            for i in range(search_limit):
                if input_ids[i:i + len(response_token_ids)].tolist() == response_token_ids:
                    response_start_idx = i + len(response_token_ids)
                    break

        if response_start_idx is not None:
            labels[:response_start_idx] = -100  # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë¬´ì‹œ

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }


def load_model_and_tokenizer(model_path):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"\n[1/3] ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ëª¨ë¸ ë¡œë“œ - bfloat16 ì‚¬ìš© (A100 ìµœì )
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,  # A100ì—ì„œ ìµœì 
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
        use_cache=False  # í•™ìŠµ ì‹œ í•„ìˆ˜
    )

    # Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()

    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.num_parameters() / 1e9:.2f}B parameters")
    return model, tokenizer


def apply_lora(model):
    """LoRA ì ìš©"""
    print(f"\n[2/3] LoRA ì„¤ì • ì¤‘...")

    # SOLAR/Llama ì•„í‚¤í…ì²˜ìš© LoRA ì„¤ì •
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        bias="none",
        modules_to_save=["embed_tokens", "lm_head"]  # ì„ë² ë”©ë„ í•™ìŠµ
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    return model


def train(model, tokenizer, train_loader, val_loader, config):
    """í•™ìŠµ ì‹¤í–‰"""
    print(f"\n[3/3] í•™ìŠµ ì‹œì‘!")
    print(f"Config: {config}")
    print("=" * 60)

    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=config.get("weight_decay", 0.01)
    )

    # Learning rate scheduler
    from torch.optim.lr_scheduler import CosineAnnealingLR
    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=config["num_epochs"] * len(train_loader),
        eta_min=config["learning_rate"] * 0.1
    )

    model.train()
    best_val_loss = float('inf')
    training_history = []

    for epoch in range(config["num_epochs"]):
        print(f"\n[Epoch {epoch + 1}/{config['num_epochs']}]")
        epoch_loss = 0
        optimizer.zero_grad()
        steps_since_opt_step = 0

        progress_bar = tqdm(train_loader, desc=f"Training")
        for step, batch in enumerate(progress_bar):
            # ë°ì´í„° ì´ë™
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss / config["gradient_accumulation_steps"]
            loss.backward()
            epoch_loss += loss.item() * config["gradient_accumulation_steps"]
            steps_since_opt_step += 1

            # Gradient accumulation
            if steps_since_opt_step == config["gradient_accumulation_steps"]:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.get("max_grad_norm", 1.0))
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                steps_since_opt_step = 0

                current_lr = scheduler.get_last_lr()[0]
                progress_bar.set_postfix({
                    "loss": f"{loss.item() * config['gradient_accumulation_steps']:.4f}",
                    "lr": f"{current_lr:.2e}"
                })

        if steps_since_opt_step != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.get("max_grad_norm", 1.0))
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        avg_train_loss = epoch_loss / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                val_loss += outputs.loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        training_history.append({
            "epoch": epoch + 1,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "learning_rate": scheduler.get_last_lr()[0]
        })

        # Best model ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_path = OUTPUT_PATH / "best_model"
            model.save_pretrained(checkpoint_path)
            tokenizer.save_pretrained(checkpoint_path)
            print(f"âœ¨ Best model saved: {checkpoint_path}")

        # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸
        if (epoch + 1) % config.get("save_epochs", 5) == 0:
            checkpoint_path = OUTPUT_PATH / f"checkpoint-epoch-{epoch+1}"
            model.save_pretrained(checkpoint_path)
            print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")

        model.train()

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_path = OUTPUT_PATH / "final_model"
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)

    # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
    history_file = OUTPUT_PATH / "training_history.json"
    with open(history_file, 'w') as f:
        json.dump(training_history, f, indent=2)

    return best_val_loss, training_history


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í•™ìŠµ ì„¤ì •
    config = {
        "batch_size": 2,
        "gradient_accumulation_steps": 4,  # ì‹¤íš¨ ë°°ì¹˜ = 8
        "learning_rate": 2e-4,
        "num_epochs": 5,
        "max_length": 512,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_epochs": 2,
        "warmup_ratio": 0.1
    }

    # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(MODEL_PATH)

    # 2. LoRA ì ìš©
    model = apply_lora(model)

    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
    print("\në°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")

    # HIRA ë°ì´í„° ì‚¬ìš© (ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±)
    train_file = DATA_PATH / "train.jsonl"
    val_file = DATA_PATH / "validation.jsonl"

    if not train_file.exists():
        print("âš ï¸ HIRA ë°ì´í„° ì—†ìŒ. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = [
            {
                "instruction": "ê±´ê°•ë³´í—˜ ë¹…ë°ì´í„° ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”.",
                "input": "MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ”?",
                "output": "2024ë…„ MRI ê²€ì‚¬ëŠ” ì´ 420ë§Œê±´ ì‹œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
            },
            {
                "instruction": "ì˜ë£Œë¹„ í†µê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.",
                "input": "ì—°ê°„ ì´ ì§„ë£Œë¹„ëŠ”?",
                "output": "2024ë…„ ì´ ì§„ë£Œë¹„ëŠ” 125ì¡° 2,450ì–µì›ì…ë‹ˆë‹¤."
            }
        ] * 50  # 100ê°œ

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        train_file = OUTPUT_PATH / "temp_train.jsonl"
        val_file = OUTPUT_PATH / "temp_val.jsonl"

        with open(train_file, 'w') as f:
            for item in test_data[:90]:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        with open(val_file, 'w') as f:
            for item in test_data[90:]:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

    train_dataset = HIRADataset(train_file, tokenizer, config["max_length"])
    val_dataset = HIRADataset(val_file, tokenizer, config["max_length"])

    print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}")

    # 4. DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        num_workers=2,
        pin_memory=True
    )

    # 5. í•™ìŠµ ì‹¤í–‰
    start_time = datetime.now()
    best_val_loss, history = train(model, tokenizer, train_loader, val_loader, config)
    end_time = datetime.now()

    # 6. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"Best Val Loss: {best_val_loss:.4f}")
    print(f"ì†Œìš” ì‹œê°„: {end_time - start_time}")
    print(f"ëª¨ë¸ ì €ì¥: {OUTPUT_PATH}")
    print("=" * 60)


if __name__ == "__main__":
    main()
