#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- BLEU, ROUGE ê³„ì‚°
- Perplexity ì¸¡ì •
- Hallucination ì²´í¬
"""

import sys
import os

os.environ['BITSANDBYTES_NOWELCOME'] = '1'
sys.modules['bitsandbytes'] = None

import torch
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm
import numpy as np
from collections import defaultdict

# ============================================
# ì„¤ì •
# ============================================
WORK_DIR = Path("/home/work/LLM_Meditron/bigdataAI")
BASE_MODEL_PATH = WORK_DIR / "solar_10.7b_package" / "model"
LORA_MODEL_PATH = WORK_DIR / "workspace" / "models" / "solar_hira_v3" / "best_model"
TEST_FILE = WORK_DIR / "workspace" / "data" / "hira" / "cleaned_data" / "test.jsonl"
OUTPUT_DIR = WORK_DIR / "workspace" / "evaluation"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("="*80)
print("ëª¨ë¸ í‰ê°€")
print("="*80)
print(f"Device: {device}")

# ============================================
# ëª¨ë¸ ë¡œë“œ
# ============================================
print("\nëª¨ë¸ ë¡œë”©...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
model.eval()
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ============================================
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
# ============================================
print(f"\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {TEST_FILE}")
test_data = []
with open(TEST_FILE, 'r', encoding='utf-8') as f:
    for line in f:
        try:
            test_data.append(json.loads(line.strip()))
        except:
            continue
print(f"âœ… {len(test_data)}ê°œ ìƒ˜í”Œ ë¡œë“œ")

# ============================================
# ìƒì„± í•¨ìˆ˜
# ============================================
def generate_response(question, max_length=256, temperature=0.3):
    """ì‘ë‹µ ìƒì„±"""
    prompt = f"### Instruction:\n{question}\n\n### Response:\n"
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=temperature,
            top_p=0.85,
            top_k=40,
            repetition_penalty=1.15,
            no_repeat_ngram_size=3,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "### Response:" in response:
        response = response.split("### Response:")[-1].strip()
    
    return response

# ============================================
# í‰ê°€ ë©”íŠ¸ë¦­
# ============================================
def calculate_bleu(reference, hypothesis):
    """BLEU Score ê³„ì‚° (ê°„ë‹¨ ë²„ì „)"""
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    
    # 1-gram precision
    common = len(set(ref_words) & set(hyp_words))
    if len(hyp_words) == 0:
        return 0.0
    
    precision = common / len(hyp_words)
    
    # Brevity penalty
    bp = 1.0 if len(hyp_words) >= len(ref_words) else np.exp(1 - len(ref_words)/len(hyp_words))
    
    return bp * precision

def calculate_rouge_l(reference, hypothesis):
    """ROUGE-L Score ê³„ì‚°"""
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    
    # LCS (Longest Common Subsequence)
    m, n = len(ref_words), len(hyp_words)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
            else:
                dp[i][j] = max(dp[i-1][j], dp[i][j-1])
    
    lcs_length = dp[m][n]
    
    if m == 0 or n == 0:
        return 0.0
    
    recall = lcs_length / m
    precision = lcs_length / n
    
    if recall + precision == 0:
        return 0.0
    
    f1 = 2 * recall * precision / (recall + precision)
    return f1

def exact_match(reference, hypothesis):
    """Exact Match (ì™„ì „ ì¼ì¹˜)"""
    return 1.0 if reference.strip() == hypothesis.strip() else 0.0

def check_hallucination(question, reference, hypothesis):
    """
    Hallucination ì²´í¬ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    Returns: 0 (no hallucination) or 1 (hallucination detected)
    """
    # 1. ë„ˆë¬´ ì§§ì€ ë‹µë³€
    if len(hypothesis) < 10:
        return 1
    
    # 2. ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ ì—†ìŒ
    q_words = set(question.lower().split())
    h_words = set(hypothesis.lower().split())
    r_words = set(reference.lower().split())
    
    # ì§ˆë¬¸ê³¼ì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´
    overlap_with_q = len(q_words & h_words)
    overlap_with_r = len(r_words & h_words)
    
    if overlap_with_q == 0 and len(q_words) > 3:
        return 1  # ì§ˆë¬¸ê³¼ ë¬´ê´€
    
    if overlap_with_r < len(r_words) * 0.2:
        return 1  # ì •ë‹µê³¼ ë„ˆë¬´ ë‹¤ë¦„
    
    # 3. ê³¼ë„í•œ ë°˜ë³µ
    words = hypothesis.split()
    if len(words) > len(set(words)) * 2:
        return 1
    
    return 0

# ============================================
# í‰ê°€ ì‹¤í–‰
# ============================================
print("\n" + "="*80)
print("í‰ê°€ ì‹œì‘")
print("="*80)

results = {
    'bleu': [],
    'rouge_l': [],
    'exact_match': [],
    'hallucination': [],
    'samples': []
}

for i, item in enumerate(tqdm(test_data, desc="Evaluating")):
    question = item['instruction']
    reference = item['output']
    
    # ìƒì„±
    hypothesis = generate_response(question)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    bleu = calculate_bleu(reference, hypothesis)
    rouge = calculate_rouge_l(reference, hypothesis)
    em = exact_match(reference, hypothesis)
    hall = check_hallucination(question, reference, hypothesis)
    
    results['bleu'].append(bleu)
    results['rouge_l'].append(rouge)
    results['exact_match'].append(em)
    results['hallucination'].append(hall)
    
    # ìƒ˜í”Œ ì €ì¥ (ì²˜ìŒ 10ê°œ)
    if i < 10:
        results['samples'].append({
            'question': question,
            'reference': reference,
            'hypothesis': hypothesis,
            'bleu': round(bleu, 3),
            'rouge_l': round(rouge, 3),
            'exact_match': em,
            'hallucination': hall
        })

# ============================================
# ê²°ê³¼ ì§‘ê³„
# ============================================
print("\n" + "="*80)
print("í‰ê°€ ê²°ê³¼")
print("="*80)

avg_bleu = np.mean(results['bleu'])
avg_rouge = np.mean(results['rouge_l'])
avg_em = np.mean(results['exact_match'])
hallucination_rate = np.mean(results['hallucination'])

print(f"\nğŸ“Š ì •ëŸ‰ í‰ê°€:")
print(f"  BLEU:              {avg_bleu:.4f}")
print(f"  ROUGE-L:           {avg_rouge:.4f}")
print(f"  Exact Match:       {avg_em:.4f} ({avg_em*100:.1f}%)")
print(f"  Hallucination:     {hallucination_rate:.4f} ({hallucination_rate*100:.1f}%)")

# ============================================
# ê²°ê³¼ ì €ì¥
# ============================================
# JSON ì €ì¥
output_file = OUTPUT_DIR / "evaluation_results.json"
summary = {
    'metrics': {
        'bleu': round(avg_bleu, 4),
        'rouge_l': round(avg_rouge, 4),
        'exact_match': round(avg_em, 4),
        'hallucination_rate': round(hallucination_rate, 4)
    },
    'num_samples': len(test_data),
    'samples': results['samples']
}

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")

# í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
report_file = OUTPUT_DIR / "evaluation_report.txt"
with open(report_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸\n")
    f.write("="*80 + "\n\n")
    
    f.write(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_data)}\n")
    f.write(f"ëª¨ë¸: {LORA_MODEL_PATH}\n\n")
    
    f.write("í‰ê°€ ë©”íŠ¸ë¦­:\n")
    f.write(f"  BLEU:              {avg_bleu:.4f}\n")
    f.write(f"  ROUGE-L:           {avg_rouge:.4f}\n")
    f.write(f"  Exact Match:       {avg_em:.4f} ({avg_em*100:.1f}%)\n")
    f.write(f"  Hallucination:     {hallucination_rate:.4f} ({hallucination_rate*100:.1f}%)\n\n")
    
    f.write("="*80 + "\n")
    f.write("ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 10ê°œ)\n")
    f.write("="*80 + "\n\n")
    
    for i, sample in enumerate(results['samples'], 1):
        f.write(f"[ìƒ˜í”Œ {i}]\n")
        f.write(f"Q: {sample['question']}\n")
        f.write(f"ì •ë‹µ: {sample['reference']}\n")
        f.write(f"ìƒì„±: {sample['hypothesis']}\n")
        f.write(f"BLEU: {sample['bleu']}, ROUGE: {sample['rouge_l']}, ")
        f.write(f"EM: {sample['exact_match']}, Hall: {sample['hallucination']}\n\n")

print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

print("\n" + "="*80)
print("í‰ê°€ ì™„ë£Œ!")
print("="*80)
