#!/usr/bin/env python3
"""
HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Requests í¬ë¡¤ëŸ¬ (ì•ˆì •ì )
URL: https://opendata.hira.or.kr/home.do
"""

import json
import time
import re
import warnings
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# SSL ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', message='Unverified HTTPS request')


class HIRACrawler:
    """HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ëŸ¬ (Requests)"""

    def __init__(self, timeout: int = 30):
        """
        Args:
            timeout: ìš”ì²­ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.base_url = "https://opendata.hira.or.kr"
        self.timeout = timeout

        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })

        # ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
        self.data = {
            "site_info": {
                "url": self.base_url,
                "crawled_at": datetime.now().isoformat(),
                "crawler_version": "1.0.0",
                "engine": "Requests"
            },
            "menus": [],
            "pages": [],
            "statistics": {}
        }

    def crawl(self):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("="*80)
        print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ì‹œì‘ (Requests)")
        print("="*80)

        try:
            # 1. í™ˆí˜ì´ì§€ ì ‘ì†
            print(f"\n[1/5] í™ˆí˜ì´ì§€ ì ‘ì†: {self.base_url}/home.do")
            homepage_html = self._access_homepage()

            if not homepage_html:
                print("   âš ï¸ í™ˆí˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨, ì œí•œëœ í¬ë¡¤ë§ ì§„í–‰")

            # 2. ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘
            print("\n[2/5] ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘ ì¤‘...")
            if homepage_html:
                self._collect_menu_structure(homepage_html)
            else:
                print("   âš ï¸ ê±´ë„ˆëœ€")

            # 3. ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§
            print("\n[3/5] ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
            self._crawl_main_pages()

            # 4. í†µê³„ ì •ë³´ ìˆ˜ì§‘
            print("\n[4/5] í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            self._collect_statistics()

            # 5. ë°ì´í„° ì •ë¦¬
            print("\n[5/5] ë°ì´í„° ì •ë¦¬ ì¤‘...")
            self._finalize_data()

            print("\n" + "="*80)
            print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print("="*80)

        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    def _access_homepage(self) -> Optional[str]:
        """í™ˆí˜ì´ì§€ ì ‘ì†"""
        try:
            url = f"{self.base_url}/home.do"
            response = self.session.get(url, timeout=self.timeout, verify=False)

            print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
            print(f"   ì¸ì½”ë”©: {response.encoding}")
            print(f"   Content-Type: {response.headers.get('Content-Type', 'N/A')}")

            if response.status_code == 200:
                print(f"   âœ“ ì ‘ì† ì„±ê³µ! (ì‘ë‹µ í¬ê¸°: {len(response.text):,} bytes)")
                return response.text
            else:
                print(f"   âœ— ì ‘ì† ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                return None

        except requests.exceptions.SSLError as e:
            print(f"   âš ï¸ SSL ì˜¤ë¥˜: {e}")
            return None
        except requests.exceptions.Timeout:
            print(f"   âš ï¸ íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ)")
            return None
        except Exception as e:
            print(f"   âš ï¸ ì˜¤ë¥˜: {e}")
            return None

    def _collect_menu_structure(self, html: str):
        """ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘"""
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # í˜ì´ì§€ ì œëª© í™•ì¸
            title = soup.find('title')
            if title:
                print(f"   í˜ì´ì§€ ì œëª©: {title.get_text(strip=True)}")

            # ë©”ë‰´ ì°¾ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            menu_selectors = [
                'nav ul',
                'div.gnb ul',
                'div.lnb ul',
                'ul.menu',
                'ul.nav',
                'header ul',
                'div[id*="menu"]',
                'div[class*="menu"]',
                '#gnb',
                '.gnb',
                '#header',
                '.header',
            ]

            menus_found = []
            all_links = []

            # ëª¨ë“  ë§í¬ ìˆ˜ì§‘ (fallback)
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if text and 2 < len(text) < 80 and href:
                    full_url = href
                    if not href.startswith('http'):
                        if href.startswith('/'):
                            full_url = f"{self.base_url}{href}"
                        elif not href.startswith('#') and not href.startswith('javascript:'):
                            full_url = f"{self.base_url}/{href}"
                        else:
                            continue

                    all_links.append({
                        "text": text,
                        "url": full_url,
                        "original_href": href
                    })

            # ë©”ë‰´ë¡œ ì¶”ì •ë˜ëŠ” ë§í¬ í•„í„°ë§
            menu_keywords = ['ì†Œê°œ', 'ë°ì´í„°', 'í†µê³„', 'API', 'ê³ ê°', 'ì§€ì›', 'ìë£Œ', 'ì„œë¹„ìŠ¤', 'ë¶„ì„', 'ì •ë³´']
            menu_links = []

            for link in all_links:
                if any(keyword in link['text'] for keyword in menu_keywords):
                    menu_links.append(link)

            # ì¤‘ë³µ ì œê±°
            unique_links = []
            seen_urls = set()

            for link in menu_links:
                if link['url'] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link['url'])

            # ë©”ë‰´ êµ¬ì¡° ìƒì„±
            if unique_links:
                menu_item = {
                    "id": "main_menu",
                    "text": "ì£¼ìš” ë©”ë‰´",
                    "links": unique_links
                }
                menus_found.append(menu_item)

            self.data["menus"] = menus_found
            print(f"   ìˆ˜ì§‘ëœ ë©”ë‰´ ë§í¬: {len(unique_links)}ê°œ")

            # ìƒ˜í”Œ ì¶œë ¥
            for link in unique_links[:10]:
                print(f"      - {link['text']}")

        except Exception as e:
            print(f"   âš ï¸ ë©”ë‰´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    def _crawl_main_pages(self):
        """ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§"""
        # ì£¼ìš” í˜ì´ì§€ URL ëª©ë¡
        main_pages = [
            {"path": "/home.do", "name": "í™ˆ"},
            {"path": "/op/opc/selectOpenData.do", "name": "ë°ì´í„° ëª©ë¡"},
            {"path": "/st/stc/selectStcList.do", "name": "í†µê³„ ëª©ë¡"},
            {"path": "/op/opi/selectOpenApiList.do", "name": "Open API"},
            {"path": "/cm/cm_info.do?pgmId=HIRAA010000000000", "name": "ì„œë¹„ìŠ¤ ì†Œê°œ"},
            {"path": "/bd/ay/selectBdUseList.do", "name": "ë¹…ë°ì´í„° í™œìš©"},
            {"path": "/cs/ntt/selectBoardList.do?bbsId=BBSMSTR_000000000012", "name": "ê³µì§€ì‚¬í•­"},
        ]

        for page_info in main_pages:
            try:
                url = f"{self.base_url}{page_info['path']}"
                print(f"   í¬ë¡¤ë§: {page_info['name']}")
                print(f"      URL: {url}")

                # í˜ì´ì§€ ìš”ì²­
                response = self.session.get(url, timeout=self.timeout, verify=False)

                if response.status_code == 200:
                    # í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
                    page_data = self._extract_page_data(response.text, page_info['name'], url)
                    if page_data:
                        self.data["pages"].append(page_data)
                        print(f"      âœ“ ìˆ˜ì§‘ ì™„ë£Œ (ì½˜í…ì¸ : {page_data.get('content_length', 0):,}ì)")
                else:
                    print(f"      âœ— ì‹¤íŒ¨ (ìƒíƒœ: {response.status_code})")

                time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"      âš ï¸ ì˜¤ë¥˜: {e}")
                continue

    def _extract_page_data(self, html: str, page_name: str, url: str) -> Optional[Dict]:
        """í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('title')
            title = title_elem.get_text(strip=True) if title_elem else ""

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_selectors = [
                'div.content',
                'div.container',
                'div.main-content',
                'div#content',
                'main',
                'article',
                'div.board',
                'div.cont',
            ]

            content_text = ""
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
                    for script in content.find_all(['script', 'style']):
                        script.decompose()
                    content_text = content.get_text(separator='\n', strip=True)
                    break

            # ì „ì²´ body (fallback)
            if not content_text:
                body = soup.find('body')
                if body:
                    for script in body.find_all(['script', 'style']):
                        script.decompose()
                    content_text = body.get_text(separator='\n', strip=True)

            # í…Œì´ë¸” ì¶”ì¶œ
            tables = []
            for table in soup.find_all('table')[:10]:
                table_data = self._extract_table_data(table)
                if table_data:
                    tables.append(table_data)

            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True)[:100]:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if text and 2 < len(text) < 100 and href:
                    full_url = href
                    if not href.startswith('http'):
                        if href.startswith('/'):
                            full_url = f"{self.base_url}{href}"
                        elif not href.startswith('#') and not href.startswith('javascript:'):
                            full_url = f"{self.base_url}/{href}"
                        else:
                            continue

                    links.append({
                        "text": text,
                        "url": full_url
                    })

            # ì¤‘ë³µ ë§í¬ ì œê±°
            unique_links = []
            seen_urls = set()
            for link in links:
                if link["url"] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link["url"])

            return {
                "name": page_name,
                "url": url,
                "title": title,
                "content_preview": content_text[:800] if content_text else "",
                "content_length": len(content_text),
                "tables_count": len(tables),
                "tables": tables[:5],  # ìµœëŒ€ 5ê°œ í…Œì´ë¸”ë§Œ
                "links_count": len(unique_links),
                "links": unique_links[:30],  # ìµœëŒ€ 30ê°œ ë§í¬
                "crawled_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"         í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def _extract_table_data(self, table) -> Optional[Dict]:
        """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        try:
            headers = []
            rows = []

            # í—¤ë” ì¶”ì¶œ
            thead = table.find('thead')
            if thead:
                for tr in thead.find_all('tr'):
                    header_cells = tr.find_all(['th', 'td'])
                    if header_cells:
                        headers = [cell.get_text(strip=True) for cell in header_cells if cell.get_text(strip=True)]
                        if headers:
                            break

            # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í–‰ í™•ì¸
            if not headers:
                first_row = table.find('tr')
                if first_row:
                    header_cells = first_row.find_all('th')
                    if header_cells:
                        headers = [cell.get_text(strip=True) for cell in header_cells]

            # ë°ì´í„° í–‰ ì¶”ì¶œ
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr')[:20]:  # ìµœëŒ€ 20í–‰
                cells = tr.find_all(['td', 'th'])
                if cells:
                    row = [cell.get_text(strip=True) for cell in cells]
                    # ë¹ˆ í–‰ì´ ì•„ë‹ˆë©´ ì¶”ê°€
                    if any(cell for cell in row):
                        rows.append(row)

            if rows:
                return {
                    "headers": headers,
                    "rows": rows[:15],  # ìµœëŒ€ 15í–‰ë§Œ ì €ì¥
                    "row_count": len(rows),
                    "column_count": len(rows[0]) if rows else 0
                }

            return None

        except Exception as e:
            return None

    def _collect_statistics(self):
        """í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
        stats = {
            "total_menus": len(self.data["menus"]),
            "total_pages": len(self.data["pages"]),
            "total_links": sum(page.get("links_count", 0) for page in self.data["pages"]),
            "total_tables": sum(page.get("tables_count", 0) for page in self.data["pages"]),
            "total_menu_links": sum(len(menu.get("links", [])) for menu in self.data["menus"]),
            "total_content_length": sum(page.get("content_length", 0) for page in self.data["pages"]),
        }

        self.data["statistics"] = stats
        print(f"   ë©”ë‰´: {stats['total_menus']}ê°œ")
        print(f"   ë©”ë‰´ ë§í¬: {stats['total_menu_links']}ê°œ")
        print(f"   í¬ë¡¤ë§ í˜ì´ì§€: {stats['total_pages']}ê°œ")
        print(f"   ìˆ˜ì§‘ ë§í¬: {stats['total_links']}ê°œ")
        print(f"   ìˆ˜ì§‘ í…Œì´ë¸”: {stats['total_tables']}ê°œ")
        print(f"   ì´ ì½˜í…ì¸ : {stats['total_content_length']:,}ì")

    def _finalize_data(self):
        """ë°ì´í„° ì •ë¦¬ ë° ìµœì¢…í™”"""
        self.data["site_info"]["completed_at"] = datetime.now().isoformat()
        print("   ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")

    def save_json(self, output_path: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size
        print(f"\nâœ… JSON ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")

    def save_pretty_json(self, output_path: str):
        """ì½ê¸° ì‰¬ìš´ JSON ì €ì¥ (ìš”ì•½ í¬í•¨)"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ìš”ì•½ ë°ì´í„° ìƒì„±
        summary = {
            "crawl_summary": {
                "site": self.base_url,
                "crawled_at": self.data["site_info"]["crawled_at"],
                "completed_at": self.data["site_info"].get("completed_at", ""),
                "engine": "Requests + BeautifulSoup",
                "statistics": self.data["statistics"]
            },
            "menu_structure": [
                {
                    "id": menu["id"],
                    "name": menu.get("text", "")[:60],
                    "links_count": len(menu.get("links", [])),
                    "sample_links": [
                        {
                            "text": link["text"],
                            "url": link["url"]
                        }
                        for link in menu.get("links", [])[:5]
                    ]
                }
                for menu in self.data["menus"]
            ],
            "pages_overview": [
                {
                    "name": page["name"],
                    "url": page["url"],
                    "title": page["title"],
                    "content_length": page.get("content_length", 0),
                    "tables": page["tables_count"],
                    "links": page["links_count"]
                }
                for page in self.data["pages"]
            ],
            "full_data": self.data
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size
        print(f"âœ… ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Requests í¬ë¡¤ëŸ¬ v1.0")
    print("="*80 + "\n")

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HIRACrawler(timeout=30)

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler.crawl()

        # JSON ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("/home/user/bigdataptAI/hira_crawler/output")
        output_dir.mkdir(exist_ok=True)

        # 1. ì „ì²´ ë°ì´í„°
        crawler.save_json(output_dir / f"hira_crawled_data_{timestamp}.json")

        # 2. ìš”ì•½ ë°ì´í„°
        crawler.save_pretty_json(output_dir / f"hira_summary_{timestamp}.json")

        print("\n" + "="*80)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
