#!/usr/bin/env python3
"""
HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Selenium í¬ë¡¤ëŸ¬
URL: https://opendata.hira.or.kr/home.do
"""

import json
import time
import re
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup


class HIRACrawler:
    """HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = True, timeout: int = 10):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            timeout: í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.base_url = "https://opendata.hira.or.kr"
        self.timeout = timeout
        self.driver = self._init_driver(headless)
        self.wait = WebDriverWait(self.driver, timeout)

        # ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
        self.data = {
            "site_info": {
                "url": self.base_url,
                "crawled_at": datetime.now().isoformat(),
                "crawler_version": "1.0.0"
            },
            "menus": [],
            "pages": [],
            "statistics": {}
        }

    def _init_driver(self, headless: bool) -> webdriver.Chrome:
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        options = Options()

        if headless:
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')

        # User-Agent ì„¤ì • (403 ìš°íšŒ)
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        # ê¸°íƒ€ ì˜µì…˜
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        return driver

    def crawl(self):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("="*80)
        print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ì‹œì‘")
        print("="*80)

        try:
            # 1. í™ˆí˜ì´ì§€ ì ‘ì†
            print(f"\n[1/5] í™ˆí˜ì´ì§€ ì ‘ì†: {self.base_url}/home.do")
            self._access_homepage()

            # 2. ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘
            print("\n[2/5] ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘ ì¤‘...")
            self._collect_menu_structure()

            # 3. ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§
            print("\n[3/5] ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
            self._crawl_main_pages()

            # 4. í†µê³„ ì •ë³´ ìˆ˜ì§‘
            print("\n[4/5] í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            self._collect_statistics()

            # 5. ë°ì´í„° ì •ë¦¬
            print("\n[5/5] ë°ì´í„° ì •ë¦¬ ì¤‘...")
            self._finalize_data()

            print("\n" + "="*80)
            print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print("="*80)

        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

        finally:
            self.driver.quit()

    def _access_homepage(self):
        """í™ˆí˜ì´ì§€ ì ‘ì†"""
        self.driver.get(f"{self.base_url}/home.do")
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # í˜ì´ì§€ ì œëª© í™•ì¸
        page_title = self.driver.title
        print(f"   í˜ì´ì§€ ì œëª©: {page_title}")

        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (ë””ë²„ê¹…ìš©)
        screenshot_path = Path("/home/user/bigdataptAI/hira_crawler/screenshots")
        screenshot_path.mkdir(exist_ok=True)
        self.driver.save_screenshot(str(screenshot_path / "homepage.png"))
        print(f"   ìŠ¤í¬ë¦°ìƒ· ì €ì¥: screenshots/homepage.png")

    def _collect_menu_structure(self):
        """ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘"""
        try:
            # HTML íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # ë©”ë‰´ ì°¾ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            menu_selectors = [
                'nav ul.menu',
                'div.gnb ul',
                'ul.nav',
                'header nav ul',
                'div[id*="menu"] ul',
                'div[class*="menu"] ul',
                'nav li',
            ]

            menus_found = []

            for selector in menu_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   ë°œê²¬ëœ ë©”ë‰´ ìš”ì†Œ: {selector} ({len(elements)}ê°œ)")

                    for idx, elem in enumerate(elements[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                        menu_text = elem.get_text(strip=True)
                        menu_links = elem.find_all('a')

                        if menu_text and len(menu_text) < 100:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
                            menu_item = {
                                "id": f"menu_{idx}",
                                "text": menu_text,
                                "selector": selector,
                                "links": []
                            }

                            for link in menu_links:
                                href = link.get('href', '')
                                link_text = link.get_text(strip=True)
                                if href and link_text:
                                    menu_item["links"].append({
                                        "text": link_text,
                                        "url": href if href.startswith('http') else f"{self.base_url}{href}"
                                    })

                            if menu_item["links"]:  # ë§í¬ê°€ ìˆëŠ” ë©”ë‰´ë§Œ ì €ì¥
                                menus_found.append(menu_item)

            # ì¤‘ë³µ ì œê±°
            unique_menus = []
            seen_texts = set()

            for menu in menus_found:
                if menu["text"] not in seen_texts:
                    unique_menus.append(menu)
                    seen_texts.add(menu["text"])

            self.data["menus"] = unique_menus
            print(f"   ìˆ˜ì§‘ëœ ë©”ë‰´: {len(unique_menus)}ê°œ")

            # ìƒ˜í”Œ ì¶œë ¥
            for menu in unique_menus[:3]:
                print(f"      - {menu['text']} ({len(menu['links'])}ê°œ ë§í¬)")

        except Exception as e:
            print(f"   âš ï¸ ë©”ë‰´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

    def _crawl_main_pages(self):
        """ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§"""
        # ì£¼ìš” í˜ì´ì§€ URL ëª©ë¡
        main_pages = [
            {"path": "/home.do", "name": "í™ˆ"},
            {"path": "/data/datalist.do", "name": "ë°ì´í„° ëª©ë¡"},
            {"path": "/stat/statlist.do", "name": "í†µê³„ ëª©ë¡"},
            {"path": "/openapi/list.do", "name": "Open API"},
            {"path": "/guide/guide.do", "name": "ì´ìš©ì•ˆë‚´"},
        ]

        for page in main_pages:
            try:
                url = f"{self.base_url}{page['path']}"
                print(f"   í¬ë¡¤ë§: {page['name']} - {url}")

                self.driver.get(url)
                time.sleep(2)

                # í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
                page_data = self._extract_page_data(page['name'], url)
                if page_data:
                    self.data["pages"].append(page_data)
                    print(f"      âœ“ ìˆ˜ì§‘ ì™„ë£Œ")

            except Exception as e:
                print(f"      âš ï¸ ì˜¤ë¥˜: {e}")
                continue

    def _extract_page_data(self, page_name: str, url: str) -> Optional[Dict]:
        """í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title = self.driver.title

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_selectors = [
                'div.content',
                'div.main-content',
                'div#content',
                'main',
                'article',
            ]

            content_text = ""
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    content_text = content.get_text(separator='\n', strip=True)[:2000]  # ì²˜ìŒ 2000ì
                    break

            # í…Œì´ë¸” ì¶”ì¶œ
            tables = []
            for table in soup.find_all('table')[:5]:  # ìµœëŒ€ 5ê°œ
                table_data = self._extract_table_data(table)
                if table_data:
                    tables.append(table_data)

            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True)[:50]:  # ìµœëŒ€ 50ê°œ
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if text and len(text) < 100:
                    links.append({
                        "text": text,
                        "url": href if href.startswith('http') else f"{self.base_url}{href}"
                    })

            return {
                "name": page_name,
                "url": url,
                "title": title,
                "content_preview": content_text[:500] if content_text else "",
                "tables_count": len(tables),
                "tables": tables,
                "links_count": len(links),
                "links": links[:10],  # ìƒìœ„ 10ê°œë§Œ ì €ì¥
                "crawled_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"         í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def _extract_table_data(self, table) -> Optional[Dict]:
        """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        try:
            headers = []
            rows = []

            # í—¤ë” ì¶”ì¶œ
            thead = table.find('thead')
            if thead:
                header_cells = thead.find_all(['th', 'td'])
                headers = [cell.get_text(strip=True) for cell in header_cells]

            # ë°ì´í„° í–‰ ì¶”ì¶œ
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr')[:10]:  # ìµœëŒ€ 10í–‰
                cells = tr.find_all(['td', 'th'])
                if cells:
                    row = [cell.get_text(strip=True) for cell in cells]
                    rows.append(row)

            if rows:
                return {
                    "headers": headers,
                    "rows": rows,
                    "row_count": len(rows)
                }

        except Exception as e:
            return None

    def _collect_statistics(self):
        """í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
        stats = {
            "total_menus": len(self.data["menus"]),
            "total_pages": len(self.data["pages"]),
            "total_links": sum(len(page.get("links", [])) for page in self.data["pages"]),
            "total_tables": sum(page.get("tables_count", 0) for page in self.data["pages"]),
        }

        self.data["statistics"] = stats
        print(f"   ë©”ë‰´: {stats['total_menus']}ê°œ")
        print(f"   í˜ì´ì§€: {stats['total_pages']}ê°œ")
        print(f"   ë§í¬: {stats['total_links']}ê°œ")
        print(f"   í…Œì´ë¸”: {stats['total_tables']}ê°œ")

    def _finalize_data(self):
        """ë°ì´í„° ì •ë¦¬ ë° ìµœì¢…í™”"""
        # ì¤‘ë³µ ì œê±°, ë°ì´í„° ê²€ì¦ ë“±
        self.data["site_info"]["completed_at"] = datetime.now().isoformat()
        print("   ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")

    def save_json(self, output_path: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… JSON ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {output_path.stat().st_size:,} bytes")

    def save_pretty_json(self, output_path: str):
        """ì½ê¸° ì‰¬ìš´ JSON ì €ì¥ (ìš”ì•½ í¬í•¨)"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ìš”ì•½ ë°ì´í„° ìƒì„±
        summary = {
            "crawl_summary": {
                "site": self.base_url,
                "crawled_at": self.data["site_info"]["crawled_at"],
                "completed_at": self.data["site_info"].get("completed_at", ""),
                "statistics": self.data["statistics"]
            },
            "menu_structure": [
                {
                    "id": menu["id"],
                    "name": menu["text"],
                    "links_count": len(menu["links"])
                }
                for menu in self.data["menus"]
            ],
            "pages_overview": [
                {
                    "name": page["name"],
                    "url": page["url"],
                    "title": page["title"],
                    "tables": page["tables_count"],
                    "links": page["links_count"]
                }
                for page in self.data["pages"]
            ],
            "full_data": self.data
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"âœ… ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Selenium í¬ë¡¤ëŸ¬ v1.0")
    print("="*80 + "\n")

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HIRACrawler(headless=True, timeout=10)

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler.crawl()

        # JSON ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("/home/user/bigdataptAI/hira_crawler/output")
        output_dir.mkdir(exist_ok=True)

        # 1. ì „ì²´ ë°ì´í„°
        crawler.save_json(output_dir / f"hira_crawled_data_{timestamp}.json")

        # 2. ìš”ì•½ ë°ì´í„°
        crawler.save_pretty_json(output_dir / f"hira_summary_{timestamp}.json")

        print("\n" + "="*80)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
