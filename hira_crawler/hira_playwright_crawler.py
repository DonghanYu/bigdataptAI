#!/usr/bin/env python3
"""
HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Playwright í¬ë¡¤ëŸ¬
URL: https://opendata.hira.or.kr/home.do
"""

import json
import time
import re
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from playwright.sync_api import sync_playwright, Page, Browser
from bs4 import BeautifulSoup


class HIRACrawler:
    """HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ëŸ¬ (Playwright)"""

    def __init__(self, headless: bool = True, timeout: int = 30000):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            timeout: í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        self.base_url = "https://opendata.hira.or.kr"
        self.timeout = timeout
        self.headless = headless

        # ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
        self.data = {
            "site_info": {
                "url": self.base_url,
                "crawled_at": datetime.now().isoformat(),
                "crawler_version": "1.0.0",
                "engine": "Playwright"
            },
            "menus": [],
            "pages": [],
            "statistics": {}
        }

    def crawl(self):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("="*80)
        print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ì‹œì‘ (Playwright)")
        print("="*80)

        with sync_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹œì‘
            browser = p.chromium.launch(headless=self.headless)

            # SSL ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ
            context = browser.new_context(ignore_https_errors=True)
            page = context.new_page()
            page.set_default_timeout(self.timeout)

            try:
                # 1. í™ˆí˜ì´ì§€ ì ‘ì†
                print(f"\n[1/5] í™ˆí˜ì´ì§€ ì ‘ì†: {self.base_url}/home.do")
                self._access_homepage(page)

                # 2. ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘
                print("\n[2/5] ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘ ì¤‘...")
                self._collect_menu_structure(page)

                # 3. ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§
                print("\n[3/5] ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
                self._crawl_main_pages(page)

                # 4. í†µê³„ ì •ë³´ ìˆ˜ì§‘
                print("\n[4/5] í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                self._collect_statistics()

                # 5. ë°ì´í„° ì •ë¦¬
                print("\n[5/5] ë°ì´í„° ì •ë¦¬ ì¤‘...")
                self._finalize_data()

                print("\n" + "="*80)
                print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
                print("="*80)

            except Exception as e:
                print(f"\nâŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()

            finally:
                browser.close()

    def _access_homepage(self, page: Page):
        """í™ˆí˜ì´ì§€ ì ‘ì†"""
        try:
            page.goto(f"{self.base_url}/home.do", wait_until="networkidle")
            time.sleep(2)  # ì¶”ê°€ ëŒ€ê¸°

            # í˜ì´ì§€ ì œëª© í™•ì¸
            page_title = page.title()
            print(f"   í˜ì´ì§€ ì œëª©: {page_title}")

            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            screenshot_path = Path("/home/user/bigdataptAI/hira_crawler/screenshots")
            screenshot_path.mkdir(exist_ok=True)
            page.screenshot(path=str(screenshot_path / "homepage.png"))
            print(f"   ìŠ¤í¬ë¦°ìƒ· ì €ì¥: screenshots/homepage.png")

        except Exception as e:
            print(f"   âš ï¸ í™ˆí˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜: {e}")
            # 403ì´ì–´ë„ ê³„ì† ì§„í–‰
            try:
                page.goto(f"{self.base_url}/home.do")
                time.sleep(3)
            except:
                pass

    def _collect_menu_structure(self, page: Page):
        """ë©”ë‰´ êµ¬ì¡° ìˆ˜ì§‘"""
        try:
            # HTML íŒŒì‹±
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')

            # ë©”ë‰´ ì°¾ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            menu_selectors = [
                'nav ul.gnb',
                'nav ul',
                'div.gnb ul',
                'ul.menu',
                'header nav ul',
                'div[id*="menu"] ul',
                'div[class*="menu"] ul',
                '#header ul',
                '.header ul',
            ]

            menus_found = []

            for selector in menu_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   ë°œê²¬ëœ ë©”ë‰´ ìš”ì†Œ: {selector} ({len(elements)}ê°œ)")

                    for idx, elem in enumerate(elements[:15]):
                        menu_text = elem.get_text(strip=True)
                        menu_links = elem.find_all('a')

                        # ë©”ë‰´ë¡œ ë³´ì´ëŠ” ê²ƒë§Œ (í…ìŠ¤íŠ¸ê°€ ì ë‹¹íˆ ì§§ê³  ë§í¬ê°€ ìˆìŒ)
                        if menu_text and 5 < len(menu_text) < 200 and menu_links:
                            menu_item = {
                                "id": f"menu_{len(menus_found)}",
                                "text": menu_text[:100],
                                "selector": selector,
                                "links": []
                            }

                            for link in menu_links:
                                href = link.get('href', '')
                                link_text = link.get_text(strip=True)
                                if href and link_text and len(link_text) < 50:
                                    full_url = href
                                    if not href.startswith('http'):
                                        if href.startswith('/'):
                                            full_url = f"{self.base_url}{href}"
                                        else:
                                            full_url = f"{self.base_url}/{href}"

                                    menu_item["links"].append({
                                        "text": link_text,
                                        "url": full_url
                                    })

                            if menu_item["links"]:
                                menus_found.append(menu_item)

            # ì¤‘ë³µ ì œê±° (í…ìŠ¤íŠ¸ ê¸°ì¤€)
            unique_menus = []
            seen_texts = set()

            for menu in menus_found:
                # ì²« 50ìë¡œ ë¹„êµ
                text_key = menu["text"][:50]
                if text_key not in seen_texts:
                    unique_menus.append(menu)
                    seen_texts.add(text_key)

            self.data["menus"] = unique_menus
            print(f"   ìˆ˜ì§‘ëœ ê³ ìœ  ë©”ë‰´: {len(unique_menus)}ê°œ")

            # ìƒ˜í”Œ ì¶œë ¥
            for menu in unique_menus[:5]:
                print(f"      - {menu['text'][:40]}... ({len(menu['links'])}ê°œ ë§í¬)")

        except Exception as e:
            print(f"   âš ï¸ ë©”ë‰´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    def _crawl_main_pages(self, page: Page):
        """ì£¼ìš” í˜ì´ì§€ í¬ë¡¤ë§"""
        # ì£¼ìš” í˜ì´ì§€ URL ëª©ë¡
        main_pages = [
            {"path": "/home.do", "name": "í™ˆ"},
            {"path": "/op/opc/selectOpenData.do", "name": "ë°ì´í„° ëª©ë¡"},
            {"path": "/st/stc/selectStcList.do", "name": "í†µê³„ ëª©ë¡"},
            {"path": "/op/opi/selectOpenApiList.do", "name": "Open API"},
            {"path": "/cm/cm_info.do?pgmId=HIRAA010000000000", "name": "ì„œë¹„ìŠ¤ ì†Œê°œ"},
        ]

        for page_info in main_pages:
            try:
                url = f"{self.base_url}{page_info['path']}"
                print(f"   í¬ë¡¤ë§: {page_info['name']} - {url}")

                # í˜ì´ì§€ ì´ë™
                try:
                    page.goto(url, wait_until="domcontentloaded", timeout=15000)
                    time.sleep(1)
                except Exception as e:
                    print(f"      âš ï¸ í˜ì´ì§€ ë¡œë”© ì˜¤ë¥˜, ê³„ì† ì§„í–‰: {e}")

                # í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
                page_data = self._extract_page_data(page, page_info['name'], url)
                if page_data:
                    self.data["pages"].append(page_data)
                    print(f"      âœ“ ìˆ˜ì§‘ ì™„ë£Œ")

            except Exception as e:
                print(f"      âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue

    def _extract_page_data(self, page: Page, page_name: str, url: str) -> Optional[Dict]:
        """í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title = page.title()

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_selectors = [
                'div.content',
                'div.container',
                'div.main-content',
                'div#content',
                'main',
                'article',
                'div.board',
            ]

            content_text = ""
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    content_text = content.get_text(separator='\n', strip=True)[:3000]
                    break

            # ì „ì²´ í…ìŠ¤íŠ¸ (fallback)
            if not content_text:
                body = soup.find('body')
                if body:
                    content_text = body.get_text(separator='\n', strip=True)[:3000]

            # í…Œì´ë¸” ì¶”ì¶œ
            tables = []
            for table in soup.find_all('table')[:5]:
                table_data = self._extract_table_data(table)
                if table_data:
                    tables.append(table_data)

            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True)[:100]:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if text and 3 < len(text) < 100:
                    full_url = href
                    if not href.startswith('http'):
                        if href.startswith('/'):
                            full_url = f"{self.base_url}{href}"
                        else:
                            full_url = f"{self.base_url}/{href}"

                    links.append({
                        "text": text,
                        "url": full_url
                    })

            # ì¤‘ë³µ ë§í¬ ì œê±°
            unique_links = []
            seen_urls = set()
            for link in links:
                if link["url"] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link["url"])

            return {
                "name": page_name,
                "url": url,
                "title": title,
                "content_preview": content_text[:500] if content_text else "",
                "content_length": len(content_text),
                "tables_count": len(tables),
                "tables": tables,
                "links_count": len(unique_links),
                "links": unique_links[:20],  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
                "crawled_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"         í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def _extract_table_data(self, table) -> Optional[Dict]:
        """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        try:
            headers = []
            rows = []

            # í—¤ë” ì¶”ì¶œ
            thead = table.find('thead')
            if thead:
                header_cells = thead.find_all(['th', 'td'])
                headers = [cell.get_text(strip=True) for cell in header_cells if cell.get_text(strip=True)]

            # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ
            if not headers:
                first_row = table.find('tr')
                if first_row:
                    header_cells = first_row.find_all(['th', 'td'])
                    if header_cells and all(cell.name == 'th' for cell in header_cells):
                        headers = [cell.get_text(strip=True) for cell in header_cells]

            # ë°ì´í„° í–‰ ì¶”ì¶œ
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr')[:15]:  # ìµœëŒ€ 15í–‰
                cells = tr.find_all(['td', 'th'])
                if cells:
                    row = [cell.get_text(strip=True) for cell in cells]
                    # ë¹ˆ í–‰ì´ ì•„ë‹ˆë©´ ì¶”ê°€
                    if any(cell for cell in row):
                        rows.append(row)

            if rows:
                return {
                    "headers": headers,
                    "rows": rows,
                    "row_count": len(rows),
                    "column_count": len(rows[0]) if rows else 0
                }

            return None

        except Exception as e:
            return None

    def _collect_statistics(self):
        """í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
        stats = {
            "total_menus": len(self.data["menus"]),
            "total_pages": len(self.data["pages"]),
            "total_links": sum(page.get("links_count", 0) for page in self.data["pages"]),
            "total_tables": sum(page.get("tables_count", 0) for page in self.data["pages"]),
            "total_menu_links": sum(len(menu.get("links", [])) for menu in self.data["menus"]),
        }

        self.data["statistics"] = stats
        print(f"   ë©”ë‰´: {stats['total_menus']}ê°œ")
        print(f"   ë©”ë‰´ ë§í¬: {stats['total_menu_links']}ê°œ")
        print(f"   í˜ì´ì§€: {stats['total_pages']}ê°œ")
        print(f"   í˜ì´ì§€ ë§í¬: {stats['total_links']}ê°œ")
        print(f"   í…Œì´ë¸”: {stats['total_tables']}ê°œ")

    def _finalize_data(self):
        """ë°ì´í„° ì •ë¦¬ ë° ìµœì¢…í™”"""
        self.data["site_info"]["completed_at"] = datetime.now().isoformat()
        print("   ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")

    def save_json(self, output_path: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size
        print(f"\nâœ… JSON ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")

    def save_pretty_json(self, output_path: str):
        """ì½ê¸° ì‰¬ìš´ JSON ì €ì¥ (ìš”ì•½ í¬í•¨)"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ìš”ì•½ ë°ì´í„° ìƒì„±
        summary = {
            "crawl_summary": {
                "site": self.base_url,
                "crawled_at": self.data["site_info"]["crawled_at"],
                "completed_at": self.data["site_info"].get("completed_at", ""),
                "engine": "Playwright",
                "statistics": self.data["statistics"]
            },
            "menu_structure": [
                {
                    "id": menu["id"],
                    "name": menu["text"][:60],
                    "links_count": len(menu["links"]),
                    "sample_links": menu["links"][:3]
                }
                for menu in self.data["menus"]
            ],
            "pages_overview": [
                {
                    "name": page["name"],
                    "url": page["url"],
                    "title": page["title"],
                    "content_length": page.get("content_length", 0),
                    "tables": page["tables_count"],
                    "links": page["links_count"]
                }
                for page in self.data["pages"]
            ],
            "full_data": self.data
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        file_size = output_path.stat().st_size
        print(f"âœ… ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("HIRA ì˜¤í”ˆë°ì´í„° í¬í„¸ Playwright í¬ë¡¤ëŸ¬ v1.0")
    print("="*80 + "\n")

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = HIRACrawler(headless=True, timeout=30000)

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler.crawl()

        # JSON ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("/home/user/bigdataptAI/hira_crawler/output")
        output_dir.mkdir(exist_ok=True)

        # 1. ì „ì²´ ë°ì´í„°
        crawler.save_json(output_dir / f"hira_crawled_data_{timestamp}.json")

        # 2. ìš”ì•½ ë°ì´í„°
        crawler.save_pretty_json(output_dir / f"hira_summary_{timestamp}.json")

        print("\n" + "="*80)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
