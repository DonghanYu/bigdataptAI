# HIRA λΉ…λ°μ΄ν„° μ±—λ΄‡ - λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“

**5λ¶„ μ•μ— μ‹μ‘ν•κΈ°**

---

## π€ 3λ‹¨κ³„λ΅ μ‹μ‘ν•κΈ°

### Step 1: ν™κ²½ ν™•μΈ (1λ¶„)

```bash
# GPU ν™•μΈ
nvidia-smi

# Python λ²„μ „ ν™•μΈ (3.8 μ΄μƒ)
python --version

# ν•„μ λΌμ΄λΈλ¬λ¦¬ ν™•μΈ
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"
```

**λ¬Έμ κ°€ μλ‹¤λ©΄?**
```bash
# λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ (νμ‡„λ§μ—μ„λ” λ―Έλ¦¬ μ¤€λΉ„ ν•„μ”)
pip install torch==2.1.0 transformers peft accelerate datasets tqdm
```

### Step 2: μ„¤μ • νμΌ μμ • (2λ¶„)

```bash
# μ„¤μ • νμΌ μ—΄κΈ°
vi config/config.yaml

# λλ”
nano config/config.yaml
```

**ν•„μ μμ • μ‚¬ν•­:**

```yaml
paths:
  # SOLAR λ¨λΈ κ²½λ΅ (λ³ΈμΈ ν™κ²½μ— λ§κ² μμ •)
  model_path: "/μ‹¤μ /κ²½λ΅/solar_10.7b_package/model"

  # λ°μ΄ν„° κ²½λ΅ (μƒλ€ κ²½λ΅ μ‚¬μ© κ°€λ¥)
  data:
    cleaned: "workspace/data/hira/cleaned"
```

**κ²½λ΅ μμ‹:**

| ν™κ²½ | κ²½λ΅ μμ‹ |
|------|-----------|
| Backend.AI | `/home/work/LLM_Meditron/bigdataAI/solar_10.7b_package/model` |
| λ΅μ»¬ | `/Users/name/projects/solar_10.7b_package/model` |
| μƒλ€ κ²½λ΅ | `solar_10.7b_package/model` |

### Step 3: JupyterLab μ‹¤ν–‰ (2λ¶„)

```bash
# JupyterLab μ‹μ‘
jupyter lab

# ν„°λ―Έλ„μ—μ„ ν‘μ‹λ URL λ³µμ‚¬
# μ: http://localhost:8888/?token=xxxxx
```

**λΈλΌμ°μ €μ—μ„:**

1. `notebooks/HIRA_Training_SOLAR_LoRA.ipynb` μ—΄κΈ°
2. μ²« μ…€λ¶€ν„° μμ°¨ μ‹¤ν–‰ (Shift + Enter)
3. GPU λ° κ²½λ΅ ν™•μΈ
4. ν•™μµ μ‹μ‘!

---

## π“ μ²΄ν¬λ¦¬μ¤νΈ

μ‹μ‘ μ „μ— λ‹¤μμ„ ν™•μΈν•μ„Έμ”:

```
β… GPU μ‚¬μ© κ°€λ¥ (nvidia-smi ν™•μΈ)
β… SOLAR-10.7B λ¨λΈ λ‹¤μ΄λ΅λ“ μ™„λ£
β… config.yamlμ—μ„ model_path μμ •
β… JupyterLab μ‹¤ν–‰ κ°€λ¥
β… λ””μ¤ν¬ μ—¬μ  κ³µκ°„ 100GB μ΄μƒ
```

---

## π― μ²« μ‹¤ν–‰ ν”λ΅μ°

### μµμ… A: ν…μ¤νΈ λ°μ΄ν„°λ΅ λΉ λ¥Έ μ‹¤ν—

```python
# λ…ΈνΈλ¶ μ‹¤ν–‰ μ‹ μλ™μΌλ΅ ν…μ¤νΈ λ°μ΄ν„° μƒμ„±
# 300κ° μƒν”, ν•™μµ μ‹κ°„ μ•½ 15-20λ¶„
```

**μ¥μ **: μ¦‰μ‹ μ‹μ‘ κ°€λ¥
**λ‹¨μ **: μ‹¤μ  μ„±λ¥ μΈ΅μ • λ¶κ°€

### μµμ… B: HIRA μ‹¤μ  λ°μ΄ν„° μ‚¬μ©

1. **λ°μ΄ν„° μ¤€λΉ„** (1-2μ£Ό)
   - `docs/01_HIRA_DATA_PREPARATION_GUIDE.md` μ°Έμ΅°
   - HIRA μ›Ήμ‚¬μ΄νΈμ—μ„ λ°μ΄ν„° λ‹¤μ΄λ΅λ“
   - QA ν•μ‹μΌλ΅ λ³€ν™

2. **λ°μ΄ν„° μ •μ **
   ```bash
   python 01_data_cleaning.py
   ```

3. **ν•™μµ μ‹μ‘**
   - λ…ΈνΈλ¶ μ‹¤ν–‰ λλ” μ¤ν¬λ¦½νΈ μ‚¬μ©

---

## π–¥οΈ μ‹¤ν–‰ λ°©λ²• λΉ„κµ

| λ°©λ²• | λ‚μ΄λ„ | μ¥μ  | λ‹¨μ  |
|------|--------|------|------|
| **JupyterLab** (κ¶μ¥) | β­ μ‰¬μ›€ | - λ‹¨κ³„λ³„ μ‹¤ν–‰<br>- μ‹κ°ν™” μ§€μ›<br>- λ””λ²„κΉ… μ©μ΄ | - μ„λ²„ ν•„μ” |
| **Python μ¤ν¬λ¦½νΈ** | β­β­ λ³΄ν†µ | - μλ™ μ‹¤ν–‰<br>- λ°±κ·ΈλΌμ΄λ“ κ°€λ¥ | - μ¤‘κ°„ ν™•μΈ μ–΄λ ¤μ›€ |
| **Flask μ›Ή** | β­β­β­ μ–΄λ ¤μ›€ | - μ›Ή UI<br>- λ‹¤μ¤‘ μ‚¬μ©μ | - μ„¤μ • λ³µμ΅ |

### JupyterLab (κ¶μ¥)

```bash
# 1. μ‹¤ν–‰
jupyter lab

# 2. λ…ΈνΈλ¶ μ—΄κΈ°
notebooks/HIRA_Training_SOLAR_LoRA.ipynb

# 3. μ…€ μ‹¤ν–‰
Shift + Enter
```

### Python μ¤ν¬λ¦½νΈ

```bash
# λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰
nohup python 02_train_with_validation.py > training.log 2>&1 &

# λ΅κ·Έ ν™•μΈ
tail -f training.log
```

### Flask μ›Ή μΈν„°νμ΄μ¤ (ν•™μµ ν›„)

```bash
# μ„λ²„ μ‹μ‘
python 03_improved_interface.py

# λΈλΌμ°μ € μ ‘μ†
http://localhost:8888
```

---

## β±οΈ μμƒ μ†μ” μ‹κ°„

| λ‹¨κ³„ | ν…μ¤νΈ λ°μ΄ν„° | μ‹¤μ  λ°μ΄ν„° |
|------|--------------|------------|
| ν™κ²½ μ„¤μ • | 5λ¶„ | 5λ¶„ |
| λ°μ΄ν„° μ¤€λΉ„ | μλ™ (1λ¶„) | 1-2μ£Ό |
| λ¨λΈ ν•™μµ | 15-20λ¶„ | 2-3μ‹κ°„ |
| ν‰κ°€ ν…μ¤νΈ | 5λ¶„ | 30λ¶„ |
| **μ΄ μ‹κ°„** | **~30λ¶„** | **2μ£Ό + 4μ‹κ°„** |

*A100 80G x2 κΈ°μ¤€*

---

## π” μ‹¤ν–‰ μ¤‘ ν™•μΈμ‚¬ν•­

### 1. GPU λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§

```bash
# λ³„λ„ ν„°λ―Έλ„μ—μ„ μ‹¤ν–‰
watch -n 1 nvidia-smi
```

**μ •μƒ λ²”μ„:**
- ν•™μµ μ¤‘: 25-35GB per GPU
- μ¶”λ΅  μ¤‘: 20-25GB per GPU

### 2. ν•™μµ μ§„ν–‰ ν™•μΈ

λ…ΈνΈλ¶μ—μ„ Progress Bar ν™•μΈ:
```
Epoch 1/10: 100%|β–β–β–β–β–β–β–β–β–β–| 135/135 [15:23<00:00, 6.84s/it]
Train Loss: 1.245, Val Loss: 1.182
```

### 3. μ²΄ν¬ν¬μΈνΈ ν™•μΈ

```bash
# μ €μ¥λ λ¨λΈ ν™•μΈ
ls -lh workspace/models/solar_hira_lora/

# μ¶λ ¥ μμ‹:
# best_model/
# checkpoint-epoch-2/
# checkpoint-epoch-4/
# training_history.json
```

---

## β“ μμ£Ό λ¬»λ” μ§λ¬Έ (FAQ)

### Q1: "CUDA out of memory" μ¤λ¥κ°€ λ°μƒν•΄μ”

```yaml
# config.yaml μμ •
training:
  batch_size: 1  # 2μ—μ„ 1λ΅ μ¤„μ΄κΈ°
  gradient_accumulation_steps: 8  # 4μ—μ„ 8λ΅ λλ¦¬κΈ°
```

### Q2: λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†λ‹¤κ³  λ‚μ™€μ”

```bash
# κ²½λ΅ ν™•μΈ
ls solar_10.7b_package/model/config.json

# μ—†μΌλ©΄ μ λ€ κ²½λ΅ μ‚¬μ©
# config.yaml:
# model_path: "/μ „μ²΄/κ²½λ΅/solar_10.7b_package/model"
```

### Q3: ν•™μµμ΄ λ„λ¬΄ λλ ¤μ”

**ν™•μΈμ‚¬ν•­:**
1. GPU μ‚¬μ© μ¤‘μΈμ§€ ν™•μΈ: `torch.cuda.is_available()`
2. bfloat16 μ‚¬μ© μ¤‘μΈμ§€ ν™•μΈ (A100 μµμ ν™”)
3. Gradient checkpointing ν™μ„±ν™”

### Q4: ν…μ¤νΈ λ°μ΄ν„°λ” μ–΄λ””μ„ λ°›λ‚μ”?

λ…ΈνΈλ¶ μ‹¤ν–‰ μ‹ μλ™ μƒμ„±λ©λ‹λ‹¤.
λλ”:
```bash
# docs/01_HIRA_DATA_PREPARATION_GUIDE.md μ°Έμ΅°
# HIRA μ›Ήμ‚¬μ΄νΈμ—μ„ μ‹¤μ  λ°μ΄ν„° λ‹¤μ΄λ΅λ“
```

---

## π“ λ„μ›€μ΄ ν•„μ”ν•λ©΄

1. **λ¬Έμ„ ν™•μΈ**
   - `PROJECT_README.md` - μ „μ²΄ κ°€μ΄λ“
   - `docs/01_HIRA_DATA_PREPARATION_GUIDE.md` - λ°μ΄ν„° κ°€μ΄λ“

2. **λ΅κ·Έ ν™•μΈ**
   - λ…ΈνΈλ¶: μ…€ μ¶λ ¥ ν™•μΈ
   - μ¤ν¬λ¦½νΈ: `training.log` νμΌ ν™•μΈ

3. **μ΄μ λ³΄κ³ **
   - GitHub Issues
   - λλ” ν”„λ΅μ νΈ λ‹΄λ‹Ήμμ—κ² λ¬Έμ

---

## π‰ μ„±κ³µμ μΌλ΅ μ‹μ‘ν–λ‹¤λ©΄

λ‹¤μ λ‹¨κ³„:

1. β… `notebooks/HIRA_Interface.ipynb`λ΅ λ¨λΈ ν…μ¤νΈ
2. β… μ‹¤μ  HIRA λ°μ΄ν„° μ¤€λΉ„ μ‹μ‘
3. β… ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹¤ν—
4. β… ν€μ›κ³Ό κ²°κ³Ό κ³µμ 

**Happy Training! π€**
