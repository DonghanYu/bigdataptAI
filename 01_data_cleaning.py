#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„° í´ë¦¬ë‹ ë° Train/Val/Test ë¶„í• 
- ì¤‘ë³µ ì œê±°
- í’ˆì§ˆ í•„í„°ë§
- ì „ëµì  ë¶„í• 
"""

import json
import hashlib
from pathlib import Path
from collections import Counter
import random

def load_jsonl(file_path):
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line.strip()))
            except:
                continue
    return data

def save_jsonl(data, file_path):
    """JSONL íŒŒì¼ ì €ì¥"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def get_hash(text):
    """í…ìŠ¤íŠ¸ í•´ì‹œê°’ ìƒì„±"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def clean_data(data):
    """ë°ì´í„° í´ë¦¬ë‹"""
    print("\n" + "="*70)
    print("ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘")
    print("="*70)
    
    original_count = len(data)
    print(f"\nì›ë³¸ ë°ì´í„°: {original_count}ê°œ")
    
    # 1. ì¤‘ë³µ ì œê±° (ì™„ì „ ì¤‘ë³µ)
    seen_outputs = set()
    unique_data = []
    duplicates = 0
    
    for item in data:
        output_hash = get_hash(item['output'])
        if output_hash not in seen_outputs:
            seen_outputs.add(output_hash)
            unique_data.append(item)
        else:
            duplicates += 1
    
    print(f"  â”œâ”€ ì™„ì „ ì¤‘ë³µ ì œê±°: {duplicates}ê°œ")
    data = unique_data
    
    # 2. í’ˆì§ˆ í•„í„°ë§
    quality_filtered = []
    
    for item in data:
        output = item['output'].strip()
        instruction = item['instruction'].strip()
        
        # í•„í„°ë§ ì¡°ê±´
        if len(output) < 10:  # ë„ˆë¬´ ì§§ì€ ë‹µë³€
            continue
        if len(instruction) < 5:  # ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸
            continue
        if output.count('ì´ê²ƒì´ ì¤‘ìš”í•œ ì´ìœ ëŠ”') > 1:  # ì¤‘ë³µ í…œí”Œë¦¿
            continue
        
        quality_filtered.append(item)
    
    removed = len(data) - len(quality_filtered)
    print(f"  â”œâ”€ í’ˆì§ˆ í•„í„°ë§: {removed}ê°œ ì œê±°")
    data = quality_filtered
    
    # 3. í…œí”Œë¦¿ ë¬¸êµ¬ ì •ë¦¬
    cleaned_data = []
    
    for item in data:
        output = item['output']
        
        # ê³¼ë„í•œ ë°˜ë³µ ë¬¸êµ¬ ì œê±°
        if output.count('\n\nì´ê²ƒì´ ì¤‘ìš”í•œ ì´ìœ ëŠ”') == 1:
            # í…œí”Œë¦¿ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ë³€ê²½
            output = output.replace(
                '\n\nì´ê²ƒì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ê±´ê°•ë³´í—˜ ì œë„ì™€ ë°ì´í„° ë¶„ì„ì˜ ê¸°ì´ˆê°€ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.',
                ''
            ).strip()
        
        item['output'] = output
        cleaned_data.append(item)
    
    print(f"  â””â”€ ìµœì¢… ì •ì œ ë°ì´í„°: {len(cleaned_data)}ê°œ")
    
    return cleaned_data

def analyze_data(data, title="ë°ì´í„° ë¶„ì„"):
    """ë°ì´í„° í†µê³„ ì¶œë ¥"""
    print(f"\nğŸ“Š {title}")
    print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(data)}")
    
    # ë‹µë³€ ê¸¸ì´ ë¶„ì„
    output_lengths = [len(item['output']) for item in data]
    print(f"  ë‹µë³€ ê¸¸ì´:")
    print(f"    í‰ê· : {sum(output_lengths)/len(output_lengths):.1f}ì")
    print(f"    ìµœì†Œ: {min(output_lengths)}ì")
    print(f"    ìµœëŒ€: {max(output_lengths)}ì")
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
    instructions = [item['instruction'] for item in data]
    first_words = [inst.split()[0] if inst.split() else '' for inst in instructions]
    common_starts = Counter(first_words).most_common(5)
    print(f"  ë¹ˆë²ˆí•œ ì§ˆë¬¸ ì‹œì‘ì–´:")
    for word, count in common_starts:
        print(f"    '{word}': {count}ê°œ")

def split_data(data, train_ratio=0.8, val_ratio=0.1, seed=42):
    """Train/Val/Test ë¶„í• """
    print("\n" + "="*70)
    print("ë°ì´í„° ë¶„í• ")
    print("="*70)
    
    random.seed(seed)
    random.shuffle(data)
    
    total = len(data)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)
    
    train_data = data[:train_size]
    val_data = data[train_size:train_size + val_size]
    test_data = data[train_size + val_size:]
    
    print(f"\në¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_data)}ê°œ ({len(train_data)/total*100:.1f}%)")
    print(f"  Val:   {len(val_data)}ê°œ ({len(val_data)/total*100:.1f}%)")
    print(f"  Test:  {len(test_data)}ê°œ ({len(test_data)/total*100:.1f}%)")
    
    return train_data, val_data, test_data

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    input_file = Path("all_data_expanded.jsonl")  # ì…ë ¥ íŒŒì¼
    output_dir = Path("cleaned_data")
    output_dir.mkdir(exist_ok=True)
    
    print("="*70)
    print("HIRA ë°ì´í„° í´ë¦¬ë‹ & ë¶„í• ")
    print("="*70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ: {input_file}")
    data = load_jsonl(input_file)
    analyze_data(data, "ì›ë³¸ ë°ì´í„°")
    
    # 2. í´ë¦¬ë‹
    cleaned_data = clean_data(data)
    analyze_data(cleaned_data, "ì •ì œ ë°ì´í„°")
    
    # 3. ë¶„í• 
    train_data, val_data, test_data = split_data(cleaned_data)
    
    # 4. ì €ì¥
    print("\n" + "="*70)
    print("íŒŒì¼ ì €ì¥")
    print("="*70)
    
    train_file = output_dir / "train.jsonl"
    val_file = output_dir / "val.jsonl"
    test_file = output_dir / "test.jsonl"
    
    save_jsonl(train_data, train_file)
    save_jsonl(val_data, val_file)
    save_jsonl(test_data, test_file)
    
    print(f"  âœ… Train: {train_file}")
    print(f"  âœ… Val:   {val_file}")
    print(f"  âœ… Test:  {test_file}")
    
    # 5. ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "="*70)
    print("ìƒ˜í”Œ ë°ì´í„° í™•ì¸")
    print("="*70)
    
    for i, item in enumerate(train_data[:3], 1):
        print(f"\n[Train ìƒ˜í”Œ {i}]")
        print(f"Q: {item['instruction']}")
        print(f"A: {item['output'][:100]}...")
    
    print("\n" + "="*70)
    print("âœ… ì™„ë£Œ!")
    print("="*70)

if __name__ == "__main__":
    main()
