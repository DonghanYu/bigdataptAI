{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡ - ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "**í”„ë¡œì íŠ¸**: HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡  \n",
    "**ëª©ì **: í•™ìŠµëœ ëª¨ë¸ì„ ì§ì ‘ í…ŒìŠ¤íŠ¸í•˜ê³  í‰ê°€  \n",
    "**í™˜ê²½**: JupyterLab\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "1. [ëª¨ë¸ ë¡œë“œ](#1-ëª¨ë¸-ë¡œë“œ)\n",
    "2. [ì¸í„°ë™í‹°ë¸Œ ì§ˆì˜ì‘ë‹µ](#2-ì¸í„°ë™í‹°ë¸Œ-ì§ˆì˜ì‘ë‹µ)\n",
    "3. [ë°°ì¹˜ í‰ê°€](#3-ë°°ì¹˜-í‰ê°€)\n",
    "4. [ì„±ëŠ¥ ë¶„ì„](#4-ì„±ëŠ¥-ë¶„ì„)\n",
    "5. [ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰](#5-ì›¹-ì¸í„°í˜ì´ìŠ¤-ì‹¤í–‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "### 1.1 í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes ì°¨ë‹¨\n",
    "import sys\n",
    "import os\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "sys.modules['bitsandbytes'] = None\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ\n",
    "BASE_MODEL_PATH = PROJECT_ROOT / \"solar_10.7b_package\" / \"model\"\n",
    "LORA_MODEL_PATH = PROJECT_ROOT / \"workspace\" / \"models\" / \"solar_hira_lora\" / \"best_model\"\n",
    "\n",
    "# ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©:\n",
    "# BASE_MODEL_PATH = Path(\"/path/to/solar_10.7b_package/model\")\n",
    "# LORA_MODEL_PATH = Path(\"/path/to/workspace/models/solar_hira_lora/best_model\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ğŸ“‚ ê²½ë¡œ ì„¤ì •:\")\n",
    "print(f\"  ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL_PATH}\")\n",
    "print(f\"  LoRA ëª¨ë¸: {LORA_MODEL_PATH}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# ê²½ë¡œ ì¡´ì¬ í™•ì¸\n",
    "if not BASE_MODEL_PATH.exists():\n",
    "    print(f\"âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {BASE_MODEL_PATH}\")\n",
    "if not LORA_MODEL_PATH.exists():\n",
    "    print(f\"âš ï¸ LoRA ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {LORA_MODEL_PATH}\")\n",
    "    print(f\"  â†’ ë¨¼ì € HIRA_Training_SOLAR_LoRA.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (1-2ë¶„ ì†Œìš”)\\n\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"[1/3] í† í¬ë‚˜ì´ì € ë¡œë“œ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(BASE_MODEL_PATH),\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "print(\"\\n[2/3] ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(BASE_MODEL_PATH),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({base_model.num_parameters() / 1e9:.2f}B params)\")\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ë¡œë“œ\n",
    "print(\"\\n[3/3] LoRA ì–´ëŒ‘í„° ë¡œë“œ...\")\n",
    "model = PeftModel.from_pretrained(base_model, str(LORA_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ì¶”ë¡  í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    question,\n",
    "    instruction=\"ê±´ê°•ë³´í—˜ ë¹…ë°ì´í„° ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.3,\n",
    "    top_p=0.85,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.15\n",
    "):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    question : str\n",
    "        ì‚¬ìš©ì ì§ˆë¬¸\n",
    "    instruction : str\n",
    "        ì‹œìŠ¤í…œ ì§€ì‹œë¬¸\n",
    "    max_new_tokens : int\n",
    "        ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    temperature : float\n",
    "        ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ë¡ ì )\n",
    "    top_p : float\n",
    "        Nucleus sampling ì„ê³„ê°’\n",
    "    top_k : int\n",
    "        Top-K sampling í¬ê¸°\n",
    "    repetition_penalty : float\n",
    "        ë°˜ë³µ í˜ë„í‹°\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : ìƒì„±ëœ ë‹µë³€\n",
    "    \"\"\"\n",
    "    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    generation_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Response ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "    metadata = {\n",
    "        \"generation_time\": generation_time,\n",
    "        \"tokens_generated\": len(outputs[0]) - len(inputs[\"input_ids\"][0]),\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    \n",
    "    return response, metadata\n",
    "\n",
    "print(\"âœ… ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì¸í„°ë™í‹°ë¸Œ ì§ˆì˜ì‘ë‹µ\n",
    "\n",
    "### 2.1 ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_question = \"2023ë…„ MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\"\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {test_question}\\n\")\n",
    "response, metadata = generate_response(test_question)\n",
    "print(f\"ë‹µë³€: {response}\\n\")\n",
    "print(f\"ë©”íƒ€ë°ì´í„°:\")\n",
    "print(f\"  - ìƒì„± ì‹œê°„: {metadata['generation_time']:.2f}ì´ˆ\")\n",
    "print(f\"  - ìƒì„± í† í° ìˆ˜: {metadata['tokens_generated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ìœ„ì ¯ ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI ìœ„ì ¯ ìƒì„±\n",
    "question_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...',\n",
    "    description='ì§ˆë¬¸:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì¡°ì ˆ ìŠ¬ë¼ì´ë”\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "max_tokens_slider = widgets.IntSlider(\n",
    "    value=256,\n",
    "    min=64,\n",
    "    max=512,\n",
    "    step=32,\n",
    "    description='Max Tokens:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=0.85,\n",
    "    min=0.5,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Top-p:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='ë‹µë³€ ìƒì„±',\n",
    "    button_style='primary',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬\n",
    "def on_generate_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        question = question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            print(\"âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ”„ ë‹µë³€ ìƒì„± ì¤‘...\\n\")\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        response, metadata = generate_response(\n",
    "            question,\n",
    "            max_new_tokens=max_tokens_slider.value,\n",
    "            temperature=temperature_slider.value,\n",
    "            top_p=top_p_slider.value\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥ (HTML ìŠ¤íƒ€ì¼)\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"background-color: #f0f0f0; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
    "            <h3 style=\"color: #2c3e50; margin-top: 0;\">ì§ˆë¬¸</h3>\n",
    "            <p style=\"font-size: 16px; color: #34495e;\">{question}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 10px; margin-bottom: 15px;\">\n",
    "            <h3 style=\"color: #1b5e20; margin-top: 0;\">ë‹µë³€</h3>\n",
    "            <p style=\"font-size: 16px; color: #2e7d32; line-height: 1.6;\">{response}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background-color: #fff3e0; padding: 10px; border-radius: 5px;\">\n",
    "            <p style=\"margin: 5px 0; font-size: 14px;\"><strong>ìƒì„± ì‹œê°„:</strong> {metadata['generation_time']:.2f}ì´ˆ</p>\n",
    "            <p style=\"margin: 5px 0; font-size: 14px;\"><strong>ìƒì„± í† í°:</strong> {metadata['tokens_generated']}ê°œ</p>\n",
    "            <p style=\"margin: 5px 0; font-size: 14px;\"><strong>Temperature:</strong> {metadata['temperature']}</p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "# UI ë ˆì´ì•„ì›ƒ\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>ğŸ¥ HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡</h2>\"),\n",
    "    question_input,\n",
    "    widgets.HTML(\"<h3>ìƒì„± íŒŒë¼ë¯¸í„°</h3>\"),\n",
    "    temperature_slider,\n",
    "    max_tokens_slider,\n",
    "    top_p_slider,\n",
    "    generate_button,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (ì…€ì„ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ ê°€ëŠ¥)\n",
    "print(\"ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit')\\n\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nì§ˆë¬¸: \").strip()\n",
    "    \n",
    "    if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:\n",
    "        print(\"\\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "    \n",
    "    if not question:\n",
    "        continue\n",
    "    \n",
    "    response, metadata = generate_response(question)\n",
    "    \n",
    "    print(f\"\\në‹µë³€: {response}\")\n",
    "    print(f\"\\n[ìƒì„± ì‹œê°„: {metadata['generation_time']:.2f}ì´ˆ]\")\n",
    "    \n",
    "    # ëŒ€í™” ê¸°ë¡ ì €ì¥\n",
    "    conversation_history.append({\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "# ëŒ€í™” ê¸°ë¡ ì €ì¥\n",
    "if conversation_history:\n",
    "    output_file = PROJECT_ROOT / \"workspace\" / \"logs\" / f\"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(conversation_history, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nëŒ€í™” ê¸°ë¡ ì €ì¥: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°°ì¹˜ í‰ê°€\n",
    "\n",
    "### 3.1 ë¯¸ë¦¬ ì •ì˜ëœ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸\n",
    "test_questions = [\n",
    "    \"2023ë…„ MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\",\n",
    "    \"DRGê°€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ê³ í˜ˆì•• í™˜ì ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\",\n",
    "    \"ê±´ê°•ë³´í—˜ ì´ ì§„ë£Œë¹„ëŠ”?\",\n",
    "    \"ìš”ì–‘ë³‘ì›ê³¼ ì¢…í•©ë³‘ì›ì˜ ì°¨ì´ëŠ”?\",\n",
    "    \"ë³¸ì¸ë¶€ë‹´ê¸ˆì´ë€?\",\n",
    "    \"ì‹ í¬ê´„ìˆ˜ê°€ì œë„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "    \"ê±´ê°•ê²€ì§„ ëŒ€ìƒìëŠ”?\",\n",
    "    \"ì•” í™˜ì ì§„ë£Œë¹„ ì§€ì› ì œë„ëŠ”?\",\n",
    "    \"ì²˜ë°©ì „ ìœ íš¨ê¸°ê°„ì€?\"\n",
    "]\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {len(test_questions)}ê°œë¥¼ í‰ê°€í•©ë‹ˆë‹¤...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"[{i}/{len(test_questions)}] {question}\")\n",
    "    \n",
    "    response, metadata = generate_response(question, temperature=0.3)\n",
    "    \n",
    "    print(f\"ë‹µë³€: {response[:100]}...\")\n",
    "    print(f\"ìƒì„± ì‹œê°„: {metadata['generation_time']:.2f}ì´ˆ\\n\")\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… ë°°ì¹˜ í‰ê°€ ì™„ë£Œ\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "result_file = PROJECT_ROOT / \"workspace\" / \"logs\" / f\"batch_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "result_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ê²°ê³¼ ì €ì¥: {result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ìƒì„± ì‹œê°„ ë¶„ì„\n",
    "generation_times = [r['metadata']['generation_time'] for r in results]\n",
    "tokens_generated = [r['metadata']['tokens_generated'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ìƒì„± ì‹œê°„ ë¶„í¬\n",
    "ax1.bar(range(len(generation_times)), generation_times, color='skyblue')\n",
    "ax1.set_xlabel('ì§ˆë¬¸ ë²ˆí˜¸')\n",
    "ax1.set_ylabel('ìƒì„± ì‹œê°„ (ì´ˆ)')\n",
    "ax1.set_title('ì§ˆë¬¸ë³„ ìƒì„± ì‹œê°„')\n",
    "ax1.axhline(np.mean(generation_times), color='red', linestyle='--', label=f'í‰ê· : {np.mean(generation_times):.2f}ì´ˆ')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# í† í° ìˆ˜ ë¶„í¬\n",
    "ax2.bar(range(len(tokens_generated)), tokens_generated, color='lightgreen')\n",
    "ax2.set_xlabel('ì§ˆë¬¸ ë²ˆí˜¸')\n",
    "ax2.set_ylabel('ìƒì„± í† í° ìˆ˜')\n",
    "ax2.set_title('ì§ˆë¬¸ë³„ ìƒì„± í† í° ìˆ˜')\n",
    "ax2.axhline(np.mean(tokens_generated), color='red', linestyle='--', label=f'í‰ê· : {np.mean(tokens_generated):.1f}ê°œ')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š í†µê³„:\")\n",
    "print(f\"  í‰ê·  ìƒì„± ì‹œê°„: {np.mean(generation_times):.2f}ì´ˆ\")\n",
    "print(f\"  ìµœì†Œ ìƒì„± ì‹œê°„: {np.min(generation_times):.2f}ì´ˆ\")\n",
    "print(f\"  ìµœëŒ€ ìƒì„± ì‹œê°„: {np.max(generation_times):.2f}ì´ˆ\")\n",
    "print(f\"  í‰ê·  í† í° ìˆ˜: {np.mean(tokens_generated):.1f}ê°œ\")\n",
    "print(f\"  í† í°/ì´ˆ: {np.mean(tokens_generated) / np.mean(generation_times):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì„±ëŠ¥ ë¶„ì„\n",
    "\n",
    "### 4.1 ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ í‰ê°€ë¥¼ ìœ„í•œ UI\n",
    "def create_evaluation_ui(results):\n",
    "    \"\"\"\n",
    "    ê° ë‹µë³€ì— ëŒ€í•œ í‰ê°€ UI ìƒì„±\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ì§ˆë¬¸ {i+1}/{len(results)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nQ: {result['question']}\")\n",
    "        print(f\"\\nA: {result['response']}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        \n",
    "        # í‰ê°€ ì…ë ¥\n",
    "        print(\"\\ní‰ê°€ (1-5ì ):\")\n",
    "        print(\"  1: ë§¤ìš° ë‚˜ì¨\")\n",
    "        print(\"  2: ë‚˜ì¨\")\n",
    "        print(\"  3: ë³´í†µ\")\n",
    "        print(\"  4: ì¢‹ìŒ\")\n",
    "        print(\"  5: ë§¤ìš° ì¢‹ìŒ\")\n",
    "        \n",
    "        try:\n",
    "            score = int(input(\"\\nì ìˆ˜: \").strip())\n",
    "            if score < 1 or score > 5:\n",
    "                score = 3\n",
    "        except:\n",
    "            score = 3\n",
    "        \n",
    "        comment = input(\"ì½”ë©˜íŠ¸ (ì„ íƒ): \").strip()\n",
    "        \n",
    "        evaluations.append({\n",
    "            \"question\": result['question'],\n",
    "            \"response\": result['response'],\n",
    "            \"score\": score,\n",
    "            \"comment\": comment\n",
    "        })\n",
    "        \n",
    "        print(\"\\nâœ… í‰ê°€ ì €ì¥ë¨\")\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰ (ì„ íƒì‚¬í•­)\n",
    "# evaluations = create_evaluation_ui(results)\n",
    "\n",
    "print(\"ìˆ˜ë™ í‰ê°€ UI ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"ì£¼ì„ì„ ì œê±°í•˜ì—¬ í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 íŒŒë¼ë¯¸í„° ë¹„êµ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature ë¹„êµ\n",
    "test_q = \"DRGê°€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "temperatures = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {test_q}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n[Temperature = {temp}]\")\n",
    "    response, _ = generate_response(test_q, temperature=temp)\n",
    "    print(f\"{response}\\n\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "\n",
    "### 5.1 Flask ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n",
    "# ì£¼ì˜: ì£¼í”¼í„°ë©ì—ì„œëŠ” ë³„ë„ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥\n",
    "\n",
    "print(\"ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´:\")\n",
    "print(\"\\n1. í„°ë¯¸ë„ ì—´ê¸°\")\n",
    "print(\"2. ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰:\")\n",
    "print(\"\\n   python 03_improved_interface.py\")\n",
    "print(\"\\n3. ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†:\")\n",
    "print(\"   http://localhost:8888\")\n",
    "print(\"\\në˜ëŠ” í”„ë¡ì‹œ URL ì‚¬ìš© (Backend.AI ë“±):\")\n",
    "print(\"   http://<proxy-url>/proxy/8888/opnAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ê°„ë‹¨í•œ HTTP ì„œë²„ (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ (ì„ íƒì‚¬í•­)\n",
    "# pip install gradio í•„ìš”\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    \n",
    "    def gradio_generate(question, temperature, max_tokens):\n",
    "        \"\"\"\n",
    "        Gradioìš© ìƒì„± í•¨ìˆ˜\n",
    "        \"\"\"\n",
    "        if not question.strip():\n",
    "            return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "        \n",
    "        response, metadata = generate_response(\n",
    "            question,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        result = f\"{response}\\n\\n\"\n",
    "        result += f\"[ìƒì„± ì‹œê°„: {metadata['generation_time']:.2f}ì´ˆ, \"\n",
    "        result += f\"í† í°: {metadata['tokens_generated']}ê°œ]\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Gradio UI\n",
    "    iface = gr.Interface(\n",
    "        fn=gradio_generate,\n",
    "        inputs=[\n",
    "            gr.Textbox(lines=3, placeholder=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\", label=\"ì§ˆë¬¸\"),\n",
    "            gr.Slider(0.1, 1.0, value=0.3, step=0.1, label=\"Temperature\"),\n",
    "            gr.Slider(64, 512, value=256, step=32, label=\"Max Tokens\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(lines=10, label=\"ë‹µë³€\"),\n",
    "        title=\"ğŸ¥ HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡\",\n",
    "        description=\"ê±´ê°•ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.\",\n",
    "        examples=[\n",
    "            [\"2023ë…„ MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ”?\", 0.3, 256],\n",
    "            [\"DRGê°€ ë¬´ì—‡ì¸ê°€ìš”?\", 0.3, 256],\n",
    "            [\"ê³ í˜ˆì•• í™˜ì ìˆ˜ëŠ”?\", 0.3, 256]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ì„œë²„ ì‹¤í–‰ (í¬íŠ¸ 7860)\n",
    "    # iface.launch(share=False, server_port=7860)\n",
    "    \n",
    "    print(\"âœ… Gradio ì¸í„°í˜ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    print(\"ì£¼ì„ì„ ì œê±°í•˜ê³  iface.launch()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Gradioê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì„¤ì¹˜: pip install gradio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ!\n",
    "\n",
    "ì´ì œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ìœ„ì ¯ ì¸í„°í˜ì´ìŠ¤**: ìœ„ì˜ 2.2 ì…€ ì‚¬ìš©\n",
    "2. **ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤**: ìœ„ì˜ 2.3 ì…€ ì‹¤í–‰\n",
    "3. **ë°°ì¹˜ í‰ê°€**: 3.1 ì…€ë¡œ ëŒ€ëŸ‰ í…ŒìŠ¤íŠ¸\n",
    "4. **ì›¹ ì¸í„°í˜ì´ìŠ¤**: Flask ë˜ëŠ” Gradio\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- ì‹¤ì œ HIRA ë°ì´í„°ë¡œ ì¬í•™ìŠµ\n",
    "- ë” ë§ì€ QA ìŒ ì¶”ê°€\n",
    "- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "- RAG ì‹œìŠ¤í…œ í†µí•©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
