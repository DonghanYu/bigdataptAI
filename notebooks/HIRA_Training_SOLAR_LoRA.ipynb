{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡ - SOLAR-10.7B LoRA í•™ìŠµ\n",
    "\n",
    "**í”„ë¡œì íŠ¸**: HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡  \n",
    "**ëª¨ë¸**: SOLAR-10.7B-v1.0  \n",
    "**ë°©ë²•**: LoRA (Low-Rank Adaptation)  \n",
    "**í™˜ê²½**: A100 80G x2, PyTorch 2.1, JupyterLab\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)\n",
    "2. [ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬](#2-ë°ì´í„°-ë¡œë“œ-ë°-ì „ì²˜ë¦¬)\n",
    "3. [ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ](#3-ëª¨ë¸-ë°-í† í¬ë‚˜ì´ì €-ë¡œë“œ)\n",
    "4. [LoRA ì„¤ì •](#4-lora-ì„¤ì •)\n",
    "5. [í•™ìŠµ ì„¤ì •](#5-í•™ìŠµ-ì„¤ì •)\n",
    "6. [í•™ìŠµ ì‹¤í–‰](#6-í•™ìŠµ-ì‹¤í–‰)\n",
    "7. [ëª¨ë¸ ì €ì¥ ë° í‰ê°€](#7-ëª¨ë¸-ì €ì¥-ë°-í‰ê°€)\n",
    "8. [ì¶”ë¡  í…ŒìŠ¤íŠ¸](#8-ì¶”ë¡ -í…ŒìŠ¤íŠ¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### 1.1 í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes ì°¨ë‹¨ (íì‡„ë§ í™˜ê²½)\n",
    "import sys\n",
    "import os\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "sys.modules['bitsandbytes'] = None\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ê²½ë¡œ ì„¤ì • (ìœ ì—°í•œ êµ¬ì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ê²½ë¡œ ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "# ========================================\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ (í˜„ì¬ ë…¸íŠ¸ë¶ ìœ„ì¹˜ ê¸°ì¤€)\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ (SOLAR-10.7B ëª¨ë¸ì´ ì €ì¥ëœ ìœ„ì¹˜)\n",
    "MODEL_PATH = PROJECT_ROOT / \"solar_10.7b_package\" / \"model\"\n",
    "# ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©:\n",
    "# MODEL_PATH = Path(\"/path/to/solar_10.7b_package/model\")\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "DATA_PATH = PROJECT_ROOT / \"workspace\" / \"data\" / \"hira\" / \"cleaned\"\n",
    "\n",
    "# ì¶œë ¥ ê²½ë¡œ\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"workspace\" / \"models\" / \"solar_hira_lora\"\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“‚ ê²½ë¡œ ì„¤ì •:\")\n",
    "print(f\"  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"  ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}\")\n",
    "print(f\"  ë°ì´í„° ê²½ë¡œ: {DATA_PATH}\")\n",
    "print(f\"  ì¶œë ¥ ê²½ë¡œ: {OUTPUT_PATH}\")\n",
    "\n",
    "# ê²½ë¡œ ì¡´ì¬ í™•ì¸\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"âš ï¸ ê²½ê³ : ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MODEL_PATH}\")\n",
    "if not DATA_PATH.exists():\n",
    "    print(f\"âš ï¸ ê²½ê³ : ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_PATH}\")\n",
    "    print(f\"  â†’ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GPU í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì •ë³´ ì¶œë ¥\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ì •ë³´:\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    - VRAM: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"    - Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"  CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"  âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"\\nâœ… GPU ìµœì í™” ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "### 2.1 Dataset í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIRADataset(Dataset):\n",
    "    \"\"\"\n",
    "    HIRA ê±´ê°•ë³´í—˜ ë°ì´í„°ì…‹\n",
    "    \n",
    "    í˜•ì‹: JSONL (JSON Lines)\n",
    "    êµ¬ì¡°: {\"instruction\": ..., \"input\": ..., \"output\": ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # JSONL íŒŒì¼ ë¡œë“œ\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        self.data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"âš ï¸ ë¼ì¸ {line_num} JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        print(f\"ğŸ“‚ ë¡œë“œ ì™„ë£Œ: {file_path.name} ({len(self.data)}ê°œ ìƒ˜í”Œ)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # SOLAR í”„ë¡¬í”„íŠ¸ í˜•ì‹\n",
    "        instruction = sample.get('instruction', '').strip()\n",
    "        input_text = sample.get('input', '').strip()\n",
    "        output_text = sample.get('output', '').strip()\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        if input_text:\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "        else:\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output_text}\"\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì§•\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # ë ˆì´ë¸” ìƒì„± (ì…ë ¥ê³¼ ë™ì¼, paddingì€ -100ìœ¼ë¡œ)\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        \n",
    "        # Response ë¶€ë¶„ë§Œ í•™ìŠµ (ì„ íƒì‚¬í•­)\n",
    "        # Response ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°\n",
    "        response_token_ids = self.tokenizer.encode(\"### Response:\", add_special_tokens=False)\n",
    "        response_start_idx = None\n",
    "        \n",
    "        search_limit = len(input_ids) - len(response_token_ids) + 1\n",
    "        if search_limit > 0:\n",
    "            for i in range(search_limit):\n",
    "                if input_ids[i:i + len(response_token_ids)].tolist() == response_token_ids:\n",
    "                    response_start_idx = i + len(response_token_ids)\n",
    "                    break\n",
    "        \n",
    "        # Instruction/Input ë¶€ë¶„ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ\n",
    "        if response_start_idx is not None:\n",
    "            labels[:response_start_idx] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "print(\"âœ… HIRADataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ìƒ˜í”Œ ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "train_file = DATA_PATH / \"train.jsonl\"\n",
    "val_file = DATA_PATH / \"val.jsonl\"\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "if train_file.exists():\n",
    "    # ì²« 3ê°œ ìƒ˜í”Œ í™•ì¸\n",
    "    print(\"ğŸ“„ ë°ì´í„° ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:\\n\")\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            sample = json.loads(line)\n",
    "            print(f\"[ìƒ˜í”Œ {i+1}]\")\n",
    "            print(f\"Instruction: {sample.get('instruction', '')}\")\n",
    "            print(f\"Input: {sample.get('input', '')}\")\n",
    "            print(f\"Output: {sample.get('output', '')[:100]}...\")\n",
    "            print()\nelse:\n",
    "    print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    test_data = [\n",
    "        {\n",
    "            \"instruction\": \"ê±´ê°•ë³´í—˜ ë¹…ë°ì´í„° ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\",\n",
    "            \"input\": \"2023ë…„ MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\",\n",
    "            \"output\": \"2023ë…„ ê±´ê°•ë³´í—˜ ì ìš© MRI ê²€ì‚¬ëŠ” ì´ 4,251,032ê±´ì´ ì‹œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì „ë…„ ëŒ€ë¹„ ì•½ 3.2% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"ê±´ê°•ë³´í—˜ ì œë„ë¥¼ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\",\n",
    "            \"input\": \"DRGê°€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "            \"output\": \"DRG(ì§„ë‹¨ëª… ê¸°ì¤€ í™˜ìêµ° ë¶„ë¥˜)ëŠ” ê°™ì€ ì§ˆë³‘ìœ¼ë¡œ ì…ì›í•œ í™˜ìë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë¯¸ë¦¬ ì •í•´ì§„ ê¸ˆì•¡ì„ ì§€ë¶ˆí•˜ëŠ” ì œë„ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë§¹ì¥ ìˆ˜ìˆ ì„ ë°›ìœ¼ë©´ ì…ì› ê¸°ê°„ì´ë‚˜ ì‚¬ìš©í•œ ì•½ì— ê´€ê³„ì—†ì´ ì¼ì • ê¸ˆì•¡ë§Œ ë‚´ê²Œ ë©ë‹ˆë‹¤.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"ê±´ê°•ë³´í—˜ ë¹…ë°ì´í„° ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\",\n",
    "            \"input\": \"2023ë…„ ì „ì²´ ì§„ë£Œë¹„ëŠ”?\",\n",
    "            \"output\": \"2023ë…„ ê±´ê°•ë³´í—˜ ì´ ì§„ë£Œë¹„ëŠ” ì•½ 125ì¡° 2,450ì–µì›ìœ¼ë¡œ ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì „ë…„ ëŒ€ë¹„ ì•½ 8.2% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.\"\n",
    "        }\n",
    "    ] * 100  # 300ê°œ ìƒì„±\n",
    "    \n",
    "    # Train/Val ë¶„í•  (90:10)\n",
    "    split_idx = int(len(test_data) * 0.9)\n",
    "    \n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        for item in test_data[:split_idx]:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        for item in test_data[split_idx:]:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "    print(f\"  Train: {train_file} ({split_idx}ê°œ)\")\n",
    "    print(f\"  Val: {val_file} ({len(test_data) - split_idx}ê°œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "\n",
    "### 3.1 í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Padding í† í° ì„¤ì •\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"  pad_tokenì„ eos_tokenìœ¼ë¡œ ì„¤ì •: {tokenizer.eos_token}\")\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Vocab í¬ê¸°: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SOLAR ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ SOLAR-10.7B ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "print(f\"  ê²½ë¡œ: {MODEL_PATH}\")\n",
    "\n",
    "# bfloat16ìœ¼ë¡œ ë¡œë“œ (A100 ìµœì )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # ìë™ìœ¼ë¡œ GPUì— ë¶„ì‚°\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    use_cache=False  # í•™ìŠµ ì‹œ í•„ìˆ˜\n",
    ")\n",
    "\n",
    "# Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  dtype: {model.dtype}\")\n",
    "print(f\"  device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA ì„¤ì •\n",
    "\n",
    "### 4.1 LoRA Config ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ LoRA ì„¤ì • ì¤‘...\")\n",
    "\n",
    "# LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"]  # ì„ë² ë”©ë„ í•™ìŠµ (ì„ íƒì‚¬í•­)\n",
    ")\n",
    "\n",
    "print(\"LoRA ì„¤ì •:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target Modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LoRA ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nâœ… LoRA ì ìš© ì™„ë£Œ\")\n",
    "print(f\"  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"  ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ ì„¤ì •\n",
    "\n",
    "### 5.1 í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì„¤ì •\n",
    "config = {\n",
    "    \"batch_size\": 2,  # A100 80Gì—ì„œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°\n",
    "    \"gradient_accumulation_steps\": 4,  # ì‹¤íš¨ ë°°ì¹˜ = 8\n",
    "    \"learning_rate\": 2e-4,  # LoRA ê¶Œì¥ í•™ìŠµë¥ \n",
    "    \"num_epochs\": 10,\n",
    "    \"max_length\": 512,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"save_epochs\": 2,  # 2 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    \"eval_steps\": 50,  # 50 ìŠ¤í…ë§ˆë‹¤ ê²€ì¦\n",
    "    \"logging_steps\": 10,\n",
    "    \"patience\": 5  # Early stopping patience\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ í•™ìŠµ ì„¤ì •:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...\\n\")\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = HIRADataset(train_file, tokenizer, config[\"max_length\"])\n",
    "val_dataset = HIRADataset(val_file, tokenizer, config[\"max_length\"])\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¶„í¬:\")\n",
    "print(f\"  í•™ìŠµ: {len(train_dataset)}ê°œ\")\n",
    "print(f\"  ê²€ì¦: {len(val_dataset)}ê°œ\")\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DataLoader ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "print(f\"  ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optimizer ë° Scheduler ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler (Cosine Annealing)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "total_steps = config[\"num_epochs\"] * len(train_loader)\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=config[\"learning_rate\"] * 0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… Optimizer ë° Scheduler ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"  ì´ í•™ìŠµ ìŠ¤í…: {total_steps}\")\n",
    "print(f\"  ì´ˆê¸° í•™ìŠµë¥ : {config['learning_rate']:.2e}\")\n",
    "print(f\"  ìµœì†Œ í•™ìŠµë¥ : {config['learning_rate'] * 0.1:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "### 6.1 í‰ê°€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ í‰ê°€\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 í•™ìŠµ ë£¨í”„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"learning_rate\": [],\n",
    "    \"epochs\": []\n",
    "}\n",
    "\n",
    "# Early stopping ë³€ìˆ˜\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    steps_since_opt_step = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # ë°ì´í„° ì´ë™\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss += loss.item() * config[\"gradient_accumulation_steps\"]\n",
    "        steps_since_opt_step += 1\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if steps_since_opt_step == config[\"gradient_accumulation_steps\"]:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            steps_since_opt_step = 0\n",
    "            \n",
    "            # Progress bar ì—…ë°ì´íŠ¸\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item() * config['gradient_accumulation_steps']:.4f}\",\n",
    "                \"lr\": f\"{current_lr:.2e}\"\n",
    "            })\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë‚¨ì€ gradient ì²˜ë¦¬\n",
    "    if steps_since_opt_step > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Epoch í‰ê·  ì†ì‹¤\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} í‰ê°€ ì¤‘...\")\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"learning_rate\"].append(scheduler.get_last_lr()[0])\n",
    "    history[\"epochs\"].append(epoch + 1)\n",
    "    \n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  LR:         {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Best model ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint_path = OUTPUT_PATH / \"best_model\"\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        \n",
    "        print(f\"  âœ… Best model ì €ì¥: {checkpoint_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  âš ï¸ Validation loss ê°œì„  ì—†ìŒ. Patience: {patience_counter}/{config['patience']}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config[\"patience\"]:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch + 1}\")\n",
    "            print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    if (epoch + 1) % config[\"save_epochs\"] == 0:\n",
    "        checkpoint_path = OUTPUT_PATH / f\"checkpoint-epoch-{epoch+1}\"\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"  ğŸ’¾ Checkpoint ì €ì¥: {checkpoint_path}\")\n",
    "\n",
    "# í•™ìŠµ ì¢…ë£Œ\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  ì´ ì†Œìš” ì‹œê°„: {training_time}\")\n",
    "print(f\"  ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 í•™ìŠµ ê³¡ì„  ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  í”Œë¡¯\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss ê³¡ì„ \n",
    "ax1.plot(history[\"epochs\"], history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
    "ax1.plot(history[\"epochs\"], history[\"val_loss\"], label=\"Val Loss\", marker='s')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Validation Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Learning Rate ê³¡ì„ \n",
    "ax2.plot(history[\"epochs\"], history[\"learning_rate\"], marker='o', color='green')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Learning Rate\")\n",
    "ax2.set_title(\"Learning Rate Schedule\")\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {OUTPUT_PATH / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì €ì¥ ë° í‰ê°€\n",
    "\n",
    "### 7.1 ìµœì¢… ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "final_path = OUTPUT_PATH / \"final_model\"\n",
    "model.save_pretrained(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "\n",
    "print(f\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_path}\")\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "history_file = OUTPUT_PATH / \"training_history.json\"\n",
    "with open(history_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {history_file}\")\n",
    "\n",
    "# ì„¤ì • ì €ì¥\n",
    "config_file = OUTPUT_PATH / \"training_config.json\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… í•™ìŠµ ì„¤ì • ì €ì¥: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 í•™ìŠµ ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nëª¨ë¸ ì •ë³´:\")\n",
    "print(f\"  ë² ì´ìŠ¤ ëª¨ë¸: SOLAR-10.7B-v1.0\")\n",
    "print(f\"  LoRA Rank: {lora_config.r}\")\n",
    "print(f\"  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ë°ì´í„°:\")\n",
    "print(f\"  Train: {len(train_dataset)}ê°œ\")\n",
    "print(f\"  Val: {len(val_dataset)}ê°œ\")\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ì„¤ì •:\")\n",
    "print(f\"  Epochs: {len(history['epochs'])}\")\n",
    "print(f\"  Batch Size: {config['batch_size']}\")\n",
    "print(f\"  Gradient Accumulation: {config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning Rate: {config['learning_rate']:.2e}\")\n",
    "\n",
    "print(f\"\\nìµœì¢… ì„±ëŠ¥:\")\n",
    "print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜:\")\n",
    "print(f\"  Best Model: {OUTPUT_PATH / 'best_model'}\")\n",
    "print(f\"  Final Model: {OUTPUT_PATH / 'final_model'}\")\n",
    "print(f\"  History: {history_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### 8.1 Best ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Best ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ë¡œë“œ\n",
    "best_model_path = OUTPUT_PATH / \"best_model\"\n",
    "test_model = PeftModel.from_pretrained(base_model, str(best_model_path))\n",
    "test_model.eval()\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ì¶”ë¡  í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, max_new_tokens=256, temperature=0.3):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    \"\"\"\n",
    "    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    prompt = f\"### Instruction:\\nê±´ê°•ë³´í—˜ ë¹…ë°ì´í„° ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.85,\n",
    "            top_k=40,\n",
    "            repetition_penalty=1.15,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Response ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ… ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"2023ë…„ MRI ê²€ì‚¬ ê±´ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\",\n",
    "    \"DRGê°€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ê³ í˜ˆì•• í™˜ì ìˆ˜ëŠ”?\",\n",
    "    \"ê±´ê°•ë³´í—˜ ì´ ì§„ë£Œë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"ìš”ì–‘ë³‘ì›ê³¼ ì¢…í•©ë³‘ì›ì˜ ì°¨ì´ëŠ”?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"[ì§ˆë¬¸ {i}] {question}\")\n",
    "    response = generate_response(question)\n",
    "    print(f\"[ë‹µë³€] {response}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ì ‘ ì§ˆë¬¸í•´ë³´ê¸° (ì´ ì…€ì„ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ ê°€ëŠ¥)\n",
    "print(\"ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit')\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"Q: \").strip()\n",
    "    \n",
    "    if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:\n",
    "        print(\"\\ní…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "    \n",
    "    if not question:\n",
    "        continue\n",
    "    \n",
    "    response = generate_response(question)\n",
    "    print(f\"\\nA: {response}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ!\n",
    "\n",
    "HIRA ë¹…ë°ì´í„° ìƒë‹´ ì±—ë´‡ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **ëª¨ë¸ í‰ê°€**: `HIRA_Evaluation.ipynb` ë…¸íŠ¸ë¶ ì‹¤í–‰\n",
    "2. **ì¸í„°í˜ì´ìŠ¤ ë°°í¬**: `HIRA_Interface.ipynb` ë…¸íŠ¸ë¶ ì‹¤í–‰\n",
    "3. **ëª¨ë¸ ê°œì„ **: ë°ì´í„° ì¦ê°• ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "\n",
    "### ì €ì¥ëœ íŒŒì¼\n",
    "\n",
    "- Best Model: `workspace/models/solar_hira_lora/best_model/`\n",
    "- Training History: `workspace/models/solar_hira_lora/training_history.json`\n",
    "- Training Curves: `workspace/models/solar_hira_lora/training_curves.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
